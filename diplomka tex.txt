\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx}

\title{MASTER'S THESIS}
\author{Bc. Martin Oharek }
\date{December 2019}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}

\chapter{Classification}
\section{State-of-the-art classification models}
\subsection{Logistic regression}
\subsection{Support vector machines (SVM)}
\subsection{Random forest}
\subsubsection{Decision tree}
\subsubsection{Ensemble of decision trees}
\subsection{Neural network (NN)}
\subsubsection{Perceptron and neuron}
\subsubsection{Activation functions}
\subsubsection{Backpropagation algorithm}
\section{Evaluation metrics}

\chapter{Decision-Tree-Inspired NN architecture}
In this section is provided detailed derivation of the basic neural network architecture, which is further employed in all subsequently proposed models. The main idea is built on the article \cite{NRF} describing 1 to 1 transformation of arbitrary regression tree (random forest regressor) to the specifically designed neural network (ensemble of neural networks).

Unfortunately, an approach presented in \cite{NRF} is not uniformly convertible to the classification problem. Based on this issue, the  alternative architecture and initial settings are proposed in order to simulate the exact behaviour of corresponding decision tree classifier.

This reformulation provides a sensible opportunity to enhance the decision tree performance, because parameters of newly constructed neural network could be better adapted with usage of the backpropagation algorithm of neural networks and therefore achieve superior classification.

\section{Architecture and initial settings}

The initial weights and biases of neurons and architecture of input layer and first two hidden layers will remain same among all proposed models. The settings was motivated by \cite{NRF}. A sample of such architecture (only input layer and first two hidden layers) is illustrated in Figure \ref{fig:basic_arch} copying decisions of the decision tree depicted in \ref{fig:dec_ilust}, which splits the space by two hyperplanes as illustrated also in \ref{fig:dec_ilust}. Let's first consider neurons in the first and second hidden layer of network as perceptrons. This means, that activation function is
\begin{equation}
    \tau(\boldsymbol{x}) = 2\mathbf{1}_{\boldsymbol{x} \geq 0} - 1\hspace{0.5cm},
\end{equation}
where all math operations on vector $\boldsymbol{x}$ are element-wise. Also
\begin{equation}
    (\mathbf{1}_{\boldsymbol{x} \geq 0})_i = \begin{cases}
    1, & \text{if}\hspace{0.25cm} x_i \geq 0 \\
    0, & \text{otherwise}
    \end{cases}\hspace{0.5cm},
\end{equation}

where $(\mathbf{1}_{\boldsymbol{x} \geq 0})_i$ is $i$-th element of $\mathbf{1}_{\boldsymbol{x} \geq 0}$.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{ilustrace_in_space.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{decision_tree_ilustrace.png}
\end{minipage}
\caption{In the left Figure are depicted 3 color-coded classes divided by the decision tree in the right Figure, that are separated by two hyperplanes: \\$x_1 - 2 = 0$ and $x_2 - 1 = 0$. Inner nodes of the decision tree are green colored with corresponding split function next to them, whereas the leaves are red colored. All nodes are numbered.}
\label{fig:dec_ilust}
\end{figure}
\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{ilustrace_basic_structure.png}
\caption{Transformation of the decision tree in Figure \ref{fig:dec_ilust} to the neural network with two hidden layers. The first hidden layer detects the decisions of inner nodes same numbered as in \ref{fig:dec_ilust}. The second hidden layer retrieves leaf membership of input vector same as in the decision tree. Not null connections between neurons are bold higlighted with corresponding weights. Biases are written next to the neurons. All dashed connections indicates null connection (weight equals 0).}
\label{fig:basic_arch}
\end{figure}

So perceptrons from the first and second hidden layer outputs $+1$ or $-1$. Why is it chosen in such manner will be clear from further explanations.

\subsection{First hidden layer}

The first hidden layer should copy decisions of all inner nodes present in the corresponding decision tree. The first hidden layer has same number of neurons as number of inner nodes. As was explained in the chapter about decision trees and random forests, each inner node $k \in \{1,...,L-1\}$ of the decision tree, where $L-1$ is total number of inner nodes (in fact, if $L-1$ is total number of inner nodes, then $L$ is total number of leaves present in the decision tree), possesses split function with parameters $j_k \in \{1,...,n\}$, which is one dimension in a $n$-dimensional space that is used for split and also $\alpha_{j_k}$, which is a threshold. Let's also define function $s_k$ as 
\begin{equation}
    s_k(\boldsymbol{x}) = x_{j_k} - \alpha_{j_k}\hspace{0.5cm}.
\end{equation}
It is apparent that equation $s_k(\boldsymbol{x}) = 0$ defines a hyperplane in $\mathbb{R}^n$, which splits the space in inner node $k$.

In order to obtain all decisions of inner nodes in the corresponding neurons of our neural network, we initialize weights in neuron $k$ as $(0,..,0,1,0,..,0)^T$ with single 1 in $j_k$-th position and 0 otherwise. Bias is set to $-\alpha_{j_k}$. Hence, output of the first hidden layer is $(\tau(s_1(\boldsymbol{x})),\tau(s_2(\boldsymbol{x})),...,\tau(s_{L-1}(\boldsymbol{x}))$ and it precisely copies decisions of inner nodes, with +1 indicating that input vector belongs to the right side of the hyperplane and -1 otherwise (and +1 if it belongs to the hyperplane). This is also done for the inner nodes outside the path of the input vector. The illustration can be seen in Figure \ref{fig:first_hid_layer}.


\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{first_hidden_layer_space.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{first_hidden_layer.png}
\end{minipage}
\caption{Space in the left figure is divided by red (in inner node number 1) and black (in inner node number 2) hyperplanes. If we consider point $\boldsymbol{x} = (x_1,x_2)^T = (2.75,0.45)^T$ highlighted in left Figure, the output from the first hidden layer is +1 from neuron 1 (corresponding to the red hyperplane) and -1 from neuron 2 (corresponding to the black hyperplane).}
\label{fig:first_hid_layer}
\end{figure}

\subsection{Second hidden layer}

From all inner node decisions obtained by the first hidden layer it should be possible to reconstruct the exact leaf membership of the input vector $\boldsymbol{x}$. This is the main task to accomplish by the second hidden layer. If there are $L-1$ neurons in the first hidden layer, then the second hidden layer consists of $L$ neurons, each one corresponding to the one individual leaf in the decision tree.

We connect neuron $m$ in the first hidden layer to neuron $m'$ in the second hidden layer with not null weight if and only if inner node corresponding to the neuron $m$ belongs to the path from root node to the leaf corresponding to the neuron $m'$. The weight is initialized to +1 if the split by inner node $m$ is to the right child and -1 otherwise. If neuron $m$ is not part of the path from root to the leaf, the weight is initialized always to 0.

Based on this setting, it could be simply deduced that number of not null connections from the first hidden layer (weights) to the (arbitrary) neuron $m'$ in the second hidden layer is same as length of the path from root to the leaf $m'$. This is illustrated in Figure \ref{fig:sec_hidd_len}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{sec_hidd.png}
\caption{In the left figure is highlighted blue path from the root node 1 to the leaf 3. This has length 1 and also only one initialized not null connection from the first hidden layer exists, because only root node is part of the path. In the remaining figures are depicted paths for other leaves (blue colored) from the root node (except of leaf 6, which is in same depth as leaf 7). It is easy to see, that equality between length of the path from the root node to the particular leaf and number of not null connections from the first hidden layer to the corresponding neuron holds always.}
\label{fig:sec_hidd_len}
\end{figure}

If output from the first hidden layer is $\boldsymbol{v} = (\pm 1, \pm 1,....,\pm 1)^T$, which encodes all decisions of inner nodes of the decision tree, then output from the neuron $m'$ in the second hidden layer is $\tau(\sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m'))$, where\\ $\boldsymbol{w}^{m'} = (w_1^{m'},w_2^{m'},...,w_{L-1}^{m'})^T$ is a vector of  weights for connections to neuron $m'$. These weights are not null if the corresponding inner node is involved in the path from root to the leaf $m'$ and are +1 if it is sent to the right child and -1 otherwise. For all inner nodes that are not involved in the root-leaf path are weights initialized to 0.

Desired behaviour of the neuron $m'$ in the second hidden layer is to output +1 if the input vector ends in leaf $m'$ and -1 otherwise. For this purpose, $bias(m')$ must be correctly set. To do so, it is sufficient to notice that the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i)$ as the first term in the argument of $\tau(\cdot)$ function equals to the length of the path from root node to leaf $m'$, if and only if the input ends in leaf $m'$. For convenience, let us denote this length as $l(m')$. It is a simple consequence of the fact, that the number of not null weights $w_{i_k}^{m'}$ is same as $l(m')$ and also they have same magnitude ($|w_{i_k}^{m'}| = 1$) and same sign as $v_{i_k}$, where $i_k \in \{1,...,L-1\}, w_{i_k}^{m'} \neq 0$. Therefore, each member of the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i) = \sum_{i_k=1,w_{i_k}^{m'} \neq 0}^{L-1}(w_{i_k}^{m'}v_{i_k})$ equals to +1 and all members sum up to $l(m')$.

Moreover, if the input does not end in leaf $m'$, then in the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i)$ exists a not null member in which interfere 2 integers (ones) with different signs, resulting in -1. Hence is clear, that if input does not end in leaf $m'$, the sum holds inequality $\sum_{i=1}^{L-1}(w_i^{m'}v_i) \leq l(m')-1 < l(m')$. For more precise intuition, the illustration is provided in Figure \ref{fig:sec_hidd_ilust}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{sec_hidd_ilust.png}
\caption{Demonstration of inequality $\sum_{i=1}^{L-1}(w_i^{m'}v_i) \leq l(m')-1 < l(m')$ in case of leaf number 7 (corresponding to one neuron in the second hidden layer of our network), if input does not end in leaf $m'$. The red path in the picture indicates real path of input in the decision tree. Our sample input ends in leaf number 6, as could be seen from the illustration. The output from the first hidden layer would therefore be $\boldsymbol{v} = (v_1,v_2,v_3)^T = (+1,-1,-1)^T$. But weights corresponding to neuron 7 (leaf 7) are $\boldsymbol{w}^7 = (w_1^7,w_2^7,w_3^7)^T = (+1,-1,+1)^T$. After multiplying the values in red circles and summing the results up, we obtain $\sum_{i=1}^{L-1}(w_i^{7}v_i) = 2 < 3$, where 3 means the length of the root-leaf path. A decrease is caused due to the interference of different signs in a grey circle.}
\label{fig:sec_hidd_ilust}
\end{figure}

After these considerations, the reasonable choice of $bias(m')$ is
\begin{equation}
    bias(m') = -l(m') + 0.5
\label{eq:bias}    
\end{equation}
and then $\sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m')$ has following property:
\begin{equation}
    \sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m')\begin{cases}
    > 0, & \text{if input ends in leaf}\hspace{0.18cm} m' \\
    < 0, & \text{otherwise}
    \end{cases}
\label{eq:sec_hidd_property}
\end{equation}

With respect to the property of $\tau(\cdot)$ function argument in \eqref{eq:sec_hidd_property}, the second hidden layer outputs a vector of $(-1,...,-1,+1,-1,...,-1)^T$ with a single positive 1 indicating the correct leaf membership of an input.

To retain this $\tau(\cdot)$ argument property, it is sufficient to choose any other arbitrary constant in \eqref{eq:bias} in range $(0,1)$ instead of 0.5. But to stay consistent with \cite{NRF}, we also used the proposed value of 0.5 in conducted experiments.

At this stage, we already defined architecture and initial weights and biases settings of first two hidden layers in order to transform decision tree into neural network with the same properties. All that remains is to gain classification predictions from the second hidden layer.

\subsection{Output layer}

In this section is proposed architecture and initial setting for output layer, that will gain same predictions as the decision tree. Unfortunately, same method proposed for regression trees in \cite{NRF} is not directly applicable in the classification case. Therefore we propose an alternative for the classification case, that give same classification outcomes as the corresponding decision tree.

The output layer will be constructed as follows: The number of neurons in output layer is equal to number of classes we desire to classify. Each neuron corresponds to only one particular class (one label). Neuron with the highest activation represents the predicted class of neural network. In the experiments were used two types of neurons in output layer - with sigmoid and softmax activation functions, as were discussed in Chapter [Activation functions].  To get the same performance as the decision tree, we must retrieve probability distributions stored in leaves from the leaf membership encoded in the second hidden layer. If the output layer outcomes the same probability values for classes as the decision tree, hence the neural network performs alike.

Let's denote output from the second hidden layer as\\ $\boldsymbol{r} = (-1,...,-1,+1,-1,...,-1)^T$, where the position of +1 indicates the leaf where the input falls in. For each leaf $l \in \{1,...,L\}$ we denote a probability vector $\boldsymbol{p^l} = (p^l_1,p^l_2,...,p^l_C)^T$ with probabilities of individual classes, that are stored in leaf $l$, where $C$ is total number of classes. If $\boldsymbol{r}$ has +1 as the first element (corresponding to the first leaf), i.e. $r_1 = +1$, the output layer should outcome $\boldsymbol{p}^1$. If $r_2 = +1$, the outcome should be $\boldsymbol{p}^2$ etc.

If we initialize biases in the output layer to 0, then appropriate initialization weights could be obtained by solving the system of linear equations with a regular matrix $\mathbb{A}$
\begin{equation}
    \mathbb{A}=\begin{pmatrix}
    1 & -1 & -1 &\hdots & -1 \\
    -1 & 1 & -1 & \hdots & -1\\
    \vdots & \ddots & \ddots & \ddots&\vdots \\
    -1 & \ddots & \ddots & \ddots&-1 \\
    -1 & -1 & \hdots & -1 & 1
    \end{pmatrix}\hspace{0.5cm}.
\end{equation}

Matrix $\mathbb{A} \in \mathbb{R}^{L\times L}$, where $L$ is a total number of leaves in the decision tree. The determinant of matrix $\mathbb{A}$ is
\begin{equation}
    \text{det}\mathbb{A} = \begin{vmatrix}
    1 & -1 & -1 &\hdots & -1 \\
    -1 & 1 & -1 & \hdots & -1\\
    \vdots & \ddots & \ddots & \ddots&\vdots \\
    -1 & \ddots & \ddots & \ddots&-1 \\
    -1 & -1 & \hdots & -1 & 1
    \end{vmatrix} = (-1)^{2L-3}\cdot2^{L-1}\cdot(L-2)
\label{eq:determinant}
\end{equation}

For the proof of \eqref{eq:determinant} see Chapter [Proves]. Matrix $\mathbb{A}$ is therefore always regular with except of $L = 2$. In this case, $\text{det}\mathbb{A} = 0$ and matrix $\mathbb{A}$ is singular. But for $L=2$, the decision tree has only root node and 2 leaves, which is rarely a well-functional model in practical use. It could have a good performance almost only in case of data with significantly unambigous geometric deployment, where occurs only 2 classes and exists one hyperplane that sufficiently separates them. As the conclusion, we will almost never encounter such an elementary model.

In case of invertible activation function $\sigma(\cdot)$ in the output layer (in our experiments were used sigmoid and softmax activation functions, which are both invertible), we obtain appropriate weights for neuron $c$ (also represents class $c \in \{1,...,C\}$ that this neuron corresponds to) in output layer after solving the following system of linear equations:

\begin{equation}
    \mathbb{A}\begin{pmatrix}
    w^c_1\\
    w^c_2\\
    \vdots\\
    w^c_L
    \end{pmatrix} = \sigma^{-1}(\begin{pmatrix}
    p^1_c\\
    p^2_c\\
    \vdots\\
    p^L_c
    \end{pmatrix})\hspace{0.5cm},
    \label{eq:lin_eq}
\end{equation}

where $\boldsymbol{w}^c = (w^c_1,...,w^c_L)^T$ are weights of connections from the second hidden layer to neuron $c$ in the output layer and $\sigma^{-1}(\cdot)$ is inverse function of $\sigma(\cdot)$. In other words, the appropriate weights for neuron $c$ in the output layer are obtained as

\begin{equation}
    \begin{pmatrix}
    w^c_1\\
    w^c_2\\
    \vdots\\
    w^c_L
    \end{pmatrix} = \mathbb{A}^{-1}(\sigma^{-1}(\begin{pmatrix}
    p^1_c\\
    p^2_c\\
    \vdots\\
    p^L_c
    \end{pmatrix}))\hspace{0.5cm}.
    \label{eq:weights_output}
\end{equation}

After initialization of biases to 0 and solving the system of linear equations for all neurons in the output layer (or computing the inverse of matrix $\mathbb{A}$) and for appropriate activation function chosen in advance, we gain also all initialization weights. This initial setting causes the neural network to output same predictions as from the corresponding decision tree and hence to get equally performing classification model.

\section{NN models with decision-tree-initialization}

\subsection{Perceptron activation function replacement}

In order to apply reasonable training procedure with backpropagation algorithm on proposed method, it is suitable to replace perceptron activation function $\tau(\cdot)$ with its smooth approximation. For this purpose the hyperbolic tangent function was adopted (Figure BLA). This affects original transformation, because due to this approximation the neural random tree is no longer one-to-one transformation to former decision tree. But still under certain conditions (especially the transition slope from the negative part of the $\tanh(\cdot)$ function to the positive one - the more upright, the better approximation we get) it could come fairly close to the perceptron function and therefore entire model performance remains unchanged with respect to the decision tree. Usage of $\tanh(\cdot)$ in first and second hidden layer is necessary with respect to the transformation procedure, even that $\tanh(\cdot)$ has some drawbacks as activation function (e.g. vanishing gradients [ref]). Using other activation functions in this case is not appropriate, because it will lack any decision tree relationship.

The closeness of $\tanh(\cdot)$ to the perceptron activation function in our experiments is controlled with $\beta > 0$ parameter.

\begin{equation}
    \tanh(\beta z) = \frac{e^{2\beta z - 1}}{e^{2\beta z} +1} 
\end{equation}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{tanh.png}
\caption{Shape of hyperbolic tangent for different $\beta$ parameters.}
\label{fig:tanh}
\end{figure}


The higher $\beta$ is, the better approximation of perceptron activation function is observed. With $\beta \rightarrow \infty$, $\tanh(\cdot)$ converges to the $\tau(\cdot)$. In experiments were exploited two parameters, $\beta_1$ and $\beta_2$, each to control transitions of $\tanh(\cdot)$ in different hidden layers. They will be mentioned and examined in further chapters. The influence of these parameters on the shape is depicted in the Figure \ref{fig:tanh}.

\subsection{Competitive decision-tree-motivated models}
In this section is provided overview of all decision-tree-motivated models examined in the experimental part of the thesis. Alongside the reference model proposed in Chapter 3, we propose another competitive models motivated by original decision tree structure. These models exploits knowledge acquired by training the decision tree and use it for setting the proper weights and biases up, but not all of them. Especially they relax conditions on weights in output layer (the main model from Chapter 3 has deterministic weights gained from the decision tree in all layers) and leave them random. This reduces inclination of model to overfitting, but does not provide accurate decision tree transformation as our reference model.

\subsubsection{Reference model (NRT deterministic weights)}

Acquisition of this model is described in Chapter 3. Its main purpose is to simulate corresponding decision tree behaviour as good as possible from the very beginning (quality of simulation is controlled with $\beta$ parameter of activation functions in the first and second hidden layer). With both  $\beta$s approach infinity, the reference model gives same predictions as decision tree. Of course, with respect to the predicting procedure of the decision tree, it could converge to predicting same results with $\beta$s only "high enough".
% sem dát možná nějaké vysvětlení, jaká vliv má změna v beta na prediction, použít nějaký horní odhad výstupního vektoru, maticové normy
After transformation the neural network is already a sufficient model capable of making relevant predictions, because it keeps similar behaviour as corresponding decision tree. This feature makes neural network very sensitive to the learning rate hyperparameter, which needs to be set beforehand. Also there exists significant risk of overfitting, which should be reduced by adapting regularization terms. Further analysis of these issues will be provided in later chapters. The closeness of this model to the corresponding decision tree gives good opportunity to enhance performance of the previous model by further backpropagation training. Also convergence should be very fast, which will be examined in the experimental part.

\subsubsection{NRT basic}

First competitive model has first and second hidden layer similar to the layers in the reference model. The difference is in setting weights and biases in the output layer. In this case we leave weights and biases in output layer purely random, as is depicted in Figure \ref{fig:nrt_basic}. This modification should be more relaxing with respect to the backpropagation training and reduce amount of sensitivity to the learning rate and risk of overfitting. On the other hand, it should slow down convergence in comparison with the reference model.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_basic.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_basic}
\end{figure}

\subsubsection{NRT extra layer}

Next competitive model add one extra layer of neurons between second hidden layer and output layer. This layer should serve as an aggregate layer that first summarize information from the second hidden layer (information from leaves of the decision tree) and then transfer this information to the output layer. Clearly, number of neurons in the third hidden layer is optional, but in our experiments was this value set up same as number of neurons in the output layer (number of classes). Weights and biases in this (third) hidden layer and output layer are purely random, as illustrated in the Figure \ref{fig:nrt_extra_layer}. We also exploited sigmoid function and Leaky ReLu as activation functions. This neural network setting uses decision tree to set a decisive start to the neural network to output leaf membership of an instance in the second hidden layer, then summarize information from this layer in the third hidden layer and extract output in the output layer. Adding extra layer with random parameters should in this case slow down convergence and enlarge the number of hyperparameters, but could have positive impact on the final performance and outperform previous models.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_extra_layer.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_extra_layer}
\end{figure}

\subsubsection{NRT extra layer - deterministic weights}

Final competitive model is the compound of 'NRT extra layer' and reference model. There is added an extra layer and weight and biases of the third hidden layer are initialized in the same manner as in the output layer of the reference model (usage of inverse of matrix $\mathbb{A}$). 

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_extra_layer_deterministic.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_extra_layer_det}
\end{figure}

%Describe all five competitive models that we use in experiments with illustrations and algorithms. Discuss replacement of $\tau$ with tanh activation function and its influence on performance and backpropagation. Divide basic model to sparse setting and full connected setting. In sparse setting we allow to train with backpropagation only notnull connections, to preserve decision tree interpretation. That will help us also to demonstrate the effect of backpropagation. In full connected setting we train all parameters.


\chapter{Experiments}

\begin{thebibliography}{}
\bibitem{NRF} Neural Random Forest. Gerard Biau, Erwan Scornet, Johannes Welbl

\end{thebibliography}







\end{document}
