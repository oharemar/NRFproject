\documentclass{report}
%\documentstyle[a4]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage[hang]{footmisc}
\usepackage{tablefootnote}
\usepackage{pbox}
\usepackage{url}
\usepackage{makecell}
\usepackage{color, colortbl}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{makecell}
%\usepackage{subcaption}
\usepackage{subfig}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\title{MASTER'S THESIS}
\author{Bc. Martin Oharek }
\date{December 2019}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}

\chapter{Classification}
\section{State-of-the-art classification models}
\subsection{Logistic regression}
\subsection{Support vector machines (SVM)}

\section{Evaluation}

This section is focused on the summary and description of the evaluation metrics used in the experimental part, alongside the properties of each metric and suitable application. This section includes only metrics that are applied in the testing phase. The correct choice of evaluation is the main and crucial task in the case of searching for optimal classification model. In practice, there exist numerous evaluation methods applied frequently to many classification tasks, each fits better in different cases. Before we choose set of evaluation metrics, we must be aware of the processed data and classification model to set our evaluation correctly. Also the purpose and future application scope of our model is significant. If this foremost analysis of the data and model is neglected or done wrong, we could end up choosing bad-shaped evaluation techniques and therefore misinterpret the performance of our model.

In the literature could be found significant amount of studies addressing the choice of evaluation metrics in the case of either binary classification \cite{evaluation1,evaluation2} or multi-class classification \cite{multieval1,multieval2}.

The classification models in this thesis are tested on multiple datasets, both binary and multi-class classification oriented. Consequently, there is not any particular property that could discriminate correct (the most correct) setting of evaluation methods in general, unless we study each dataset separately, which is not the purpose of this thesis at all. We only desire to compare classification performance quality of neural random forest mainly to the performance of regular random forests and provide the evidence of the superiority of neural random forests. Therefore we select subset of worldwide-accepted, well-functional evaluation techniques, summarize them and describe them separately. Combination of these metrics provides sufficient evaluation to build a conclusion.

Firstly, we define important notation that is referenced in the upcoming descriptions. This applies generally to the multi-class classification with arbitrary number of classes (labels). Set of all classes is denoted as $\mathbb{C}$.

\begin{definition}
\textit{Positive} class $c \in \mathbb{C}$ is such label, which is in the current scope of interest. Other classes $d \neq c$, $d \in \mathbb{C}$, are called \textit{negative} classes.
\label{definition_pos_neg}
\end{definition}

To clarify \ref{definition_pos_neg}, if we test the  classification performance of our model with respect to class $c \in \mathbb{C}$, then $c$ is called positive class and other labels $d \neq c$, $d \in \mathbb{C}$, are called negative classes. It is important to realize, that there could be many positive classes. It depends chiefly on the current scope of interest. For instance, computing accuracy of class $c_1 \in \mathbb{C}$ means that $c_1$ is (currently) positive class and other classes are negative. In the following, current positive class will be put into parentheses (such as $\text{TP}(c), \text{FN}(c)$ etc.) Also the instances belonging to the positive, resp. negative class could be referenced as positive, resp. negative instances.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    
       $\text{TP}(c)$ &\makecell{count of positive instances (class $c \in \mathbb{C}$) \\ correctly predicted } \\ \hline
         $\text{FP}(c)$& \makecell{count of negative instances \\ predicted incorrectly as positive class $c$ }  \\ \hline
         $\text{TN}(c)$ &\makecell{count of negative instances (with respect to the positive class $c$) \\ correctly predicted  }\\ \hline
         $\text{FN}(c)$ & \makecell{count of positive instances (class $c \in \mathbb{C}$) incorrectly predicted as negative \\ class (with respect to the positive class $c$) }\\ \hline
    \end{tabular}
    \caption{Definitions of TP,FP,TN and FN.}
    \label{tab:definitions}
\end{table}
 


\subsubsection{Accuracy} 


Probably the most frequently applied evaluation metric in the machine learning society, which provides quick and simple evaluation of the classification performance. Outcome of this metric is the ratio between number of correctly classified instances and number of all instances. It is usually a single value per classifier (not specific to individual classes) and it is defined as

\begin{equation}
    Accuracy = \frac{\text{number of correctly classified instances}}{\text{number of all instances}} \hspace{0.2cm}
\end{equation}

Altough gaining this simple value for one classification model is very easy and quick, this metric possesses some significant drawbacks \cite{multieval1} and therefore relying only on this metric during final evaluation is dangerous. It does not take into account membership of instances to either positive or negative classes. Serious problem arises especially when processing imbalanced data \cite{imbalance}, which is the case of many real-world applications (fraud and malware detection \cite{bakalarka} etc.).

For instance, imagine data distributed into two classes, class 0 with 98\% occurrence and class 1 with only 2\% occurrence. This dataset is clearly significantly imbalanced. Moreover, we put higher preference on correctly classifying minority class 1 (e.g. consider class 1 as malware type communication and class 0 as arbitrary safe network communication, it could be basically anything rare vs. normal). Also define naive classifier to constantly classifying any instance as class 0. In this case, accuracy of this model is $Acc = 98\%$, which is very high. Based only on this value, the classification performance of our naive model is perfect. But claiming that this model is good is wrong, since it completely fails in classifying minority class, which has the highest priority. So even that accuracy is very high, the model itself is absolutely useless.

\subsubsection{Precision}

Another evaluation metric is called precision and is class-sensitive. In the contrast to the accuracy, it is computed for any particular class $c \in \mathbb{C}$ as
\begin{equation}
    Precision(c) = \frac{\text{TP}(c)}{\text{TP}(c)+\text{FP}(c)}
\end{equation}

It expresses the ratio between correctly predicted positive instances and all instances predicted as positive. In other words, it shows how precise is our classification model in predicting positive class.

This metric is also dependent on the rate of imbalance among current data. Let's suppose adding negative instances to the dataset. Then 

$$\text{FP}^{\text{less}}(c) \leq \text{FP}^{\text{more}}(c) \implies Precision^{\text{less}}(c) \geq Precision^{\text{more}}(c)$$

So even if our classifier predicts all positive instances correctly, the precision tends to decrease with increasing number of negative instances.

\subsubsection{Recall}

This metric is computed for class $c \in \mathbb{C}$ as

\begin{equation}
    Recall(c) = \frac{\text{TP}(c)}{\text{TP}(c) + \text{FN}(c)}
\end{equation}

Recall expresses ratio between number of correctly classified positive instances and number of all positive instances. Clearly, this metric does not depend on the rate of imbalance in our data and therefore is suitable choice for evaluation on imbalance datasets.

In many applications of classification on imbalance datasets is put a strong requirement on model to predict minority classes as precisely as possible (capture all positive instances, but also not to cause a lot of mistakes on negative instances), because misdetections could inflict great damage (e.g. malware detection \cite{bakalarka}). In some studies it turns out that combination of precision and recall is appropriate choice to satisfy these requirements and provides reliable evaluation \cite{imbalance_data}.

\subsubsection{F-measure}

F-measure metric combines precision and recall into single value as

\begin{equation}
    F_\beta(c) = \frac{(1+\beta^2)\cdot Precision(c) \cdot Recall(c)}{\beta^2 \cdot Precision(c) + Recall(c)}\hspace{0.5cm} ,
\end{equation}
where $c \in \mathbb{C}$ and $\beta$ is a coefficient which serves to adjust relative importance between precision and recall (often $\beta = 1$). For $\beta = 1$ it expresses harmonic mean of precision and recall. As $\beta \rightarrow 0$ the formula considers only precision and as $\beta \rightarrow \infty$ the formula considers only recall. Generally, $\beta < 1$ favors precision and $\beta > 1$ favors recall. It depends on a specific application to choose $\beta$ appropriately. For $\beta = 1$ we refer to the corresponding F-measure as F1-score.

\subsubsection{G-measure}

G-measure is computed as

\begin{equation}
    G(c) = \sqrt{\frac{\text{TP}(c)}{\text{TP}(c) + \text{FN}(c)} \cdot \frac{\text{TN}(c)}{\text{TN}(c) + \text{FP}(c)}} \hspace{0.5cm},
    \label{g-meas}
\end{equation}
where $c \in \mathbb{C}$. The left factor in multiplication under the square root in \eqref{g-meas} is called \textit{Sensitivity} or \textit{true positive rate} (same as recall) and the right factor is called \textit{Specificity} or \textit{true negative rate}.

Optimization of this metric secures good balance between classification performance on both minority and majority classes. In the case of imbalance, even if the classification performance on negative instances is perfect, G-measure would end significantly low if classification of positive instances is poor \cite{imbalance3}. This is valuable property which makes this metric well-applicable to evaluation of the classification model on imbalanced data.

\textbf{Note:} Since we compare different classification models and track their overall performance on various datasets in the experimental part, the main purpose is not to compare metric values for each individual class, but rather to compare summaries of these evaluation outcomes. For this reason we adopt \textbf{macro/weighted average} to represent the individual metrics for different classes as single value.

For arbitrary (class-sensitive) evaluation metric $f(\cdot)$ is macro/weighted average defined as

\begin{equation}
    MacroAvg(f) = \frac{\sum_{l=1}^{M}f(c_l)}{M}
\end{equation}
\begin{equation}
    WeightedAvg(f) = \frac{\sum_{l=1}^{M}n_l \cdot f(c_l)}{\sum_{l=1}^{M}n_l} \hspace{0.5cm},
\end{equation}
where $c_l, l \in \{1,2,...,M\}$ is $l$-th class, $n_l$ is count of (testing) instances belonging to $c_l$ and $M$ is total number of classes.

It is also worth to mention \textbf{micro average} of precision and recall, which is defined as

\begin{equation}
    MicroAvg(Precision) = \frac{\sum_{l=1}^{M}\text{TP}(c_l)}{\sum_{l=1}^{M}(\text{TP}(c_l) + \text{FP}(c_l))}
\end{equation}

\begin{equation}
    MicroAvg(Recall) = \frac{\sum_{l=1}^{M}\text{TP}(c_l)}{\sum_{l=1}^{M}(\text{TP}(c_l) + \text{FN}(c_l))}
\end{equation}

$MicroAvg(Precision)$ computes basically fraction of the sum of all correctly predicted instances and sum of all correctly and incorrectly predicted instances, which is the sum of all instances. Therefore it expresses same value as accuracy of the model. Furthermore, since $\sum_{l=1}^{M}\text{FN}(c_l) = \sum_{l=1}^{M}\text{FP}(c_l)$, both values are equal. Note that this applies to the classification problems where predicting 


\subsubsection{ROC curve}

ROC (receiver operator characteristics) curve \cite{roc} is a popular graphical technique used for comparison of performance of classifiers mainly in the case of binary classification tasks. It is a two dimensional visualization depicting the true positive rate (TPR) on the vertical axis and the false positive rate (FPR) on the horizontal axis for all meaningful decision thresholds. In the binary settings (negative class 0 and positive class 1), TPR and FPR are defined as

\begin{align}
    \text{TPR} &= \frac{\text{TP}}{\text{TP} + \text{FN}} \\
    \text{FPR} &= \frac{\text{FP}}{\text{FP} + \text{TN}} \hspace{0.2cm}
\end{align}
where we omit parentheses (as defined in Table \ref{definition_pos_neg}) since there is only one positive class and one negative class.

Each threshold represents the decision boundary between negative and positive class (in binary classification) and therefore defines a specific classifier. If the prediction score (e.g. probability) of classifier exceeds the given threshold, the prediction is set to positive class. Otherwise the prediction is a negative class. So each threshold defines FPR and TPR pairs which are then drawn in the 2D visualization.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{roc_prezentace.png}
\caption{This graph depicts examples of ROC curves. In the blue is visualized ROC curve of Random Forest classifier. In the red is visualized ROC curve of random classifier and in the orange is ROC curve of perfect classifier.}
\label{fig:roc}
\end{figure}

By meaningful thresholds are considered values that define different TP, FP, TN and FN combinations, which determine FPR and TPR pairs. Imagine a model classifying 5 instances belonging to 2 different classes (class 0 and class 1) labeled as 0,0,1,1,0. The model outputted probabilities of positive class for these instances as 0.1, 0.6, 0.8, 0.4, 0.1. Then if we choose the decision threshold as 0.2, we obtain TP=2, FP=1, TN=2, FN=0. But same values are obtained also if the decision threshold is set to 0.3, therefore both choices of the decision threshold result in the same FPR and TPR pair. Thus only one threshold is used and the other is redundant. One possible procedure of picking the thresholds would be first to select the unique prediction scores (probabilities), then sort them by size, add the limit values below respectively above the lowest respectively the highest value and finally select one value in each outlined interval.

The example of ROC curves is depicted in the Figure \ref{fig:roc}. Imagine purely random classifier assigning positive class probability randomly (drawn from uniform distribution between 0 and 1) to each instance. Then its ROC curve corresponds to the line connecting points [0,0] and [1,1] (red ROC curve in the Figure \ref{fig:roc}) \cite{roc_analysis}. This can be correctly proven by means of probability, but we can intuitively imagine that each instance is predicted with the positive class probability of 0.5 and therefore there are only two possible thresholds (below and above 0.5), which corresponds to the [0,0] and [1,1] points in ROC space, because once is everything classified as negative and once as positive. Classifiers with a similar ROC curve shape as this straight diagonal line perform similarly as random guessing, which is not very good. On the contrary, a perfect classifier assigning always 1 to positive instances and 0 to negative instances has same shape as the orange ROC curve in the Figure \ref{fig:roc}. This represents the best possible performance that may be depicted in the ROC space. It is generally considered that the closer the classifier is in ROC space to the ideal point [0,1], the better performance it has.

Apparently it could be difficult to compare some curves only visually. Another popular technique to evaluate performance of classifiers is to aggregate ROC curve into single value by computing the area under the ROC curve (AUC) \cite{roc}. Since the ROC curves are closed in the square with the unit side, the maximum value of AUC is 1 and the minimum value is 0. AUC of the random classifier is 0.5 and AUC of the perfect classifier is 1. Usually higher AUC values characterize better performance. AUC value could be also statistically interpreted as the probability that the classifier will give higher score to the randomly chosen positive instance than to the randomly chosen negative instance \cite{roc}. Disadvantage of this method is that even very different ROC curves could reach same AUC values.

\subsubsection{Precision-Recall curve}

The Precision-Recall (PR) curve also graphically visualizes the performance of classification models. It depicts Recall of positive class on the horizontal axis and Precision of positive class on the vertical axis for varying thresholds. It is usually more informative in the case of strongly imbalanced datasets than ROC, where ROC curves could significantly overestimate the performance.

We can demonstrate this issue on the example: consider dataset of 10 positive instances and 1000 negative instances and the classifier which predicts everything correctly except of 100 negative instances, which are predicted as positive. Then TP = 10, FP = 100, TN = 900, FN = 0. Therefore TPR = 1 and $\text{FPR} \approx 0.1$, which reaches almost ideal point [0,1] in ROC space. But Precision of the positive class is only approximately 0.01, which is very poor. So even if the classifier reached almost ideal point in ROC space, in the PR space could be very far from it (ideal point in PR space is [1,1]).

There are some technical properties of PR curves, especially estimating the first and the last point of the graph. The first point corresponds to the case when everything is predicted as negative. Therefore there are no positive predictions and Precision is undefined (denominator is zero). This issue is usually handled by fixing the first point or estimating it from the second point. It is done in the way that the curve starts at the vertical axis. On the contrary, the last point could be determined accurately, because it corresponds to the case of predicting everything as positive. Therefore Recall is 1 and Precision equals $\text{P}/(\text{P}+\text{N})$, where P denotes number of all positive instances and N denotes number of all negative instances.

The PR curves in our experiments start at the [0,1] point on the vertical axis. Also only first point reaching Recall = 1 (point with the highest Precision and Recall = 1) is visualized and other points with the same Recall are omitted, since they do not add any useful information to the graph. The example of the PR curve is depicted in the Figure \ref{fig:pr}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{pr_example.png}
\caption{This graph depicts examples of PR curves. In the blue is visualized PR curve of Random Forest classifier. In the red is visualized PR curve of random classifier and in the orange is PR curve of perfect classifier.}
\label{fig:pr}
\end{figure}

PR curve of the random classifier corresponds to the straight line connecting points [0,1] and [1,$\text{P}/(\text{P}+\text{N})$], whereas PR curve of the perfect classifier corresponds to the straight line connecting points [0,1] and [1,1] (the line connecting points [1,1] and [1,$\text{P}/(\text{P}+\text{N})$] is omitted).

We also compare individual PR curves by a single value (similarly as in the case of ROC curves), but instead of AUC we adopt \textit{Average Precision score} (AP) that is computed as the weighted sum of all Precision values obtained for each threshold. It is defined as

\begin{equation}
    \text{AP} = \sum_{k}(\text{Recall}^{(k)} - \text{Recall}^{(k-1)})\text{Precision}^{(k)} \hspace{0.2cm},
\end{equation}
where $\text{Recall}^{(k)}$ denotes Recall value at $k$-th threshold and analogously for Precision. AP value will be used to compare individual PR curves in the experimental part. It is a preferred choice over AUC since \cite{pr_roc_diff} suggests that AUC may produce over-optimistic evaluation of performance. Generally higher AP values mean better performance.

In \cite{pr_roc_diff} is also proven that ROC and PR curves are closely related, specifically that curve dominates in ROC space if and only if it dominates in PR space (there is one-to-one relationship).

Both ROC and PR curves can be adapted to the multi-class environment. It is usually done by applying one-vs-all scheme (setting of one class as positive and other classes as negative) and then visualizing individual curves. We apply them only on binary classification in the experimental part, hence we will not describe the multi-class case in this thesis.


\chapter{Random Forest}

Random Forest algorithm is a popular machine learning method used primarily on the classification and regression tasks. Its functionality is based on collecting predictions from several independent classifiers (ensemble of decision trees) and merging their outcomes into final prediction. It belongs to the group of supervised learning algorithms. In this thesis is usage of random forests restricted only to the classification tasks, so the theoretical part of this section will be limited only to the random forests used for classification. Main goal is to find a decision function $f: \mathbb{X} \rightarrow \mathbb{R}$, where $X \subset \mathbb{R}^d$ is the input space in the $d$-dimensional real-valued space, which outputs the response (class, label).

Random Forest nowadays is a widespread method exploited frequently in various domains, such as computer vision \cite{computer_vision}, text classification \cite{text_rec}, bioinformatics \cite{bioinf}, network threat detection \cite{random_forest_intrusion} etc. The individual classifiers - decision trees - could be also used as a clustering algorithm [ref Egypt článek].
Popularity of random forests is gained due to excellent generalization and application to wide-ranged spectrum of classification tasks with registering competitive classification quality in comparison with other state-of-the-art methods, such as logistic regression, support vector machines, neural networks etc. Also the method itself is in its basics quite simple and therefore is fast, easy implementable and could be analyzed and interpreted with instant gain of variable importances \cite{rf_imp}.

Unlike the neural networks, it has far less parameters to tune and is better-applicable to small-sample sized datasets. With the usage of randomization, bagging, bootstrapping and other techniques based on randomness, random forests overcome easily problems of overfitting as well \cite{rf_overfit}.

\section{Decision tree}

As was mentioned previously, random forest consists of independent classifiers called decision trees (Figure \ref{fig:dec_tree}).

\begin{figure}[ht]
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{dec_tree.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{dec_tree_graph.png}
\end{minipage}
\caption{In the left part of the figure is illustrated decision tree with depth = 2. The first circle in red color is called root node. Two beige circles in the intermediate layer are inner nodes and green rectangles are leaf nodes, where probabilities of individual classes are stored . Each input vector $\boldsymbol{x} = (x_1,x_2) \in \mathbb{R}^2$ starts from the root node and traverses down the tree by applying subsequent split criterions in nodes until it reaches the leaf node. In the right figure is visualized segregation of input space by the hyperplanes (defined in root node and inner nodes of the decision tree) parallel to the axes (black lines).}
\label{fig:dec_tree}
\end{figure}

In this work we focus only on binary trees, which means, that every node has always 2 child nodes (following nodes in the hierarchical structure of the tree). The process of applying decision tree to obtain final prediction is basically described in Figure \ref{fig:dec_tree}. Beginning from the root node, the input vector $\boldsymbol{x} \in \mathbb{R}^d$ from $d$-dimensional input space is iteratively tested by the split functions associated with every single node in hierarchical order and based on the result is further sent either to the left or right child (the split functions usually define hyperplanes in the input space). This process is repeated until the vector reaches final node (leaf node). In the leaf node are stored classification predictions, which are eventually extracted.

The work with decision tree could be divided into two major parts:

\begin{enumerate}
    \item \textbf{Training stage}: In this stage the entire decision tree is built from the scratch on the basis of predefined criterions and properties. To perform this phase, the \textbf{training dataset} has to be adopted and used. It is usually selected as a subset of the complete data that are available. Magnitude of the training dataset depends on the complexity and application of our model (e.g. if the best model is selected and compared with other models, it should be sufficient to use less data or if the model is prepared for the production, it is desirable to use as much data as possible) and on the complexity of the problem. There does not exist any general consensus that dictates how large should training dataset be. It often depends on the personal preferences and experience and of course on the following evaluation results.
    
    \item \textbf{Testing/application stage}: After training is the decision tree directly applicable and therefore could be either tested with various evaluation methods or applied in practice. The usage of decision tree itself is very straightforward and was already described. We will focus on description of merging the individual results from the ensemble of decision trees and obtaining final predictions.
\end{enumerate}
Now will be provided mathematical details of the both stages.

\subsection{Training stage}

Consider training dataset $\mathbb{X} = \{\boldsymbol{x}_1,...\boldsymbol{x}_N\}$, where $\boldsymbol{x}_k \in \mathbb{R}^d$ and $k \in \{1,...,N\}$, where $N$ is total number of instances in the training dataset and $d$ is the feature space dimension. The goal of the training stage is to establish proper decision tree structure and find suitable split functions in every node that provides the best class segregation. Splitting continues until some of the predefined stopping criterions are met.

Training begins always from the root node. Starting with the training set $\mathbb{M}$ chosen for the current decision tree (could be entire training dataset or its subset), it is interatively splits in each node to the two disjunct subsets $\mathbb{M}_1$ and $\mathbb{M}_2$, where $\mathbb{M}_1,\mathbb{M}_2 \subset \mathbb{M}, \mathbb{M}_1 \cup \mathbb{M}_2 = \mathbb{M}$. $\mathbb{M}_1$ corresponds to the subset of $\mathbb{M}$ sent to the left child and $\mathbb{M}_2$ to the right child. Splitting in each node is done via split functions. The shape of the split functions associated to the nodes is defined in advance. Generally it could be expressed as the binary function

\begin{equation}
    s(\boldsymbol{x},\boldsymbol{\theta}) = \begin{cases}
    0,  & \boldsymbol{x} \text{ is sent to the left child}\\
      1, & \boldsymbol{x} \text{ is sent to the right child}
    \end{cases}
\end{equation}

The parameters of the split functions are the input vector $\boldsymbol{x}$ and parameter vector $\boldsymbol{\theta}$, which defines geometric separation of the data (e.g. hyperplanes). This comprises thresholds, group of data features which will be considered in the split function and other necessary values needed for complete determination of the split function.

In the practice, widely used choice of the split function is \cite{rf_main}

\begin{equation}
    \psi(\boldsymbol{x}, i, \tau) = \begin{cases}
    0,  & x_i < \tau\\
      1, & \text{otherwise}
    \end{cases}
    \label{split_func}
\end{equation}

Parameter vector in this case is $\boldsymbol{\theta} = (i,\tau)$, where $i \in \{1,...,d\}$ represents one feature from the $d$-dimensional feature space and $\tau \in \mathbb{R}$ is a threshold. Together, equation $x_i = \tau$ embodies hyperplane parallel with one axis and split function divides vectors into two groups based on their relative position with respect to this hyperplane. With regard to the fact, that our proposed transformation of the decision tree to the corresponding neural network is build on usage of this special split function \eqref{split_func}, we will not focus on the other types of split functions in this thesis.

In each iteration, algorithm seeks for the best split, ie. for the parameters of the split function that provides this split. It is achieved by optimizing pre-specified split criterion, which expresses measure of (non)impurity in the corresponding node. By impurity is meant that if node contains instances belonging only to one class, the impurity is 0, whereas it is increased if instances of multiple classes are present in the node.

Among widely applied impurity functions belong \textbf{Entropy}, \textbf{Gini index}, which are adopted in the heuristic algorithms as C4.5 \cite{c45}, CART \cite{cart} and ID3 \cite{id3}. Another possibility is a \textbf{Classification error}. They are defined as (in the same order as were mentioned)

\begin{align}
    H_E(\mathbb{M}) &= -\sum_{c \in \mathbb{C}}^{}p(c)\log p(c) \\
    H_G(\mathbb{M}) &= \sum_{c \in \mathbb{C}}^{}p(c)(1-p(c))\\
    H_{CE}(\mathbb{M}) &= 1 - \max_{c \in \mathbb{C}}[p(c)] \hspace{0.5cm},
\end{align}
where the sum iterates over all classes $c \in \mathbb{C}$, $\mathbb{C}$ is a set of all classes, and
\begin{equation}
    p(c) = \frac{\sum_{\boldsymbol{x} \in \mathbb{M}}^{} 1|_{\boldsymbol{x} \in c}}{|\mathbb{M}|} \hspace{0.5cm},
\end{equation}
which is probability of instance in $\mathbb{M}$ belonging to the class $c$. $|\mathbb{M}|$ represents number of instances in set $\mathbb{M}$ and

\begin{equation}
    1|_{\boldsymbol{x} \in c} = \begin{cases}
    1, & \boldsymbol{x} \in c \\
    0, & \text{otherwise}
    \end{cases}
\end{equation}

These impurity functions are in general exploited in the split criterions, which are then optimized in order to obtain the best possible split. By optimizing these criterions should decrease the impurity function value after each split.
There are plenty of possibilities to choose split criterion that suits our problem, but as the most popular choice could be considered \textbf{Information gain} criterion, which is defined as \cite{rf_main}

\begin{equation}
    I(i,\tau,\mathbb{M}) = H(\mathbb{M}) - \sum_{i \in \{1,2\}}^{} \frac{|\mathbb{M}_j(i,\tau)|}{|\mathbb{M}|}H(\mathbb{M}_j(i,\tau))
    \label{inf_gain}
\end{equation}

$H(\mathbb{M})$ is arbitrary impurity function (e.g. entropy, gini index etc.) of set $\mathbb{M}$. $\mathbb{M}_j(i,\tau), j \in \{1,2\}$ represent the subsets of $\mathbb{M}$ proceeding to the left or right child. In parentheses is emphasized, that these subsets strictly depend on parameters $i,\tau$ of the split function. By maximizing information gain in each node are obtained final parameters $i^*, \tau^*$, that are stored and used in the testing stage. Thus parameters of the split function of the $j$-th node are acquired as

\begin{equation}
    \boldsymbol{\theta}_j^* = (i_j^*,\tau_j^*) = \text{arg}\max_{(i,\tau)}I(i,\tau,\mathbb{M}_j)\hspace{0.5cm},
\end{equation}
where $\mathbb{M}_j \subset \mathbb{M}$ is the subset of $\mathbb{M}$ in the $j$-th node.

Basic simulation of finding the best split is visualized in the Figure \ref{fig:split_example}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{hor_ver_split.png}
\caption{In the Figure are illustrated two cases of different splits.}
\label{fig:split_example}
\end{figure}
Values of information gain (IG) for different impurity functions are (Figure \ref{fig:split_example}):
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
       &\textbf{Horizontal split} & \textbf{Vertical split} \\ \hline
         IG (Entropy)& 0.34 & \textbf{0.69}\\ \hline
         IG (Gini)& 0.135& \textbf{0.25}\\ \hline
         IG (Classification Error)&0.20 & \textbf{0.25}\\ \hline
    \end{tabular}
    \caption{Values of IG for splits in Figure \ref{fig:split_example}.}
    \label{tab:ig_vals}
\end{table}
With change of the horizontal split to the vertical split occured increase in values of all studied criterions (see Table \ref{tab:ig_vals}). This suggests that vertical split divides data better than the horizontal split and should be preferred.

Apart of the split function and the split criterion, another necessary feature must be added to the decision tree model in advance. We need to define stopping criterion, which interrupt the growth of the tree in the current node when the specified condition is met.

One natural choice of the stopping criterion is to stop splitting when the corresponding node contains instances belonging to the same class. Therefore there does not make any sense to continue in splitting and growth could be ended. Also it is not possible to continue if there ends only one instance in the node. In practice are frequently used many different criterions, which helps to avoid overgrown trees prone to overfitting. If one of them applies, the growth in the current node is interrupted. As example could be mentioned \cite{pruning}:
\begin{itemize}
    \item \textit{Low information gain}: Lower threshold for the information gain is set. When a subsequent split produces insufficient information gain below specified threshold, the growth is terminated and node becomes a leaf node.
    \item \textit{Maximum depth}: When the number of splits in the current tree branch reaches the maximum depth, the current node becomes a leaf node.
    \item \textit{Maximum instances in the leaf}: This is a threshold for the maximum number of instances in the leaf nodes. When the number of instances in the current node is equal or below this threshold, the node becomes a leaf node.
\end{itemize}

All of these stopping criterions defined in advance belongs to the \textit{prepruning} methods \cite{pruning}. The fact, that these methods are defined in advance embodies certain drawback. For instance, we do not know in advance, if the predefined maximum depth would produce sufficient requirements. Maybe it is too small and the decision tree will tend to underfitting or is too large and the decision tree will tend to overfitting.

For this reason, there could be adopted the \textit{postpruning methods} for the large-depth trees which encounter this issue and solves efficiently the problem of overfitting. There were not applied in our experiments, so we will provide only short summary of these methods \cite{pruning2}.
\begin{itemize}
    \item \textit{Reduced Error Pruning (REP)}: The model is pruned by usage of the independent validation (pruning) dataset different from the training dataset. The subtree starting from the chosen inner node is replaced by the leaf node and the classification error on the validation dataset is measured. If the error is lower than the error in case of the previous non-pruned tree, the tree is pruned. This procedure runs in the bottom-up manner and is ended when no improvement in the classification error is observed.
    \item \textit{Pessimistic Error Pruning (PEP)}:  Unlike the REP method, PEP uses entire training dataset for the pruning procedure. It is based on the continuity correction of the error rate, since the error rate on the training dataset is significantly biased. Details could be found in \cite{pruning2} alongside the descriptions of other applied methods.
\end{itemize}

When the growth of the tree is successfully finished, the class distributions of the instances that ended in each leaf node form a probability distribution of classes, which is stored in the leaf node. For probability $p^j(c)$ of class $c \in \mathbb{C}$ in the leaf node $j \in \{1,...,L\}$, where $L$ is a total number of leaf nodes applies
\begin{equation}
    p^j(c) = \frac{n^j_c}{N^j} \hspace{0.5cm},
\end{equation}
where $n^j_c$ is a total number of instances of class $c$ in the leaf node $j$ and $N^j$ is a total number of instances in the entire leaf node $j$.

\subsection{Testing/application stage}

After the training stage, the decision tree is obtained and could be applied in practice. Starting from the root node, the testing instance $x \in \mathbb{X} \subset \mathbb{R}^d$ is traversed down the tree by iterative application of the split functions associated with individual nodes. When the instance reaches the leaf node, the probability distribution of classes in the leaf node is extracted and the values are observed. The class which has the maximum value is outputted as the final prediction. Mathematically, the output of the single decision tree is

\begin{equation}
    c^* = \text{arg}\max_{c \in \mathbb{C}}p^j(c) \hspace{0.5cm},
\end{equation}
where $j$ is the leaf node where $x$ ended.

\section{Ensemble of decision trees}

Ensemble of decision trees forms a strong classifier with higher predictive ability than individual decision trees. Idea behind this is to combine several weak classifiers to create one strong classifier. Ensembles generally boost the performance in the comparison with the single classifier (decision tree). For instance, there is a chance of single classifiers to get stuck in the local optima when optimizing different criterions (e.g. minimizing error rate). With more and more data, it could be for many classifiers computationally unreachable to find the best optimal value. In such cases, the employment of ensemble methods is valuable, because training of independent classifiers and merging their output could approximate real prediction function far better \cite{ensemble}.

Ensemble methods also significantly benefit from the averaging of results from individual classifiers, especially in the case of insufficient number of training data points. In such case, the learning algorithm could find many applicable parameters (e.g. hyperplanes of the decision tree) that will result in the same accuracy on the training dataset. Combining those models could suppress the possibility of picking a wrong classifier \cite{ensemble}.

Among popular techniques to create an ensemble of decision trees belong:
\begin{itemize}
    \item \textbf{Bagging}: This technique divide the training datasets into several subsets with replacement (bootstrapping) and each subset is used to train the decision tree. Then the ensemble of the decision trees is obtained and the result from individual decision trees is combined. This technique helps to reduce variance of a single decision tree \cite{variance_rf}  (variance means, that if the data are randomly split more times and resulting models are compared, they could vary a lot (unlike the linear regression for example, which has generally low variance)).
    \item \textbf{Boosting} This method is based on idea of iterative boosting of weak classifiers into a strong one. In each iteration, the algorithm aims to boost previous-staged classifier by checking of the misclassified instances and attempting to fix these errors. As popular boosting algorithm could be mentioned AdaBoost algorithm \cite{adaboost2}. These methods often tend to overfitting problem \cite{adaboost}.
\end{itemize}

In the experimental part of this thesis is used entire training dataset for tuning individual decision trees, thus it is a variation on the bagging technique with bootstrapping of $N$ instances, where $N$ is a total number of instances in the training dataset. To obtain random forest from the ensemble obtained by bagging algorithm, the \textit{randomization} of single trees needs to be added into the process. This technique helps to avoid correlation between individual decision trees and secures better indepedence of classifiers. An idea is to suppress the influence of strongly distinguishing features (dimensions), that are natural choice to split by. Occurrence of these features could result in the correlation of the decision trees, because algorithm would prefer those features especially in the early stages of growing phase and therefore indicates detectable similarity of the trees. Randomization attempts to obviate this problem and selects some random subset of the features in each splitting iteration and only those selected features are used for the split.

\subsection{Combining predictions of the decision trees}

As was mentioned previously, from every decision tree could be extracted either probability distribution of classes from the leaf node or directly predicted class label. There are naturally numerous possibilities of merging these results. Unless there is not special requirement on the model, practically are used two ways. If the outputs are discrete class labels, than the voting scheme is applied and the class with maximum votes is taken as final prediction. If the outputs are probability distributions, those values are averaged and the class with maximum probability is taken. Mathematically noted, the final prediction in this case is

\begin{equation}
    c^* = \text{arg}\max_{c in \mathbb{C}}\frac{1}{|T|}\sum_{t \in \mathbb{T}}p_t(c) \hspace{0.5cm},
    \label{final_pred}
\end{equation}
where $T$ is a set of all trees in the random forest and $p_t(c)$ is a resulting probability distribution from the tree $t \in \mathbb{T}$. In \eqref{final_pred} could be also a weighted average instead, which is applied e.g. if predicting some class is more preferred over predicting others.


\chapter{Neural Networks}
\section{Introduction to Neural Networks (NN)}

For thousands of years, the people have been pursuing the dream to perfectly simulate human or animal brains and adopt their ability to solve problems on the daily basis. For this purpose, the (artificial) neural network was developed and nowadays is frequently used for solving difficult data modelling tasks, statistical analysis and many others. Its functionality is based on the simplified version of real brain information processing and reasoning, where single processing units of brain - neurons - transfer information to other neurons in predefined structure and get activated based on activity status of other adjacent neurons. In the actual brain, the signal from the neuron is transferred to the connected neuron by the synapse. When the neuron accumulates all coming signals from synapses, then it evaluates this coupled signal and if it exceeds the threshold, the neuron sends signal through synapse (connection) to other neuron. This principle is basically what fuels artificial neural networks today.

The beginnings of the artificial neural network dates to early 1950s, almost simultaneously with development and application of programmable electronic units. In 1943, Warren McCulloch and Walter Pitts introduced models inspired by real brain neurons and adopted threshold switches \cite{culloch}. They provided the evidence that even simple model (network) motivated by this approach could compute almost any logic or arithmetic function. In 1957-1958, Frank Rosenblatt and Charles Wightman et al introduced the first successful neurocomputer, Mark I Perceptron, which could identify 20x20 pixel images of simple patterns. In 1959, Frank Rosenblatt covered different version of the perceptron and formulated and proved the convergence theorem of the perceptron (perceptron will be described later). Then Marvin Minsky and Seymour Papert published mathematic paper that studied perceptron in 1969. The conclusion of this paper pointed to the insufficiency of the single perceptron, which was not able to represent classical boolean XOR or sets that are not linearly separable \cite{perceptron}. This led to the approximately 15-year black-out in the field of the neural network research. In 1974 was introduced learning algorithm called backpropagation of error by Paul Werbos \cite{werbos}. Suprisingly, this algorithm was fully acknowledged approximately ten years later. The backpropagation of error as learning procedure was further developed and expanded in 1986 and widely published by the Parallel Distributed Processing Group \cite{backprop-cit}. In that time, the non-separability could be effectively solved by multilayer perceptrons and previous negative conclusions about perceptrons were disproved right away \cite{perceptron-nonsep}. Since then, the development in the field of neural networks and machine learning in general proceeded intensively and actively continues. It is not even possible to mention all milestones in this thesis. Many different neural networks architectures and training algorithms were tested, resulting in complex deep learning structures able to handle massive amount of unlabeled data. They are sucessfully applied in many challenging tasks as bioinformatics, speech recognition, natural language processing etc \cite{nn-application}.  Special NN called Convolutional Neural Networks (CNN) are frequently used especially in the field of image processing.

\section{Building blocks of ANN}

In this section will be described main components of artificial neural network. As we mentioned in the introductory section, architecture of neural network consists of the processing units - neurons, and the connections between neurons. General neuron is associated with the so called \textit{propagation function}, \textit{activation function} and \textit{output function}. These functions process internally the input from other neurons and builds the output of neuron. Propagation function firstly considers all inputs to the neuron and output from the propagation function serves as input for the activation function. The activation function transforms this information into activation value of current neuron and this activation values is then transformed by output function and output is sent to by the connection to the following neuron. Entire process is illustrated in the Figure \ref{fig:single_neuron}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{single_neuron.png}
\caption{Processing of input and creating output of neuron could be formulated as composition of three functions - propagation function, activation function and output function.}
\label{fig:single_neuron}
\end{figure}

As propagation function is almost solely used simple weighted sum of all inputs generally with addition of a constant (called bias). Consider input as vector (if single neuron is connected by the connections from multiple neurons) $\boldsymbol{x} = (x_1,x_2,...,x_k)$, where $k \in \mathbb{N}$ is number of inputs to neuron and $b \in \mathbb{R}$ is a bias and $\boldsymbol{w} = (w_1,...,w_k) \in \mathbb{R}^k$ are connection weights used for weighted sum. Then output $z \in \mathbb{R}$ from propagation function $f: \mathbb{R}^k \rightarrow \mathbb{R}$ is

\begin{equation}
    z = f(\boldsymbol{x}) = \sum_{i=1}^{k}(w_i \cdot x_i) + b   
\end{equation}


The term \textit{weights} is crucial for neural network terminology. Weights are usually associated with the connections and bias term is associated with the neuron. It depends mainly on personal preference how to imagine and comprehend the neural network settings.

As the output function of neuron serves in the most cases simple identity function. This thesis will not focus on details of other possible choices of propagation and output functions, since in the practice are weighted sum for propagation function and identity function for output function common choice and other options are very rarely used.

With these presumptions on propagation and output function, the output of neuron is fully controlled by activation function, which takes as input the weighted sum of input values with added bias. For arbitrary activation function $\phi$ is the procedure visualized in the Figure \ref{fig:perceptron}.


\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{perceptron.png}
\caption{Sum of input values $x_1,...,x_k$, where $k \in \mathbb{N}$ weighted by the weights $w_1,...,w_k$ of connections with added bias term $b$ is taken as argument of arbitrary activation function $\phi$. Resulting value of activation function embodies amount of activity produced by neuron. Activity value of neuron is taken as single output of neuron and could be transferred to other connected neurons.}
\label{fig:perceptron}
\end{figure}


The bias term might be thought as the negative threshold of neuron ($b = - \text{threshold}$). When the weighted sum exceeds the absolute value of the threshold, it gets activated. This discrete behaviour is perfectly simulated by special neuron - perceptron. Its activation function is so called threshold function $\tau : \mathbb{R} \rightarrow \mathbb{R}$ with binary output
\begin{equation}
    \tau(z) = \begin{cases}
    1 & z > 0 \\
    0 & \text{otherwise}
    \end{cases} \hspace{0.5cm}
\end{equation}

If the argument of $\tau$ is positive (threshold value for this neuron is 0), the output is 1, otherwise it is 0. Even this simple model is applicable to many different tasks, but reliably solves only problems with linearly-separable data. This is due to the determinative equation $\sum_{i=1}^{k}(w_i \cdot x_i) + b = 0$, which defines a hyperplane in $\mathbb{R}^k$ and divides the space on two half-spaces. This drawback was surpassed by application of multilayer perceptrons, which is term for interconnected layers of multiple perceptrons (neural network with perceptrons).

\section{Architecture of neural network}

Since we defined a general shape of one processing unit of neural network - neuron, we could define more complex architectures of neurons that helps to solve more complex, non-linear problems. In this thesis we mainly focus on the so called \textbf{feedforward} neural networks \cite{feedforward}, which almost exclusively refers to the topology of multiple layers of neurons and weighted connections only between neurons of two consecutive layers (no skipping allowed). Furthermore, the first layer is referred as \textit{input layer}, then follow arbitrary number of \textit{hidden layers} and the last layer is called the \textit{output layer}. General architecture of the feedforward neural network is depicted in the Figure \ref{fig:feedforward}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{feedforward.png}
\caption{In the Figure is illustrated general architecture of feedforward neural network. Connections are only between neurons of two consecutive layers. Input (each neuron usually receives value of one feature - neuron $j \in \mathbb{N}$ of the input layer accepts value of $j$-th feature $x_j$ of input vector $\boldsymbol{x}$) of the neural network is inserted to the input layer and the output is extracted from the output layer.}
\label{fig:feedforward}
\end{figure}

The universal approximation theorem proved by G. Cybenko \cite{cybenko} states that under some presumption on the activation functions of neuron (e.g. with usage of sigmoid function, this will be described later), this neural network structure could approximate any continuous function on compact subsets of $\mathbb{R}^n$. Also later studies approves, that that the multilayered (even with only one hidden layer) feedforward neural networks could be considered as universal approximators.

When speaking about feedforward neural networks, we usually mean full-connected system, which means, that single neuron in layer $l$ is connected to all neurons in layer $l+1$. If it is necessary to turn down the neurons output to 0 (delete the connection), we force the connection weight to 0 and therefore supress the output of the neuron.

There exists another possible architectures of neural networks, which will be not examined in this thesis, but certainly worth mentioning them.

\begin{itemize}
    \item \textbf{Shortcut connections architecture}:
    This feedforward setting allowed connections skipping of one or more levels. These skipped connections have to be directed towards the output layer (no backwards skipping). Application of this architecture was successfully exploited for example in residual networks (ResNet) \cite{resnet}.
    
    \item \textbf{Direct recurrence networks}: Some neuron in networks could influence themselves. The simple realization is to connect single neuron with itself, which may therefore adaptively strenghten or weaken its activation value.
    
    \item \textbf{Indirect recurrence networks} If connections are permitted backwards (to the foregoing layers), we talk about indirect recurrence \cite{perceptron-nonsep}. 
\end{itemize}

\subsubsection{Insufficiency of perceptron}

When working with neural networks, usual procedure is to adapt all parameters of neurons (weights and biases) to force neural network to behave in the way we require. This procedure is done by employing predefined learning strategy (it will be described later in the thesis), which defines the manner in which neural networks learn on the data and then act independently.

Consider for a moment a feedforward neural network consisting of perceptrons with $\tau$ as activation function and consider arbitrary classification problem (for example classification of digits 0-9). If the neural network encounter training instance, it adapts the weights and biases in the way that the instance is then correctly classified. But even in the case of small change in the weights and biases, the output from $\tau$ function could flip vigorously to ($1 - \text{previous value}$) , so entirely to the other type of activity of neuron. This jump may cause the previous instances to be totally misclassified. So the discontinuity property of threshold function $\tau$ makes it very difficult for neural network to learn correct parameters, especially when solving more complicated, multiclass problems.

To enable better learning ability of neural networks, it is desirable to adopt the property that small change in the weights or biases $\Delta w$ and $\Delta b$ causes only a small change in the output of neuron \cite{feedforward}. This is the property that is not met by $\tau$ function. It is the key to employ other types of activation functions that meet this property in order to establish effective learning procedures.

\section{Activation functions}

In this section is provided summary and descriptions of other applied activation functions.

\subsubsection{Sigmoid function}

One way of solving a discontinuity problem of perceptron is to replace $\tau$ with its smooth approximation - sigmoid function. It is also called logistic function. The shape of this function could be seen in the Figure \ref{fig:sigmoid}. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{sigmoid.png}
\caption{Sigmoid function approximates threshold function $\tau$.}
\label{fig:sigmoid}
\end{figure}
For real input $z \in \mathbb{R}$ is sigmoid function $\sigma$ defined as

\begin{equation}
    \sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}
This function is bounded, nonlinear, differentiable and has positive derivatives. It is successfully applied especially in the output layers of deep neural networks (output lies within (0,1) range, so in the case of classification it could be thought as the probability of the instance acquiring particular label) or in the shallow neural networks (with 1 or max. 2 hidden layers) \cite{actfunc}. Despite some of its favourable properties, it suffers from significant drawbacks related to popular training methods. These drawbacks will be mentioned later once the gradient descent and backpropagation algorithm for training will be described.

\subsubsection{Hyperbolic tangent}

Hyperbolic tangent (tanh) is another alternative as activation function. The shape of the tanh is depicted in the Figure \ref{fig:hyperbolic}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{hyperbolic_tangent.png}
\caption{Hyperbolic tangent function. It has conformable shape as sigmoid function, but its range lies within (-1,1).}
\label{fig:hyperbolic}
\end{figure}

For real input $z \in \mathbb{R}$ is defined as

\begin{equation}
    tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\end{equation}

Hyperbolic tangent has very similar properties as sigmoid function, but inherents some more beneficial properties, e.g. its range lies within (-1,1) and therefore is zero-centered, which produces advantegous behaviour in backpropagation training. Also tanh became preferred choice over sigmoid function in the case of multilayer neural networks due to showing better training performance \cite{tanh_prefer}.

On the other hand, it shares few common drawbacks with the sigmoid function, e.g. vanishing gradient problem, which will be mentioned and described later in the thesis.

\subsubsection{Softmax}

Last of the exponential-based activation functions mentioned in this section is called softmax activation function $\boldsymbol{\theta} : \mathbb{R}^k \rightarrow \mathbb{R}^k$. It is defined for input vector $\boldsymbol{a} \in \mathbb{R}^k$ and $j \in \{1,...,k\}$, where $k \in \mathbb{N}$ as

\begin{equation}
    [\boldsymbol{\theta}(\boldsymbol{a})]_j = \frac{e^{a_j}}{\sum_{i=1}^{k}e^{a_i}}
\end{equation}

Softmax activation function is frequently used especially in the output layers of neural networks, because its definition secures that output vector of softmax is interpretable as probability distribution (output components can be added to 1 and range of softmax is (0,1)).

\subsubsection{Rectified Linear Unit (ReLU)}

ReLU activation function was indtroduced in 2010 by Nair and Hinto \cite{relu} and since then belongs to the most popular activation functions worldwide with state-of-the-art results. It is the most widely used activation function in the field of deep learning. ReLU function is visualized in the Figure \ref{fig:relu}

\begin{figure}[ht]
\begin{minipage}[b]{0.48\linewidth}
\centering
\includegraphics[width=\textwidth]{relu.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.48\linewidth}
\centering
\includegraphics[width=\textwidth]{leakyrelu.png}
\end{minipage}
\caption{In the left part of the Figure is graph of ReLU function. In the right part is graph of Leaky ReLU for $\alpha = 0.05$.}
\label{fig:relu}
\end{figure}


It is defined for real input $z \in \mathbb{R}$ as
\begin{equation}
    \text{ReLU}(z) = \max\{0,z\}
\end{equation}
For positive values of x it behaves as simple linear function and for negative values the output value remains 0. Its similarity with simple linear functions makes it easy to optimize learning algorithms and therefore is training of ReLU neurons more effective and quicker (it need not count exponentials and divisions). The generalization ability, speed of convergence and often better performance with comparison to the sigmoid and tanh functions cause that ReLU functions are much more often used mainly in the hidden layers than those competitive exponentially-based activation functions \cite{actfunc}. Also ReLU function lacks vanishing gradient drawback, which will be discussed later.

ReLU function also suffers from drawbacks, such as easy inclination to overfitting in comparison with the sigmoid function and production of the so called "dead" neurons. It is inflicted when argument of ReLU is negative and ReLU outputs 0. With regard to the fact, that slope of ReLU in the negative part is also 0, it is probable that output of this neuron will stuck on 0 a do not return back. This neuron has no further effect in determining the output and is useless. This problematic will be covered after description of gradient-based learning methods.

Problem of dead neurons could be solved by adoption of the Leaky ReLU activation function \cite{leaky}, which softens the strict condition of forcing output to 0 when input is negative and replaces constant 0 value in the negative part of the horizontal axis with the linear function of small slope parameter $\alpha \in \mathbb{R}$. Therefore the gradient will never be 0. It is defined for real input $z \in \mathbb{R}$ as
\begin{equation}
    \text{LeakyReLU}(z) = \begin{cases}
    z & z > 0 \\
    \alpha z & \text{otherwise}
    \end{cases}
\end{equation}
Values of parameter $\alpha$ should be small, popular choice is $\alpha = 0,01$. Shape of Leaky ReLU could be seen in the right part of the Figure \ref{fig:relu}.

\section{Training methods}

We already sufficiently define neural networks architecture and the way it works by transferring activation values between neurons. What remains is to describe the algorithms which enable our neural network to learn and adapt to new problems.

Let us define some notation in order to describe this issue more clearly. The notation is inspired by \cite{feedforward}. We denote correct (desired) output of neural network as $\boldsymbol{y}(\boldsymbol{x})$ for input vector $\boldsymbol{x}$ and real output (estimation) of the neural network as $\hat{\boldsymbol{y}}(\boldsymbol{x})$. The quality of performance is evaluated by the means of minimizing the so called \textit{loss function} (in the literature could be found also as objective function or cost function). We denote general loss function as $\Lambda$. One typical example of loss function is \textit{Mean Squared Error} (MSE), also known as quadratic cost function,which is defined as
\begin{equation}
    \Lambda(\mathbb{W},\mathbb{B},\mathbb{X}) = \frac{1}{2n} \sum_{i=1}^{n}||\boldsymbol{y}({\boldsymbol{x}}_i)- \hat{\boldsymbol{y}}({\boldsymbol{x}}_i)||^2 \hspace{0.5cm},
    \label{mse}
\end{equation}
where $\mathbb{W}$ is a set of all weights, $\mathbb{B}$ is a set of all biases and $\mathbb{X} = \{\boldsymbol{x}_1,...,\boldsymbol{x}_n\}$ is a set of all training vectors, where $n \in \mathbb{N}$ is a number of all training vectors. Norm $||\cdot|| = ||\cdot||_2$ is Euclidean norm. It is evident that estimation $\hat{\boldsymbol{y}}({\boldsymbol{x}})$ depend on $\boldsymbol{x}$ as well as on all weights and biases in the neural network. Later we will mention other frequently used loss functions with better application especially to the task of classification.

Now will be described the most popular algorithm used for finding optimal weights and biases, which optimize the general loss function $\Lambda$ - \textit{gradient descent}.

\subsection{Gradient Descent}

Gradient descent algorithm is a popular nonlinear programming technique for a wide range of minimization tasks. It is considered as a benchmark for the minimization of loss functions in the field of neural networks. Let us consider general loss function $\Lambda(\boldsymbol{v})$ depending on vector $\boldsymbol{v}$. Gradient descent constructs the sequence of $\boldsymbol{v}^{(k)}$, that for each $k \in \mathbb{N}$ satisfies the condition $\Lambda(\boldsymbol{v}^{(k+1)}) < \Lambda(\boldsymbol{v}^{(k)})$ (except when $\boldsymbol{v}^{(k)}$ is optimal). The sequence is constructed as
\begin{equation}
    \boldsymbol{v}^{(k+1)} = \boldsymbol{v}^{(k)} + \eta\Delta \boldsymbol{d} ^{(k)} \hspace{0.5cm},
\end{equation}
where $\Delta \boldsymbol{d}^{(k)}$ is a real vector called the step or search direction, with same dimension as $\boldsymbol{v}^{(k)}$ and $\eta > 0$ is called a learning rate \cite{grad}. Question is how to choose $\Delta \boldsymbol{d}^{(k)}$ in order to fulfill the condition $\Lambda(\boldsymbol{v}^{(k+1)}) < \Lambda(\boldsymbol{v}^{(k)})$.

We must choose $\Delta \boldsymbol{d}^{(k)}$ in the way the $\Lambda$ function decreases in that direction. We could apply the calculus and search the direction in the way that the following condition is met:
\begin{equation}
    \nabla_{\Delta \boldsymbol{d}^{(k)}} \Lambda(\boldsymbol{v}^{(k)}) = \Delta \boldsymbol{d}^{(k)} \cdot \nabla \Lambda(\boldsymbol{v}^{(k)}) < 0
    \label{condition_calculus} \hspace{0.5cm},
\end{equation}
where $\nabla \Lambda(\boldsymbol{v}^{(k)})$ is gradient of function $\Lambda$ and is defined for vector $\boldsymbol{v} = (v_1,...,v_m)^T$, $m \in \mathbb{N}$ as

\begin{equation}
    \nabla \Lambda(\boldsymbol{v}) = (\frac{\partial \Lambda}{\partial v_1}(\boldsymbol{v}),...,\frac{\partial \Lambda}{\partial v_m}(\boldsymbol{v}))^T.
\end{equation}
Condition \eqref{condition_calculus} expresses derivative of $\Lambda(\boldsymbol{v}^{(k)})$ in the direction of $\Delta \boldsymbol{d}^{(k)}$. If this derivative is negative, we ensures that $\Lambda$ decreases in that direction.

Suppose choosing $\Delta \boldsymbol{d}^{(k)} = - \nabla \Lambda(\boldsymbol{v}^{(k)})$. Then
\begin{equation}
    \Delta \boldsymbol{d}^{(k)} \cdot \nabla \Lambda(\boldsymbol{v}^{(k)}) = - ||\nabla \Lambda(\boldsymbol{v}^{(k)})||^2 \leq 0
    \label{condition_full}
\end{equation}

Moreover, equality in \eqref{condition_full} is held only if $\nabla \Lambda(\boldsymbol{v}^{(k)}) = \boldsymbol{0}$ and therefore no further update is performed, so the algorithm could be terminated. In the practice, euclidean norm of gradient is compared to the small positive threshold $\epsilon$ and algorithm is terminated when $||\nabla \Lambda(\boldsymbol{v}^{(k)})|| \leq \epsilon$ \cite{grad}.

Final equation for constructing the sequence is therefore

\begin{equation}
    \boldsymbol{v}^{(k+1)} = \boldsymbol{v}^{(k)} - \eta\nabla \Lambda(\boldsymbol{v}^{(k)})
\end{equation}

To provide more intuitive perspective, gradient descent algorithm iteratively searches for directions in which the value of loss function is decreasing and stops when it is sufficiently close to optimum (could be local or global minimum). The initial (starting) point of algorithm need to be selected in advance. It is usually picked randomly. The procedure is illustrated in the Figure \ref{fig:grad}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{grad2_desc.png}
\caption{Gradient descent algorithm. In each iteration it finds the direction in which loss function decreases. In this case, the green point is "rolling down" towards the global minimum $(0,0)^T$.}
\label{fig:grad}
\end{figure}
Of course, gradient descent is not universal method which always find global minimum of the arbitrary function. There are some requirements on loss function to ensure functionality. Especially when the loss function is not "nice" as one in the Figure \ref{fig:grad}, it could stuck in the local minimum instead or even do not converge, which also depends on the initial point of algorithm. Theory behind the convergence to global optimum is properly described almost exclusively for (strictly) convex functions \cite{grad}.

Also the learning rate $\eta$ significantly affects convergence. If it is too small, the time until convergence could be very long and if it is too big, it could bounce back and forth around the optimal value and never reach it. Deriving the most suitable learning rate for particular application usually requires a bit of experimenting around. There also exists methods to set learning rate or adjust it during execution. Some of them has been used in the experimental part of this thesis and will be mentioned later.

We should concretize the update rule in the case of modifying weights and biases of neural network by gradient descent. Update rules for arbitrary weight $w$, bias $b$ and $k$-th iteration of gradient descent is
\begin{align}
    w^{(k+1)} &= w^{(k)} - \eta \frac{\partial \Lambda}{\partial w}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X}) \\
    b^{(k+1)} &= b^{(k)} - \eta \frac{\partial \Lambda}{\partial b}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X})
\end{align}

\subsection{Stochastic Gradient Descent (SGD)}

In order to accelerate learning of neural network, \textit{stochastic gradient descent} algorithm was developed \cite{feedforward}. It is based on estimating the gradient of loss function instead of computing it precisely.
For instance, we show the algorithm in the case of quadratic cost function in \eqref{mse}. It could be rewritten as
\begin{align}
    \Lambda(\mathbb{W},\mathbb{B},\mathbb{X}) &= \frac{1}{n}\sum_{i=1}^{n}\Lambda_{\boldsymbol{x}_i} = \frac{1}{n}\sum_{i=1}^{n}\frac{||\boldsymbol{y}(\boldsymbol{x}_i) - \hat{\boldsymbol{y}}(\boldsymbol{x}_i)||^2}{2}\\ \Lambda_{\boldsymbol{x}_i} &= \frac{||\boldsymbol{y}(\boldsymbol{x}_i) - \hat{\boldsymbol{y}}(\boldsymbol{x}_i)||^2}{2}
\end{align}
Therefore, gradient of loss function could be written as (we omit the parameters of $\Lambda$ for simplification)

\begin{equation}
    \nabla \Lambda = \frac{1}{n}\sum_{i=1}^{n} \nabla \Lambda_{\boldsymbol{x}_i}
    \label{good_shape}
\end{equation}
Basically, this term is only average of gradients of $\Lambda_{\boldsymbol{x}_i}$ across all training instances. Idea of stochastic gradient descent is to replace averaging across all training instances with averaging only across randomly sampled mini-batch \\ $x_{i_1},...,x_{i_m}$ of size $m < n$, $m \in \mathbb{N}$. We obtain estimate
\begin{equation}
    \nabla \Lambda = \frac{1}{n}\sum_{i=1}^{n} \nabla \Lambda_{\boldsymbol{x}_i} \approx \frac{1}{m}\sum_{l=1}^{m} \nabla \Lambda_{\boldsymbol{x}_{i_l}}
\end{equation}

If the size of mini-batch is large enough, we could expect that this approximation will be roughly equal to the real gradient. This algorithm works for loss functions that could be rewritten in the same manner as quadratic cost function from the example. Of course, there could be some statistical deviations and the estimation is not always precise, but what we need is to move in direction where the loss function is expected to decrease, it need not to be exactly the direction of gradient, so working with estimation is justified. We shall rewrite the update rule in the case of SGD as
\begin{align}
    w^{(k+1)} &= w^{(k)} - \frac{\eta}{m}\sum_{l=1}^{m} \frac{\partial \Lambda_{\boldsymbol{x}_{i_l}}}{\partial w}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X})
    \label{sgd_weight}\\
    b^{(k+1)} &= b^{(k)} - \frac{\eta}{m}\sum_{l=1}^{m} \frac{\partial \Lambda_{\boldsymbol{x}_{i_l}}}{\partial b}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X})
    \label{sgd_bias}
\end{align}

The algorithm in practice usually runs in several epochs. Each epoch is characterized by random sampling of mini batches of given size $m$ and then by training with those mini batches. When one epoch is finished, training instances are randomly mixed and mini batches are sampled again and training continues in new epoch with new mini batches.

In \eqref{sgd_weight} and \eqref{sgd_bias} could be omitted $\frac{1}{m}$ factor, which corresponds only to the scaling of learning rate. Factor $\frac{1}{n}$ is sometimes omitted also in definition of MSE in \eqref{mse} \cite{feedforward}.

\subsection{Other training methods}

Altough gradient descent or SGD are both popular optimization algorithms applied in many optimization tasks (especially in the training phase of neural networks), they are not entirely flawless. In some situations they could report drawbacks which can make a learning procedure much more difficult. Especially in the means of the speed of convergence and stucking in sub-optimal (local) minima. Moreover, the overall sensitivity to the learning rate parameter is in the case of gradient descent and SGD very high, which also affects the convergence and poor choice of learning rate can even lead to divergence.

To address these issues, several alternatives to the SGD have been proposed \cite{adam}. They target to accelerate the convergence and reduce oscillations. Some of them also adapt the learning rate parameter at the runtime and thus possess better ability of controlling the direction and amount of change.

\subsubsection{SGD with momentum}

This method helps to accelerate the convergence of classic SGD and reduce fluctuations around optimal value, which is common for SGD for instance in the valley areas with only slight slope of decrease in the bottom and steep slopes on sides (basically different slopes in different dimensions). The example is depicted in the Figure \ref{fig:momentum}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{momentum_sgd.png}
\caption{In the left figure is depicted situation where classic SGD algorithm is applied. In the right figure is the case of SGD with momentum.}
\label{fig:momentum}
\end{figure}

SGD with momentum works in the way that it gradually accumulates fractions of previous update vectors and use it for a new update. The update rule is defined with usage of momentum parameter $\gamma, 0 \leq \gamma \leq 1$, which represents the fraction of previous update vector, and velocity vector at $k$-th iteration $\boldsymbol{s}^{(k)}$, which represents update vector at $k$-th iteration ($\boldsymbol{s}^{(0)} = \boldsymbol{0}$). For arbitrary loss function $\Lambda = \Lambda(\boldsymbol{v})$ (depending on vector $\boldsymbol{v}$, which could represent for instance weights and biases) and learning rate $\eta > 0$, SGD with momentum is defined as

\begin{align}
    \boldsymbol{s}^{(k+1)} &= \gamma\boldsymbol{s}^{(k)} + \eta \nabla\Lambda(\boldsymbol{v}^{(k)}) \\
    \boldsymbol{v}^{(k+1)} &= \boldsymbol{v}^{(k)} - \boldsymbol{s}^{(k+1)}
\end{align}

This mechanism could be also interpreted in the physical point of view. Imagine the ball rolling down the hill. It gradually accelerates and accumulates momentum, approaching the bottom faster and faster. It is afterwards more difficult to change suddenly its direction of movement by some lateral impulses and conversely the impulses in the direction of movement can increase the speed rapidly. Same reasoning could be applied to our case of optimization, where the effect of gradients that are deviated from the direction of movement is suppressed. This secures faster convergence and smaller fluctuations.

The magnitude of momentum parameter $\gamma$ controls the level of considering previous update vectors. The higher it is, the stronger influence of accumulated momentum is considered and it is harder for new gradients to change its direction.

Even though this algorithm possesses these improvements over SGD, it sometimes acts too thoughtlessly. It is intuitively apparent that in some situations it may be beneficial to slow down and change the direction significantly, not only follow the direction of accumulated gradients. From this reason the \textbf{Nesterov algorithm} has been developed. It is a smarter version of SGD with momentum, which roughly anticipates the future position and exploits the gradient computed in the estimated future point instead of gradient in the current point. We can roughly estimate future points as $\boldsymbol{\hat{v}^{(k+1)}} = \boldsymbol{v}^{(k)} - \gamma\boldsymbol{s}^{(k)}$, thus Nesterov algorithm computes new update vector $\boldsymbol{s}^{(k+1)}$ with usage of the gradient evaluated in this estimated point. The equations of SGD with momentum are modified to

\begin{align}
    \boldsymbol{s}^{(k+1)} &= \gamma\boldsymbol{s}^{(k)} + \eta \nabla\Lambda(\boldsymbol{v}^{(k)} - \gamma\boldsymbol{s}^{(k)}) \\
    \boldsymbol{v}^{(k+1)} &= \boldsymbol{v}^{(k)} - \boldsymbol{s}^{(k+1)} \hspace{0.2cm}.
\end{align}

\subsubsection{Adam}

Adam (Adaptive Moment Estimation) belongs to the class of optimization algorithms that adapts the learning rate for each parameter during runtime. Currently Adam is considered as a member of state-of-the-art optimization methods with wide scope of application in the context of neural networks.

The theory behind Adam is slightly complicated and not that intuitive as in the case of previous methods. We only provide a short introduction and the detailed description and analysis could be found in \cite{adam3}.

Adam at $k$-th iteration of algorithm updates exponential moving average of gradient $\boldsymbol{m}^{(k)}$, which estimates the first moment (the mean) of the gradient, and exponential moving average of squared gradient $\boldsymbol{r}^{(k)}$, which estimates the second raw moment (the uncentered variance) of the gradient. It turned out that these terms are biased towards zero, so afterwards they are bias-corrected and applied to update the parameters. The algorithm is parameterized by decay rates $0 \leq \gamma_1 < 1$ and $0 \leq \gamma_2 < 1$. The lower they are, the more the moving average is shifted towards the current values. There is also a special term $\epsilon > 0$, which helps to avoid division by zero. For simplification of notation we denote gradient of loss function $\Lambda = \Lambda(\boldsymbol{v})$ with respect to $\boldsymbol{v}$ at $k$-th iteration as $\boldsymbol{g}^{(k)}$ (\boldsymbol{v} generally symbolizes any vector of parameters in the current scope of interest). Finally, for learning rate $\eta$, the complete algorithm could be written as (all vector operations are element-wise):

\begin{align}
    \boldsymbol{m}^{(k+1)} &= \gamma_1 \boldsymbol{m}^{(k)} + (1-\gamma_1)\boldsymbol{g}^{(k)} \label{mean}\\
    \boldsymbol{r}^{(k+1)} &= \gamma_2 \boldsymbol{r}^{(k)} + (1-\gamma_2)[\boldsymbol{g}^{(k)}]^2 \label{variance}\\
    \hat{\boldsymbol{m}}^{(k+1)} &= \frac{\boldsymbol{m}^{(k+1)}}{1 - \gamma_1^{k+1}} \label{correct1}\\
    \hat{\boldsymbol{r}}^{(k+1)} &= \frac{\boldsymbol{r}^{(k+1)}}{1 - \gamma_2^{k+1}} \label{correct2}\\
    \boldsymbol{v}^{(k+1)} &= \boldsymbol{v}^{(k)} - \frac{\eta}{\sqrt{\hat{\boldsymbol{r}}^{(k+1)}} + \epsilon}\hat{\boldsymbol{m}}^{(k+1)} \label{update_adam} \hspace{0.2cm}.
\end{align}

In the \eqref{mean} respectively \eqref{variance}, the exponential moving average of gradient respectively squared gradient is updated. Then both terms are bias-corrected in \eqref{correct1} and \eqref{correct2} and the final update of parameters is performed in \eqref{update_adam}. Usually both $\boldsymbol{m}^{(0)} = \boldsymbol{0}$ and $\boldsymbol{r}^{(0)} = 0$. A good default choice of parameters suggested by \cite{adam3} is $\gamma_1 = 0.9$, $\gamma_2 = 0.999$ and $\epsilon = 10^{-8}$.



\section{Backpropagation algorithm}
\label{chapter:backprop}

We already presented an idea of training neural network via different optimization methods, but we did not provide the way of computing partial derivatives (gradients) of loss function with respect to weights and biases, which are necessary to execute the training algorithm. This could be of course done by computing the derivatives analytically, which is definitely very uncomfortable and almost insane idea, especially when adopting large neural networks. For this reason, the \textit{backpropagation algorithm} was developed in order to retrieve partial derivatives with respect to all weights and biases involved in the neural network in comfortable and quick way.

The derivation and description of backpropagation algorithm is motivated by papers \cite{backprop} and \cite{feedforward}. We define some helpful notation in advance in order to simplify the derivation. Let us denote output from $l$-th layer of neural network and $j$-th neuron of that layer as $a_j^l$, also entire output vector of $l$-th layer will be denoted as $\boldsymbol{a}^l$. Term $w_{jk}^l$ denotes weight of connection going from neuron $k$ in previous layer $(l-1)$ to neuron $j$ in layer $l$ (it is in reversed order on purpose, it will simplify further notation with respect to the matrix multiplication) and term $\mathbb{W}^l = (w_{jk}^l)_{j,k}$ denotes matrix of weights directed from layer $(l-1)$ to $l$. Bias of node $j$ in layer $l$ is denoted as $b_j^l$ and biases folded into vector of biases of layer $l$ is $\boldsymbol{b}^l$. By applying this notation, we could write comfortably a feedforward dependency for output from layer $l$ as
\begin{equation}
    \boldsymbol{a}^l = \phi(\mathbb{W}^l \boldsymbol{a}^{l-1} + \boldsymbol{b}^l) \hspace{0.5cm},
    \label{comfort_write}
\end{equation}
where $\phi(\cdot)$ is arbitrary activation function used in neurons of layer $l$ (this definition apparently depends on the current layer $l$, but we omit notation of this dependence for simplification), which is applied in element-wise fashion. We also denote argument of $\phi(\cdot)$ in \eqref{comfort_write} as
\begin{equation}
    \boldsymbol{z}^l = \mathbb{W}^l \boldsymbol{a}^{l-1} + \boldsymbol{b}^l
\end{equation}
The aim of backpropagation algorithm is to compute $\frac{\partial \Lambda}{\partial w}$ and $\frac{\partial \Lambda}{\partial b}$ (we will omit arguments of the functions for simplification, all of them are evaluated in the values of current weights, biases and training instance) with respect to arbitrary weight $w$ and bias $b$. These partial derivatives expresses, how much is loss function changed when $w$ or $b$ is changed. Of course, the main goal is to find optimal weights and biases, which corresponds to the local (global) minimum of loss function. Intuitively, we could imagine that in each layer of neural network arises some error (from imperfection of current weights and biases), which cause flaws in the output layer. Furthermore, we could say that even individual neurons produce these errors, which consequently leads to corrupted output. But what we can measure is only deviation of current output (extracted from output layer) of neural network from desired value, because we do not know in advance "correct" output (activation values) of other layers. So briefly, what backpropagation really does is that it \textit{backpropagates} error from the output layer to the previous layers and then from these \textit{error} terms computes corresponding partial derivatives $\frac{\partial \Lambda}{\partial w}$ and $\frac{\partial \Lambda}{\partial b}$.

We will now define this \textit{error} term heuristically. Output from $j$-th neuron of layer $l$ could be written as $a_j^l = \phi(z_j^l)$. Let's imagine for a moment, that layer $l$ has corrupted output (not corresponding to the optimal value) denoted as $\hat{a}_j^l$. This corrupted value could be written as $\hat{a}_j^l = \phi(\hat{z}_j^l) =\phi(z_j^l + \Delta z_j^l)$, where $\Delta z_j^l$ expresses deviation of argument $z_j^l$ from optimal value. In case that other weights and biases are already optimal, the difference between optimal cost and "corrupted" cost would be approximately $\frac{\partial \Lambda}{\partial z_j^l}\Delta z_j^l$ (from Taylor expansion, we suppose that $\Delta z_j^l$ is "small enough"). Deviation is getting larger, when value of $\frac{\partial \Lambda}{\partial z_j^l}$ is getting larger. Also when value of $\frac{\partial \Lambda}{\partial z_j^l}$ is close to 0, then neuron is very close to optimal state. Therefore, we could define error term as
\begin{equation}
    \delta_j^l = \frac{\partial \Lambda}{\partial z_j^l}
\end{equation}
This is heuristic derivation and of course, it is not absolutely precise and certainly requires deeper math behind (e.g. we omitted trade-off between $\frac{\partial \Lambda}{\partial z_j^l}$ and $\Delta z_j^l$ in estimating the deviation), but this derivation serves mainly for description of intuitive behaviour of $\delta_j^l$. Also more important is, that $\delta_j^l$ is only provisional term, which will lead us to derivation of values of real interest - $\frac{\partial \Lambda}{\partial w}$ and $\frac{\partial \Lambda}{\partial b}$.

The algorithm always backpropagates the error of single training instance $\boldsymbol{x}$ (we can use only one training instance in time), so what it really computes is $\frac{\partial\Lambda_\boldsymbol{x}}{\partial w}$ and $\frac{\partial \Lambda_\boldsymbol{x}}{\partial b}$. Therefore the loss function $\Lambda$ needs to be rewritten in the same manner as in \eqref{good_shape}, which we assume. From now, we will omit the subscript and denote $\Lambda_\boldsymbol{x}$ simply as $\Lambda$. Another assumption on loss function is that it can be expressed as function of output $\boldsymbol{a}^L$ from the output layer, where $L$ denotes the output layer. We need $\Lambda = \Lambda(\boldsymbol{a}^L)$, but it may depend also on other variables. This assumption will be explained later. For instance, both assumptions are fulfilled by quadratic cost function and also by other loss functions presented later in this thesis.


Since we defined $\delta_j^l$, we now need to find equations for values of real interest, ig. partial derivatives of loss function with respect to weights and biases. Entire backpropagation algorithm stands on 4 equations, which allow us to compute values of real interest from errors in individual layers of neural network. They can be simply derived by careful application of the chain rule from multivariate calculus. Note that all of these equations are specifically written for arbitrary activation functions $\phi: \mathbb{R}^1 \rightarrow \mathbb{R}^1$ (tanh, sigmoid, ReLU). This corresponds with our experiments since we used such functions in the hidden layers of our networks (it is also common in practice). Only one exception was the case of the output layer, where we used both sigmoid and softmax. But softmax is a vector function and therefore the general equation \eqref{BP1} should be slightly modified as a special case for softmax (or vector functions in general). Other equations remain same. We write down this special shape for softmax in the commentary to the corresponding equation.

\begin{enumerate}
    \item \textbf{Equation to calculate error $\boldsymbol{\delta}^L$ of the output layer L}:
    \begin{equation}
        \delta_j^L = \frac{\partial \Lambda}{\partial a_j^L}\phi'(z_j^L)
        \label{BP1}
    \end{equation}
    As we mentioned before, the backpropagation algorithm runs backwards. First, we need to compute error in the output layer. It is calculated by multiplication of partial derivative of loss function with respect to the output $a_j^L$ of $j$-th neuron of layer $L$ and derivative of activation function $\phi$ in $z_j^L$. This equation is perfectly intuitively understandable, because the error term is basically expressed as trade-off between "how much is loss function changing with respect to the output from neuron ($\frac{\partial \Lambda}{\partial a_j^L}$)" and "how much is output of the neuron changing with respect to the input value to the activation function ($\phi'(z_j^L)$)". If any of these values are small, then error of that neuron is also small. Furthermore, both values could be easily retrieved and the shape of derivatives is usually computed analytically (obviously depending on the shape of loss function and activation function). We may also notice that the partial derivative of $\Lambda$ is computed with respect to $a_j^L$. This is the reason why we need the assumption of $\Lambda$ depending on $\boldsymbol{a}^L$. Note that this shape corresponds to the sigmoid function $\sigma$ used in the output layer (or any arbitrary scalar function with the scalar input). For softmax function $\boldsymbol{\theta}$ it would be 
    \begin{equation}
            \delta_j^L = \sum_{k=1}^{|\mathbb{C}|}\frac{\partial \Lambda}{\partial a_k^L}\frac{\partial [\boldsymbol{\theta}(\boldsymbol{z}^L)]_k}{\partial z_j^L} \label{BP1_softmax}\hspace{0.2cm},
    \end{equation}
since $\forall k \in \{1,...,|\mathbb{C}|\}$, the function $\boldsymbol{\theta}(\boldsymbol{z}^L)]_k$ depends on $z_j^L$ where $|\mathbb{C}|$ denotes number of neurons in the output layer, which is same as number of classes (\mathbb{C} denotes a set of all classes).
    
    \item \textbf{Equation to backpropagate the error from layer $(l+1)$ to previous layer $l$}
    \begin{equation}
        \delta_j^l = [(\mathbb{W}^{(l+1)})^T\boldsymbol{\delta}^{(l+1)}]_j \phi'(z_j^l)
        \label{BP2}
    \end{equation}
    This equation gives us a way to backpropagate the error backwards. Together with \eqref{BP1}, we can compute $\boldsymbol{\delta}^l$ (vector of errors $\delta_j^l$ of neurons in layer $l$) in arbitrary layer $l$. It is obtained by multiplying transpose of weight matrix $\mathbb{W}^{(l+1)}$ with vector of errors of $(l+1)$ layer $\boldsymbol{\delta}^{(l+1)}$. This could be thought as a "projection" of error to the previous layer. Then the result is multiplied by $\phi'(z_j^l)$, same as in the case \eqref{BP1}, because the error is also influenced by the speed of change of the activation function.
    
    \item \textbf{Equation to calculate} $\frac{\partial \Lambda}{\partial b_j^l}$
    \begin{equation}
        \frac{\partial \Lambda}{\partial b_j^l} = \delta_j^l
        \label{BP3}
    \end{equation}
    This first value of real interest could be determined easily, because equation \eqref{BP3} states that it is equal directly to the error term. And \eqref{BP1} and \eqref{BP2} already give us tools to compute the error in arbitrary layer.
    
    \item \textbf{Equation to calculate} $\frac{\partial \Lambda}{\partial w_{jk}^l}$
    \begin{equation}
        \frac{\partial \Lambda}{\partial w_{jk}^l} = a_k^{(l-1)}\delta_j^l
        \label{BP4}
    \end{equation}
    This equation computes the partial derivative of the loss function with respect to any weight. It depends on activation (output) from $k$-th neuron of layer $(l-1)$ and error of $j$-th neuron of the consecutive layer $l$.
\end{enumerate}
By applying these 4 equations, we could compute all values of interest easily. Also this equations could be in some cases fully rewritten into the vector (matrix) shape and therefore solved in a fast and efficient way (some libraries often effectively computes and manipulates with matrix-organized data, such as Numpy \cite{numpy}). More information on derivation of these equations could be found in \cite{feedforward}.

\subsubsection{Problems related to activation functions}

In this part we shortly summarize and describe some properties of different activation functions related to the backpropagation that influence entire training procedure.

First we will discuss the so called \textit{vanishing gradient problem}. This problem occurs especially in the case of multi-layer neural networks. Consider equations \eqref{BP1}  and \eqref{BP2} mentioned in the previous part. These equations include the derivatives of activation functions in the corresponding layers. If the error backpropagates recursively from the output layer to previous layers as in the \eqref{BP2}, then more and more derivative terms are multiplied together. If some of them are close to zero, than the final term is supposed to be low as well. The gradients of loss function are computed with the usage of the error term (as suggested in \eqref{BP3} and \eqref{BP4}) and therefore the update of parameters could be insignificant in early layers due to small gradients. This is often the case of sigmoid activation function. The derivative of sigmoid is $\sigma'(z) = \sigma(z)(1-\sigma(z))$ \cite{note_backprop} and therefore it reaches maximum value of 0.25. Multiplication of multiple sigmoid derivatives could result in significant reduction of gradients in early layers of neural network due to chain interaction of individual derivatives.

Situation is slightly better in the case of tanh activation function, which derivative lies within 0 and 1, but still the vanishing gradient problem occurs, because values are often less than 1. Especially when the sigmoid or tanh output values close to the asymptotic tails, the derivative is close to 0 and therefore there is likely only a small update of weight and bias which hardly change the current behaviour of a neuron. This state is called \textit{saturation}. Vanishing gradient and saturation make the training procedure more difficult and could negatively affect the quality of performance.

Problem of vanishing gradient could be considerably suppressed by adoption of (Leaky) ReLU activation function. In the case of ReLU, the derivative is always either 1 or 0. When the ReLU operates in the positive area, the derivatives are always 1 and therefore no vanishing gradient occurs. Problem arises when ReLU moves to the negative area, where the derivative is 0. This produces a so called \textit{dead neurons}, because the derivative is 0 and error is not backpropagated from this neuron. Also it is not likely to recover from this state as the training proceeds. From this reason Leaky ReLU was introduced, that it has not-null derivative in the negative area and therefore allows neuron to recover. This prevents producing of dead neurons.

\section{Loss functions}

In this section we summarize popular loss functions among neural network context and discuss their properties. We already define quadratic cost function (MSE) in \eqref{mse}. It is widely-applied in the machine learning area, but it has some drawbacks related to the backpropagation algorithm. Specifically, we refer to the \textit{learning slowdown} problem \cite{feedforward,learn_slow}, which is the issue especially in the case of classification.

We demonstrate this problem on the quadratic cost function and sigmoid in the output layer. Since the quadratic cost function is a sum of functions defined in \eqref{good_shape}, we are justified to work only with these single functions. We slightly ease the notation and use some notation defined in the \autoref{chapter:backprop}. Let us denote the MSE single function as
\begin{equation}
    \Lambda = \frac{||\boldsymbol{y} - \boldsymbol{a}^L||^2}{2} \hspace{0.2cm},
\end{equation}
where $\boldsymbol{y}$ denotes the correct output and $\boldsymbol{a}^L$ denotes the output of the neural network (which is the same as output from the output layer $\boldsymbol{a}^L$). Now we will express the derivatives of loss function $\Lambda$ with respect to weights and biases in the output layer $L$ with usage of backpropagation equations. First we compute the error term for node $j$ in the output layer as in \eqref{BP1}. Since $\frac{\partial \Lambda}{\partial a_j^L} = y_j - a_j^L$, the error term is

\begin{equation}
    \delta_j^L = (y_j - a_j^L)\phi'(z_j^L) \hspace{0.2cm}.
\end{equation}

Now we substitute this term in the equations \eqref{BP3} and \eqref{BP4}.

\begin{align}
    \frac{\partial \Lambda}{\partial b_j^L} &= (y_j - a_j^L)\phi'(z_j^L)\\
    \frac{\partial \Lambda}{\partial w_{jk}^L} &= a_k^{(L-1)}(y_j - a_j^L)\phi'(z_j^L)
\end{align}

In these equations occur derivatives of the activation function (sigmoid in this example) in the output layer. Eventually, this causes the learning slowdown problem. If the values of sigmoid are close to 0 or 1 (asymptotic tails), the derivative is very close to 0 and therefore it produces very low updates, since the corresponding partial derivatives are low. Therefore training this neural network to switch values close to asymptotic tails (if this is incorrect behaviour) is very slow, hence the learning slowdown.

It would be similar in the case of MSE combined with softmax. It is easy to derive that derivative of softmax fulfills the equation \cite{note_backprop}
\begin{equation}
    \frac{\partial [[\boldsymbol{\theta}(\boldsymbol{z}^L)]_k}{\partial z_j^L} = [\boldsymbol{\theta}(\boldsymbol{z}^L)]_k(\delta_{jk} - [\boldsymbol{\theta}(\boldsymbol{z}^L)]_j) \hspace{0.2cm},
    \label{derivative_softmax}
\end{equation}

where $\delta_{jk}$ is Kronecker delta. Since softmax produces probability distribution, so if value for specific $k$ is close to 1, then other values are close to 0 and therefore $\forall k$ the derivative in \eqref{derivative_softmax} is close to 0. Thus the error term in \eqref{BP1_softmax} would be also low.

To fix this drawback, the Cross Entropy loss function can be exploited, which results in the absence of the learning slowdown problem.

\subsubsection{Cross Entropy loss function}

Cross Entropy loss function is often a preferred choice among loss functions in the case of classification. It is frequently applied in combination with softmax or sigmoid in the output layer. It is defined as

\begin{equation}
    \Lambda^{CE}(\mathbb{W},\mathbb{B},\mathbb{X}) = -\frac{1}{n}\sum_{\boldsymbol{x} \in \mathbb{X}}\sum_{i = 1}^{|\mathbb{C}|}y_i(\boldsymbol{x})\ln{\hat{y}_i(\boldsymbol{x})} \hspace{0.2cm},
    \label{CE}
\end{equation}

where $\mathbb{X}$ is a set of input instances, $n = |\mathbb{X}|$, $|\mathbb{C}|$ denotes number of neurons in the output layer (equal to number of classes), $y_i(\boldsymbol{x})$ is $i$-th component of $\boldsymbol{y}(\boldsymbol{x})$, which denotes the correct output for single instance $\boldsymbol{x}$ and $\hat{y}_i(\boldsymbol{x})$ denotes the $i$-th component of the prediction of neural network $\boldsymbol{\hat{y}}(\boldsymbol{x})$ for single instance $\boldsymbol{x}$. Also we explicitly mark the dependence on a set of weights $\mathbb{W}$ and a set of biases $\mathbb{B}$. The definition in \eqref{CE} acts as overall loss across multiple instances (averaging similarly as in the case of quadratic cost function). Cross Entropy of single instance $\boldsymbol{x}$ is therefore

\begin{equation}
    \Lambda^{CE}(\mathbb{W},\mathbb{B},\boldsymbol{x}) = -\sum_{i = 1}^{|\mathbb{C}|}y_i(\boldsymbol{x})\ln{\hat{y}_i(\boldsymbol{x})} \hspace{0.2cm}.
    \label{CE_single}
\end{equation}

It measures the similarity between correct label distribution and the predicted label distribution for $\boldsymbol{x}$. Since there exists only one $i \in \{1,...,|\mathbb{C}|\}$ (corresponding to the ground-truth label), where $y_i(\boldsymbol{x}) = 1$ (all other components are 0), then a sum in  \eqref{CE_single} reduces only to one summand. Therefore the Cross Entropy of single instance $\boldsymbol{x}$ decreases if the predicted value corresponding to the ground-truth label of $\boldsymbol{x}$ increases and vice versa, which is favourable behaviour in the task of classification.

Now we will show that combination of Cross Entropy loss function and softmax in the output layer prevents the learning slowdown problem. We compute the error term for softmax as in \eqref{BP1_softmax} and show that it does not depend on the derivatives of softmax. Also we use the notation from the \autoref{chapter:backprop} and denote $a_i^L = \hat{y}_i$. The parentheses with dependencies of $\Lambda^{CE}$ are omitted for simplification.
\begin{align*}
\delta_j^L &= \frac{\partial \Lambda^{CE}}{\partial z_j^L} = \sum_{k=1}^{|\mathbb{C}|}\frac{\partial \Lambda^{CE}}{\partial a_k^L}\frac{\partial [\boldsymbol{\theta}(\boldsymbol{z}^L)]_k}{\partial z_j^L} = \sum_{k=1}^{|\mathbb{C}|}(-\frac{y_k}{[\boldsymbol{\theta}(\boldsymbol{z}^L)]_k})[\boldsymbol{\theta}(\boldsymbol{z}^L)]_k(\delta_{kj} - [\boldsymbol{\theta}(\boldsymbol{z}^L)]_j) = \\
&=  \sum_{k=1}^{|\mathbb{C}|}(-y_k\delta_{kj}) + \sum_{k=1}^{|\mathbb{C}|}y_k[\boldsymbol{\theta}(\boldsymbol{z}^L)]_j = -y_j + [\boldsymbol{\theta}(\boldsymbol{z}^L)]_j = a_j^L - y_j = \hat{y}_j - y_j
\end{align*}

We can see that $\delta_j^L$ is fully controlled only by the difference between predicted and correct value and no derivative of activation function occurs in the final equation. Therefore this approach is not affected by the learning slowdown problem as in the case of MSE.

There are also other loss functions applied in the neural network context, but none of them are more popular than MSE and Cross Entropy. Other loss functions can be found in \cite{loss_func}.


\section{Regularization}

Regularization is a crucial term, which is almost indispensable among machine learning in general nowadays. It addresses the problem of overfitting and introduces some solutions to attain better generalization ability of machine learning models. Neural networks are able to approximate even very complicated, non-linear functions and hence are prone to overfitting quite easily. This is especially true for large networks. We focus on regularization specifically in terms of neural networks and describe some popular regularization techniques.

\subsubsection{Modification of loss function}

This technique works in a way that it adds a special regularization term to the loss function, which usually restricts the size of weights and penalizes the loss function for large weights. A heuristic idea behind this is that weight is allowed to be large only if it has a distinctive influence on decreasing the loss function by improvement of output of the neural network. Thus it is more likely that model increases only those weights that have high general effect on estimating the output and does not increase weights to gain only minor improvement of results by learning a noise in data \cite{regularization}.

We distinguish the representatives of this group based on different regularization terms that are added to the loss function. Among the most popular belong L2 and L1 regularization, which are defined as follows:

\begin{align}
    \text{\textbf{L2 regularization}}\hspace{0.2cm}\text{:}\hspace{0.2cm}&\Lambda(\mathbb{W},\mathbb{B},\mathbb{X}) = \Lambda_0(\mathbb{W},\mathbb{B},\mathbb{X}) + \frac{\lambda}{2n}\sum_{w \in \mathbb{W}}|w|^2 \nonumber \\
    &\frac{\partial \Lambda}{\partial w} = \frac{\partial \Lambda_0}{\partial w} + \frac{\lambda}{n}w  \label{L2reg} \\
    \text{Update rule}\hspace{0.2cm}\text{:}\hspace{0.2cm} &w^{(k+1)} = (1 - \eta\frac{\lambda}{n})w^{(k)} - \eta\frac{\partial \Lambda_0}{\partial w}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X}) \nonumber \\ 
    \text{\textbf{L1 regularization}}\hspace{0.2cm}\text{:}\hspace{0.2cm}&\Lambda(\mathbb{W},\mathbb{B},\mathbb{X}) = \Lambda_0(\mathbb{W},\mathbb{B},\mathbb{X}) + \frac{\lambda}{n}\sum_{w \in \mathbb{W}}|w| \nonumber \\
     &\frac{\partial \Lambda}{\partial w} = \frac{\partial \Lambda_0}{\partial w} + \frac{\lambda}{n}\text{sgn}(w)  \label{L1reg} \\
    \text{Update rule}\hspace{0.2cm}\text{:}\hspace{0.2cm} &w^{(k+1)} = w^{(k)}- \eta\frac{\lambda}{n}\text{sgn}(w^{(k)}) - \eta\frac{\partial \Lambda_0}{\partial w}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X}) \nonumber \hspace{0.2cm},
\end{align}

where $n = |\mathbb{X}|$, $\Lambda_0$ denotes original loss function and $\Lambda$ denotes loss function after addition of regularization term. The effect of regularization term is controlled by the magnitude of regularization parameter $\lambda \geq 0$. The larger $\lambda$ is, the larger penalty is put on weights and vice versa. Both variants penalizes large weights in a slightly different way. The difference is perfectly visible from the individual update rules of weights for Gradient Descent (or SGD) in \eqref{L2reg} respectively \eqref{L1reg}. We can see that in the case of L2 regularization the weights are gradually decreased proportionally to the magnitude of the corresponding weight, whereas in the case of L1 regularization the weights are modified by value of constant magnitude. This means that L2 regularization tends to shrink larger weights more than L1. L2 regularization is sometimes referred to as a \textit{weight decay}.

In the experimental part we adopt mainly L2 regularization as a regularization technique. It could be also used in adaptive optimization algorithms, such as Adam. Generally accepted approach is to replace gradient $\nabla \Lambda_0$ with $\nabla \Lambda$ for initializing corresponding terms (e.g. equations \eqref{mean} and \eqref{variance}), as we did also in the case of SGD. There has been some criticism to this approach related to the adaptive algorithms \cite{adam_criticism}, which led to some proposals and improvements, but they are beyond the scope of this thesis and we will not mention them. In our experiments we use replacement of gradients as suggested above.

\subsubsection{Early stopping}

Early stopping is possibly the most natural regularization technique. It basically terminates the training phase of neural network, if the model starts to deteriorate its generalization ability. It is usually demonstrated at some epoch of training by degradation (stagnation) of performance on the validation dataset, while the performance on the training dataset is still improving. The performance is usually measured by monitoring the accuracy or loss on the validation dataset. There is a patience constant, which determines the number of epochs the algorithm waits for the improvement. If no improvement occurs, the training is terminated.

\subsubsection{Dropout}

Dropout is a popular regularization technique among neural networks, which has shown state-of-the-art results in many applications. The algorithm basically works as follows: During the training of neural network is randomly dropped half of the neurons (temporarily) in each hidden layer for each mini-batch. Then the forward pass and backward pass is performed through the subnetwork and the weights and biases of the subnetwork are updated \cite{feedforward,dropout}. The dropped neurons are then recovered and the same procedure is repeated for the following mini-batches as training continues. More generally, the neurons could be dropped with probability $p$, but at least one neuron must stay in each hidden layer in order to conduct meaningful updates.

For a testing phase is eventually applied entire network with all neurons and (in the case of dropping half of the neurons) we halve all weights of connections outgoing from the hidden neurons since we operate with twice as many hidden neurons as the subnetworks were trained with.

There are not many theoretical researches that would clarify the functionality of this method, but there exists plenty of heuristic explanations. The first possible explanation of functionality is that Dropout basically "average" several different neural networks, and therefore the full network is supposed to have higher generalization ability than single networks. The second one is that this technique reduces influence and dependencies between individual neurons and thus the neurons are forced to learn more robust features rather than relying on outputs of particular neurons \cite{dropout}.

We mentioned three different regularization techniques frequently used among neural networks and machine learning in general. Other regularization techniques are for instance \textit{dataset augmentation}, \textit{parameter sharing}, etc. The descriptions of other methods could be found in \cite{regularization}. It is also common to combine individual regularization techniques to secure even higher generalization ability, but they must be applied carefully since the regularizers can be more decisive in training than the data itself.













\chapter{Decision-Tree-Inspired NN architecture}
In this section is provided detailed derivation of the basic neural network architecture, which is further employed in all subsequently proposed models. The main idea is built on the article \cite{NRF} describing 1 to 1 transformation of arbitrary regression tree (random forest regressor) to the specifically designed neural network (ensemble of neural networks).

Unfortunately, an approach presented in \cite{NRF} is not uniformly convertible to the classification problem. Based on this issue, the  alternative architecture and initial settings are proposed in order to simulate the exact behaviour of corresponding decision tree classifier.

This reformulation provides a sensible opportunity to enhance the decision tree performance, because parameters of newly constructed neural network could be better adapted with usage of the backpropagation algorithm of neural networks and therefore achieve superior classification.

\section{Architecture and initial settings}

The initial weights and biases of neurons and architecture of input layer and first two hidden layers will remain same among all proposed models. The settings was motivated by \cite{NRF}. A sample of such architecture (only input layer and first two hidden layers) is illustrated in Figure \ref{fig:basic_arch} copying decisions of the decision tree depicted in \ref{fig:dec_ilust}, which splits the space by two hyperplanes as illustrated also in \ref{fig:dec_ilust}. Let's first consider neurons in the first and second hidden layer of network as perceptrons. This means, that activation function is threshold function
\begin{equation}
    \tau(\boldsymbol{x}) = 2\mathbf{1}_{\boldsymbol{x} \geq 0} - 1\hspace{0.5cm},
\end{equation}
where all math operations on vector $\boldsymbol{x}$ are element-wise. Also
\begin{equation}
    (\mathbf{1}_{\boldsymbol{x} \geq 0})_i = \begin{cases}
    1, & \text{if}\hspace{0.25cm} x_i \geq 0 \\
    0, & \text{otherwise}
    \end{cases}\hspace{0.5cm},
\end{equation}

where $(\mathbf{1}_{\boldsymbol{x} \geq 0})_i$ is $i$-th element of $\mathbf{1}_{\boldsymbol{x} \geq 0}$.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{ilustrace_in_space.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{decision_tree_ilustrace.png}
\end{minipage}
\caption{In the left Figure are depicted 3 color-coded classes divided by the decision tree in the right Figure, that are separated by two hyperplanes: \\$x_1 - 2 = 0$ and $x_2 - 1 = 0$. Inner nodes of the decision tree are green colored with corresponding split function next to them, whereas the leaves are red colored. All nodes are numbered.}
\label{fig:dec_ilust}
\end{figure}
\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{ilustrace_basic_structure.png}
\caption{Transformation of the decision tree in Figure \ref{fig:dec_ilust} to the neural network with two hidden layers. The first hidden layer detects the decisions of inner nodes same numbered as in \ref{fig:dec_ilust}. The second hidden layer retrieves leaf membership of input vector same as in the decision tree. Not null connections between neurons are bold higlighted with corresponding weights. Biases are written next to the neurons. All dashed connections indicates null connection (weight equals 0).}
\label{fig:basic_arch}
\end{figure}

So perceptrons from the first and second hidden layer outputs $+1$ or $-1$. Why is it chosen in such manner will be clear from further explanations.

\subsection{First hidden layer}

The first hidden layer should copy decisions of all inner nodes present in the corresponding decision tree. The first hidden layer has same number of neurons as number of inner nodes. As was explained in the chapter about decision trees and random forests, each inner node $k \in \{1,...,L-1\}$ of the decision tree, where $L-1$ is total number of inner nodes (in fact, if $L-1$ is total number of inner nodes, then $L$ is total number of leaves present in the decision tree), possesses split function with parameters $j_k \in \{1,...,n\}$, which is one dimension in a $n$-dimensional space that is used for split and also $\alpha_{j_k}$, which is a threshold. Let's also define function $s_k$ as 
\begin{equation}
    s_k(\boldsymbol{x}) = x_{j_k} - \alpha_{j_k}\hspace{0.5cm}.
\end{equation}
It is apparent that equation $s_k(\boldsymbol{x}) = 0$ defines a hyperplane in $\mathbb{R}^n$, which splits the space in inner node $k$.

In order to obtain all decisions of inner nodes in the corresponding neurons of our neural network, we initialize weights in neuron $k$ as $(0,..,0,1,0,..,0)^T$ with single 1 in $j_k$-th position and 0 otherwise. Bias is set to $-\alpha_{j_k}$. Hence, output of the first hidden layer is $(\tau(s_1(\boldsymbol{x})),\tau(s_2(\boldsymbol{x})),...,\tau(s_{L-1}(\boldsymbol{x}))$ and it precisely copies decisions of inner nodes, with +1 indicating that input vector belongs to the right side of the hyperplane and -1 otherwise (and +1 if it belongs to the hyperplane). This is also done for the inner nodes outside the path of the input vector. The illustration can be seen in Figure \ref{fig:first_hid_layer}.


\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{first_hidden_layer_space.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{first_hidden_layer.png}
\end{minipage}
\caption{Space in the left figure is divided by red (in inner node number 1) and black (in inner node number 2) hyperplanes. If we consider point $\boldsymbol{x} = (x_1,x_2)^T = (2.75,0.45)^T$ highlighted in left Figure, the output from the first hidden layer is +1 from neuron 1 (corresponding to the red hyperplane) and -1 from neuron 2 (corresponding to the black hyperplane).}
\label{fig:first_hid_layer}
\end{figure}

\subsection{Second hidden layer}

From all inner node decisions obtained by the first hidden layer it should be possible to reconstruct the exact leaf membership of the input vector $\boldsymbol{x}$. This is the main task to accomplish by the second hidden layer. If there are $L-1$ neurons in the first hidden layer, then the second hidden layer consists of $L$ neurons, each one corresponding to the one individual leaf in the decision tree.

We connect neuron $m$ in the first hidden layer to neuron $m'$ in the second hidden layer with not null weight if and only if inner node corresponding to the neuron $m$ belongs to the path from root node to the leaf corresponding to the neuron $m'$. The weight is initialized to +1 if the split by inner node $m$ is to the right child and -1 otherwise. If neuron $m$ is not part of the path from root to the leaf, the weight is initialized always to 0.

Based on this setting, it could be simply deduced that number of not null connections from the first hidden layer (weights) to the (arbitrary) neuron $m'$ in the second hidden layer is same as length of the path from root to the leaf $m'$. This is illustrated in Figure \ref{fig:sec_hidd_len}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{sec_hidd.png}
\caption{In the left figure is highlighted blue path from the root node 1 to the leaf 3. This has length 1 and also only one initialized not null connection from the first hidden layer exists, because only root node is part of the path. In the remaining figures are depicted paths for other leaves (blue colored) from the root node (except of leaf 6, which is in same depth as leaf 7). It is easy to see, that equality between length of the path from the root node to the particular leaf and number of not null connections from the first hidden layer to the corresponding neuron holds always.}
\label{fig:sec_hidd_len}
\end{figure}

If output from the first hidden layer is $\boldsymbol{v} = (\pm 1, \pm 1,....,\pm 1)^T$, which encodes all decisions of inner nodes of the decision tree, then output from the neuron $m'$ in the second hidden layer is $\tau(\sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m'))$, where\\ $\boldsymbol{w}^{m'} = (w_1^{m'},w_2^{m'},...,w_{L-1}^{m'})^T$ is a vector of  weights for connections to neuron $m'$. These weights are not null if the corresponding inner node is involved in the path from root to the leaf $m'$ and are +1 if it is sent to the right child and -1 otherwise. For all inner nodes that are not involved in the root-leaf path are weights initialized to 0.

Desired behaviour of the neuron $m'$ in the second hidden layer is to output +1 if the input vector ends in leaf $m'$ and -1 otherwise. For this purpose, $bias(m')$ must be correctly set. To do so, it is sufficient to notice that the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i)$ as the first term in the argument of $\tau(\cdot)$ function equals to the length of the path from root node to leaf $m'$, if and only if the input ends in leaf $m'$. For convenience, let us denote this length as $l(m')$. It is a simple consequence of the fact, that the number of not null weights $w_{i_k}^{m'}$ is same as $l(m')$ and also they have same magnitude ($|w_{i_k}^{m'}| = 1$) and same sign as $v_{i_k}$, where $i_k \in \{1,...,L-1\}, w_{i_k}^{m'} \neq 0$. Therefore, each member of the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i) = \sum_{i_k=1,w_{i_k}^{m'} \neq 0}^{L-1}(w_{i_k}^{m'}v_{i_k})$ equals to +1 and all members sum up to $l(m')$.

Moreover, if the input does not end in leaf $m'$, then in the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i)$ exists a not null member in which interfere 2 integers (ones) with different signs, resulting in -1. Hence is clear, that if input does not end in leaf $m'$, the sum holds inequality $\sum_{i=1}^{L-1}(w_i^{m'}v_i) \leq l(m')-1 < l(m')$. For more precise intuition, the illustration is provided in Figure \ref{fig:sec_hidd_ilust}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{sec_hidd_ilust.png}
\caption{Demonstration of inequality $\sum_{i=1}^{L-1}(w_i^{m'}v_i) \leq l(m')-1 < l(m')$ in case of leaf number 7 (corresponding to one neuron in the second hidden layer of our network), if input does not end in leaf $m'$. The red path in the picture indicates real path of input in the decision tree. Our sample input ends in leaf number 6, as could be seen from the illustration. The output from the first hidden layer would therefore be $\boldsymbol{v} = (v_1,v_2,v_3)^T = (+1,-1,-1)^T$. But weights corresponding to neuron 7 (leaf 7) are $\boldsymbol{w}^7 = (w_1^7,w_2^7,w_3^7)^T = (+1,-1,+1)^T$. After multiplying the values in red circles and summing the results up, we obtain $\sum_{i=1}^{L-1}(w_i^{7}v_i) = 2 < 3$, where 3 means the length of the root-leaf path. A decrease is caused due to the interference of different signs in a grey circle.}
\label{fig:sec_hidd_ilust}
\end{figure}

After these considerations, the reasonable choice of $bias(m')$ is
\begin{equation}
    bias(m') = -l(m') + 0.5
\label{eq:bias}    
\end{equation}
and then $\sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m')$ has following property:
\begin{equation}
    \sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m')\begin{cases}
    > 0, & \text{if input ends in leaf}\hspace{0.18cm} m' \\
    < 0, & \text{otherwise}
    \end{cases}
\label{eq:sec_hidd_property}
\end{equation}

With respect to the property of $\tau(\cdot)$ function argument in \eqref{eq:sec_hidd_property}, the second hidden layer outputs a vector of $(-1,...,-1,+1,-1,...,-1)^T$ with a single positive 1 indicating the correct leaf membership of an input.

To retain this $\tau(\cdot)$ argument property, it is sufficient to choose any other arbitrary constant in \eqref{eq:bias} in range $(0,1)$ instead of 0.5. But to stay consistent with \cite{NRF}, we also used the proposed value of 0.5 in conducted experiments.

At this stage, we already defined architecture and initial weights and biases settings of first two hidden layers in order to transform decision tree into neural network with the same properties. All that remains is to gain classification predictions from the second hidden layer.

\subsection{Output layer}

In this section is proposed architecture and initial setting for output layer, that will gain same predictions as the decision tree. Unfortunately, same method proposed for regression trees in \cite{NRF} is not directly applicable in the classification case. Therefore we propose an alternative for the classification case, that give same classification outcomes as the corresponding decision tree.

The output layer will be constructed as follows: The number of neurons in output layer is equal to number of classes we desire to classify. Each neuron corresponds to only one particular class (one label). Neuron with the highest activation represents the predicted class of neural network. In the experiments were used two types of neurons in output layer - with sigmoid and softmax activation functions, as were discussed in Chapter [Activation functions].  To get the same performance as the decision tree, we must retrieve probability distributions stored in leaves from the leaf membership encoded in the second hidden layer. If the output layer outcomes the same probability values for classes as the decision tree, hence the neural network performs alike.

Let's denote output from the second hidden layer as\\ $\boldsymbol{r} = (-1,...,-1,+1,-1,...,-1)^T$, where the position of +1 indicates the leaf where the input falls in. For each leaf $l \in \{1,...,L\}$ we denote a probability vector $\boldsymbol{p^l} = (p^l_1,p^l_2,...,p^l_C)^T$ with probabilities of individual classes, that are stored in leaf $l$, where $C$ is total number of classes. If $\boldsymbol{r}$ has +1 as the first element (corresponding to the first leaf), i.e. $r_1 = +1$, the output layer should outcome $\boldsymbol{p}^1$. If $r_2 = +1$, the outcome should be $\boldsymbol{p}^2$ etc.

If we initialize biases in the output layer to 0, then appropriate initialization weights could be obtained by solving the system of linear equations with a regular matrix $\mathbb{A}$
\begin{equation}
    \mathbb{A}=\begin{pmatrix}
    1 & -1 & -1 &\hdots & -1 \\
    -1 & 1 & -1 & \hdots & -1\\
    \vdots & \ddots & \ddots & \ddots&\vdots \\
    -1 & \ddots & \ddots & \ddots&-1 \\
    -1 & -1 & \hdots & -1 & 1
    \end{pmatrix}\hspace{0.5cm}.
\end{equation}

Matrix $\mathbb{A} \in \mathbb{R}^{L\times L}$, where $L$ is a total number of leaves in the decision tree. The determinant of matrix $\mathbb{A}$ is
\begin{equation}
    \text{det}\mathbb{A} = \begin{vmatrix}
    1 & -1 & -1 &\hdots & -1 \\
    -1 & 1 & -1 & \hdots & -1\\
    \vdots & \ddots & \ddots & \ddots&\vdots \\
    -1 & \ddots & \ddots & \ddots&-1 \\
    -1 & -1 & \hdots & -1 & 1
    \end{vmatrix} = (-1)^{2L-3}\cdot2^{L-1}\cdot(L-2)
\label{eq:determinant}
\end{equation}

For the proof of \eqref{eq:determinant} see Chapter [Proves]. Matrix $\mathbb{A}$ is therefore always regular with except of $L = 2$. In this case, $\text{det}\mathbb{A} = 0$ and matrix $\mathbb{A}$ is singular. But for $L=2$, the decision tree has only root node and 2 leaves, which is rarely a well-functional model in practical use. It could have a good performance almost only in case of data with significantly unambigous geometric deployment, where occurs only 2 classes and exists one hyperplane that sufficiently separates them. In practice, we will almost never encounter such an elementary model.

In case of invertible activation function $\boldsymbol{\phi}(\cdot)$ in the output layer, we obtain appropriate weights for neuron $c$ (also represents class $c \in \{1,...,\mathbb{C}\}$ that this neuron corresponds to) in output layer after solving the following system of linear equations:

\begin{equation}
    \mathbb{A}\begin{pmatrix}
    w^c_1\\
    w^c_2\\
    \vdots\\
    w^c_L
    \end{pmatrix} = \boldsymbol{\phi}^{-1}(\begin{pmatrix}
    p^1_c\\
    p^2_c\\
    \vdots\\
    p^L_c
    \end{pmatrix})\hspace{0.5cm},
    \label{eq:lin_eq}
\end{equation}

where $\boldsymbol{w}^c = (w^c_1,...,w^c_L)^T$ are weights of connections from the second hidden layer to neuron $c$ in the output layer and $\boldsymbol{\phi}^{-1}(\cdot)$ is inverse function of $\boldsymbol{\phi}(\cdot)$. We used simplified notation and $\boldsymbol{\phi}$ denotes either vector functions (e.g. softmax) or scalar functions applied in element-wise fashion (sigmoid).

In other words, the appropriate weights for neuron $c$ in the output layer are obtained as

\begin{equation}
    \begin{pmatrix}
    w^c_1\\
    w^c_2\\
    \vdots\\
    w^c_L
    \end{pmatrix} = \mathbb{A}^{-1}(\boldsymbol{\phi}^{-1}(\begin{pmatrix}
    p^1_c\\
    p^2_c\\
    \vdots\\
    p^L_c
    \end{pmatrix}))\hspace{0.5cm}.
    \label{eq:weights_output}
\end{equation}

This is perfectly feasible for sigmoid or Leaky ReLU, which are both invertible. The situation is little problematic in case of softmax. It could be easily derived that softmax inversion is not unique. More specifically, if we consider vector $\boldsymbol{w}$ and $\boldsymbol{\hat{w}}$, where $\forall i \in \{1,...,L\}, \hat{w}_i = w_i + r$ for arbitrary real constant $r$, then application of softmax on both variants would produce $\forall k \in \{1,...,L\}$ same result, which follows from:

$$[\boldsymbol{\theta}(\boldsymbol{\hat{w}})]_k = \frac{e^{w_k + r}}{\sum_i^{}e^{w_i + r}} = \frac{e^{r}e^{w_k}}{e^{r}\sum_i^{}e^{w_i}} = \frac{e^{w_k}}{\sum_i^{}e^{w_i}} = [\boldsymbol{\theta}(\boldsymbol{w})]_k \hspace{0.2cm}.$$

Fortunately, we are interested in arbitrary solution, which would produce probability vector after application of softmax. Thus we define (one-sided) inversion of softmax $\forall k \in \{1,...,L\}$, $\forall \boldsymbol{z}$, where $\forall k \in \{1,...,L\}, 0 < z_k \leq 1, \sum_k^{}z_k = 1$,  as

\begin{equation}
[\boldsymbol{\theta}^{-1}(\boldsymbol{z})]_k = \ln(z_k) \hspace{0.2cm}.
\label{softmax_inversion}
\end{equation}

This definition fulfills condition $\boldsymbol{\theta}(\boldsymbol{\theta}^{-1}(\boldsymbol{z})) = \boldsymbol{z}$, because $\forall k \in \{1,...,L\}$ holds

$$\frac{e^{\ln z_k}}{\sum_i^{}e^{\ln z_i}} = \frac{z_k}{\sum_i^{}z_i} = z_k \hspace{0.2cm},$$

because $\sum_i z_i = 1$. Note that this only holds when original softmax is applied on the inversion define in \eqref{softmax_inversion}. It does not work in reversed order. In practice sometimes arises case, where $\exists k \in \{1,...,L\}, z_k = 0$. In such case we slightly modify the original value to $10^{-3}$, which secures only negligible deviation from the original probabilities and (in vast majority of cases) does not affect predictions of the model.

After initialization of biases in output layer to 0 and solving the system of linear equations in \eqref{eq:weights_output} for all neurons in the output layer (or computing the inverse of matrix $\mathbb{A}$) and for appropriate activation function chosen in advance, we gain also all initialization weights. This initial setting causes the neural network to output same predictions as from the corresponding decision tree and hence to get equally performing classification model. %We refer to this transformed neural network as \textit{neural tree}.

\section{NN models with decision-tree-initialization}
\label{NN_models}

\subsection{Perceptron activation function replacement}

In order to apply reasonable training procedure with backpropagation algorithm on proposed method, it is suitable to replace perceptron activation function $\tau(\cdot)$ (threshold function) with its smooth approximation. For this purpose the hyperbolic tangent function was adopted. This affects original transformation, because due to this approximation the resulting neural network is no longer one-to-one transformation to former decision tree. But still under certain conditions (especially the transition slope from the negative part of the $\tanh(\cdot)$ function to the positive one - the more upright, the better approximation we get) it could come fairly close to $\tau(\cdot)$ and therefore entire model performance remains unchanged with respect to the decision tree. Usage of $\tanh(\cdot)$ in first and second hidden layer is necessary with respect to the transformation procedure, even that $\tanh(\cdot)$ has some drawbacks as activation function (described in \autoref{chapter:backprop}). Using other activation functions in this case is not appropriate, because it will lack any decision tree relationship.

The closeness of $\tanh(\cdot)$ to the perceptron activation function in our experiments is controlled with $\beta > 0$ parameter.

\begin{equation}
    \tanh(\beta x) = \frac{e^{2\beta x - 1}}{e^{2\beta x} +1} 
\end{equation}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{tanh.png}
\caption{Shape of hyperbolic tangent for different $\beta$ parameters.}
\label{fig:tanh}
\end{figure}


The higher $\beta$ is, the better approximation of perceptron activation function is observed. With $\beta \rightarrow \infty$, $\tanh(\cdot)$ converges to the $\tau(\cdot)$. In the experiments were exploited two parameters, $\beta_1$ in the first hidden layer and $\beta_2$ in the second hidden layer, each to control transitions of $\tanh(\cdot)$ to $\tau(\cdot)$ in the corresponding hidden layers. We will refer to them as \textit{transition parameters}. Their effect on overall performance will be examined in further chapters. The influence of these parameters on the shape is depicted in the Figure \ref{fig:tanh}.

\subsection{Competitive decision-tree-motivated models}
In this section is provided overview of all decision-tree-motivated models examined in the experimental part of the thesis. Alongside the reference model proposed in Chapter 3, we propose another competitive models motivated by original decision tree structure. These models exploits knowledge acquired by training the decision tree and use it for setting the proper weights and biases up, but not all of them. Especially they relax conditions on weights and biases corresponding to the output layer (the reference model from Chapter 3 has deterministic weights and biases gained from the decision tree in all layers) and initialize them randomly. This does not provide accurate decision tree transformation as our reference model, but instead gives neural network more space to adapt in its own direction. Generally, in the experimental part were all random weights of connections pointing from arbitrary layer $l$ to following layer $l+1$ initialized randomly from Gaussian distribution of mean equal 0 and standard deviation equal $\frac{1}{\sqrt{n_l}}$, where $n_l$ denotes number of neurons in layer $l$. All biases were initialized randomly from normalized Gaussian distribution of mean equal 0 and standard deviation equal 1. This random weight initialization was inspired by recommendation from \cite{feedforward}.

\subsubsection{Reference model (NT)}

Acquisition of this model is described in Chapter 3. Its main purpose is to simulate corresponding decision tree behaviour as good as possible from the very beginning (quality of simulation is controlled by $\beta_1$ and $\beta_2$ transition parameters of hyperbolic tangent in the first and second hidden layer). With both $\beta$s approach infinity, the reference model gives same predictions as decision tree. Essentially, with respect to the predicting procedure of the decision tree based on picking label with the highest probability, it could converge to predicting same results with $\beta$s only "high enough".
% sem dát možná nějaké vysvětlení, jaká vliv má změna v beta na prediction, použít nějaký horní odhad výstupního vektoru, maticové normy
After transformation the neural network is already a sufficient model capable of making relevant predictions, because it could produce similar predictions as corresponding decision tree. This makes neural network to share resembling features as the decision tree, mainly regarding adjustments of decisions, inclination to overfitting, etc. The closeness of this model to the corresponding decision tree gives good opportunity to enhance performance of the decision tree by further backpropagation training, achieving improvement only after few epochs of training. Analysis of these issues will be provided in later chapters.

\subsubsection{NT\_basic}

First competitive model has weights and biases corresponding to the first and second hidden layer initialized similarly to the same layers in the reference model. The difference is in setting weights and biases corresponding to the output layer. In this case we initialize weights and biases randomly, as is depicted in the Figure \ref{fig:nrt_basic}.

This modification should reveal some properties related to advantages or disadvantages of simulating the decision tree since the beginning of backpropagation or leaving the output layer relaxed with space for neural network to adapt it in its own way. Also the sensitivity to different neural network or decision tree hyperparameters can vary. These questions will be examined and tested in the experimental part.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_basic.png}
\caption{NT\_basic model. In contrast with the reference model, the weights and biases corresponding to the output layer are initialized randomly.}
\label{fig:nrt_basic}
\end{figure}

\subsubsection{NT\_EL}

Next competitive model adds one extra layer of neurons between second hidden layer and output layer of the reference model. Weights and biases corresponding to all layers up to this new extra layer are initialized in the same manner as in the case of NT (extra layer now corresponds to the output layer of NT). This makes the extra layer to output same results as output layer of NT. Finally, weights and biases corresponding to the output layer of NT\_EL are initialized randomly. In the extra layer is used Leaky ReLU as activation function. This model is illustrated in the Figure \ref{fig:nrt_extra_layer_det}.

Adding one extra layer increases complexity, but could have positive impact on the final performance and outperform previous models.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_extra_layer_deterministic.png}
\caption{NT\_EL. Weights and biases corresponding to the extra layer are initialized deterministically as in the case of NT model. Other weights and biases are initialized randomly.}
\label{fig:nrt_extra_layer_det}
\end{figure}

\subsubsection{NT\_EL\_DW}

This model is motivated by NT\_EL model. The weights and biases up to the extra layer are initialized same as in the case of NT\_EL model. On the contrary, the weights and biases corresponding to the output layer are initialized deterministically. The goal is to adjust the weights in a way that neural network can output same predictions as the decision tree already at the beginning of backpropagation (for sufficiently high transition parameters $\beta_1$ and $\beta_2$). Since the extra layer can output same probabilities as the decision tree, it would be sufficient to traverse the output of the extra layer to the output layer. We achieve this by initializing weights of connections from the extra layer to the output layer by identity matrix. In other words, the connection from $j$-th neuron of the extra layer to $j$-th neuron of the output layer is initialized to 1 and other weights pointing from $j$-th neuron to the output layer are initialized to 0. This procedure is applied for each neuron of the extra layer. Biases corresponding to the output layer are initialized to 0. Then if we use softmax in the output layer, the probabilities would be modified, but order of these values remain same. Since the final prediction corresponds to the class with the highest probability, we end up with same predictions as the decision tree.



%Describe all five competitive models that we use in experiments with illustrations and algorithms. Discuss replacement of $\tau$ with tanh activation function and its influence on performance and backpropagation. Divide basic model to sparse setting and full connected setting. In sparse setting we allow to train with backpropagation only notnull connections, to preserve decision tree interpretation. That will help us also to demonstrate the effect of backpropagation. In full connected setting we train all parameters.

\chapter{Neural Random Forest}

Combination of multiple neural random trees into an ensemble, similarly as in the case of random forests.

\subsection{Voting}
\subsection{Averaging}


\chapter{Experiments}
\label{experiments}
\setlength\footnotemargin{8pt}

This chapter focuses on the experimental part of the thesis. Numerous experimental evaluations of the proposed models are presented alongside the thorough description and interpretation of the outcomes. The main emphasis is put especially on the comparison of the Neural Random Forest (NF) models with the corresponding Random Forest (RF) models. Moreover, the results of NF and RF models are also compared to the performance of other popular classification algorithms, such as Logistic Regression (LR) and Feedforward Neural Network (NN). The experiments are divided into three separate parts.

The first part is dedicated to the evaluation on the pre-selected public datasets. All proposed NF models are evaluated on these datasets and compared with RF and other models. Additionally, the effect of NF and RF hyperparameters on the classification performance is examined and visualized on the supporting graphs.

The second part comprises various studies of NF parameters that could be highly influential on the overall performance of the models. The experiments are conducted on the so called "toy" datasets - artificially created datasets that simulate different benefitial or disadvantegous features, which are more or less favourable to RF or NF. These experiments reveal some patterns and features of the NF models that could be exploited in the practical application.

In the last part the best NF models are applied to the classification task on the real dataset consisting of the medical data from the LUCAS project. The LUCAS project collaborates with medical institutions and hospitals in order to collect the data of the patients diagnosed with the lung cancer. These data combine personal information about the patients together with the diagnostic and treatment records. The classification task is to predict the first treatment based on the input diagnostic data of the patient. Generally, the early correct treatments play a crucial role in the overall survival of the patient with such diagnosis. This classification task is characterized by high-dimensionality, sparsity and the lack of the significant records, which makes it challenging for a lot of classification algorithms.

\section{Public dataset experiments}
\label{public:experiments}

This section provides comparison of NF and competitive models evaluated on the set of public datasets selected in advance. Also the effect of the choice of NF parameters on the overall performance is examined and visualized.

The models are compared with respect to the accuracy and macro-average of F1-score on all datasets. Both values are presented in terms of mean value and standard deviation after two subsequent 5-fold cross validations\footnote{\textit{K-fold cross validation}: First, the dataset is divided into \textit{K}$\in \mathbb{N}$ disjunct parts (usually of the same length). \textit{K}-1 parts are used as training set and the remaining part is used for testing. In the next iteration, another \textit{K}-1 parts are picked for training and the rest for testing. This procedure is repeated for \textit{K} times so in each iteration the different combination of training and testing set is applied.} (10 iterations in total) in order to avoid statistical fluctuations and gain higher robustness of the outcome.

All applied models are summarized in Table \ref{table::public_models} with their shortcut notations (column MODEL).

\definecolor{Red}{rgb}{0.95,0.3,0.9}



\begin{table}[h]
    \centering
    %\small\addtolength{\tabcolsep}{-5pt}
    \begin{tabular}{|c|c|c|}
    \hline
     \textbf{MODEL} & \textbf{DESCRIPTION} \\ \hline
     \cellcolor{yellow}LR & \thead{Logistic regression model} \\ \hline
     \cellcolor{yellow}NN & \thead{4-layer neural network} \\ \hline
     \cellcolor{green}RF20 & \thead{Random forest with 20 estimators} \\ \hline
    \cellcolor{green}RF30 & \thead{Random forest with 30 estimators} \\ \hline
     \cellcolor{green}RF50 & \thead{Random forest with 50 estimators} \\ \hline
    \cellcolor{orange}NRF & \thead{Neural random forest consisting of \\ \textit{NRT basic} estimators as in \autoref{NN_models}} \\ \hline
    \cellcolor{orange}NRF\_DW & \thead{Neural random forest consisting of \\ \textit{NRT - deterministic weights} estimators as in \autoref{NN_models}} \\ \hline
     \cellcolor{orange}NRF\_EL & \thead{Neural random forest consisting of \\ \textit{NRT extra layer} estimators as in \autoref{NN_models}} \\ \hline
     \cellcolor{orange}NRF\_EL\_DW & \thead{Neural random forest consisting of \\ \textit{NRT extra layer - deterministic weights} estimators as in \autoref{NN_models}} \\ \hline
    
    \end{tabular}
    \caption{Shortcut notation of the individual models and basic descriptions.}
    \label{table::public_models}
\end{table}

Models in Table \ref{table::public_models} are color-coded into three different areas. Yellow area includes competitive models to the RF and NF models. We adopted logistic regression with L2 regularization and 4-layer neural network with ReLU activation function in the hidden layers and softmax in the output layer. Two hidden layers have 60 neurons both. It applies Cross Entropy as loss function and optimization is done via Adam optimization algorithm (Adam had superior results in comparison with the Stochastic Gradient Descent). The mini-batch size was set to 10 and it was trained for 30 training epochs. Hyperparameters of NN were chosen as follows: $\eta = 0.01$, $\gamma_1 = 0.9$, $\gamma_2 = 0.99$, $\epsilon = 10^{-8}$, where $\eta$ is a learning rate and the rest are constants of Adam equally noted as in the Chapter [Optimization NN]. We use implementation of LR and NN from Python modules - Scikit learn \cite{scikit} and Keras \cite{keras}. 

The green area comprises RF models. We adopted three variations with different number of estimators. All RF models have same parameters: maximum depth = 6, number of randomly chosen features considered in the split = $\lfloor\sqrt{d}\rfloor$, where $d \in \mathbb{N}$ is a total number of features. All decision trees were trained with entire training dataset and we use entropy as the impurity function in the information gain criterion. We use implementation from Scikit learn.

The orange area includes all NF models. We tested many different combinations of hyperparameters and choices of different loss functions and activation functions (in layers where it is possible) and we present the best obtained solution. All NF models were built from RF20 model and use Cross Entropy as loss function and softmax in the output layer of all individual neural networks. The results with Cross Entropy were better in comparison with exploitation of Quadratic cost function (MSE). Transition parameters $\beta_1$ and $\beta_2$ in the first and second hidden layer have been set to 1. Learning rates were chosen as a compromise through a grid search so it is as suitable as possible for all our datasets. They were chosen as follows: $\eta^{\text{NRF}} = 0.002$, $\eta^{\text{NRF\_DW}} = 0.0035$, $\eta^{\text{NRF\_EL}} = 0.005$, $\eta^{\text{NRF\_EL\_DW}} = 0.0045$. We use Adam as optimization algorithm instead of SGD since the results were more favourable. The mini-batch size was set to 10 and the parameters of Adam were selected equally as in the case of NN: $\gamma_1 = 0.9$, $\gamma_2 = 0.99$, $\epsilon = 10^{-8}$. Single neural network estimators in NF models were trained for 30 epochs each. We added L2 regularization term to the loss function with regularization constant $\lambda = 0.01$. Finally, the voting scheme is applied as an ensemble method to obtain final predictions from NF (differences between results obtained by voting or averaging were negligible). We use our own implementation of NF models in Python.

Datasets used in the experiments are summarized in Table \ref{table::public_datasets}.
\begingroup
\begin{table}[h]
    \centering
    %\small\addtolength{\tabcolsep}{-5pt}
    \small\addtolength{\tabcolsep}{-5pt}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
     \textbf{ID}&\textbf{DATASET} & \textbf{\#instances} & \textbf{\#features} & \textbf{\#classes} & \textbf{feature type}\\ \hline
     1&Bank\tablefootnote{Available at: mldata.io/dataset-details/bank\_marketing/}& 4521&16&2& categorical and numerical \\ \hline
     2&Cars\tablefootnote{Available at: mldata.io/dataset-details/cars/}& 1728&6&4& categorical \\ \hline
     3&Diabetes\tablefootnote{Available at: data.world/data-society/pima-indians-diabetes-database}& 768&8&2& numerical \\ \hline
     4&Messidor\tablefootnote{Available at: archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set}& 1151&19&2& numerical \\ \hline
     5&USPS\tablefootnote{Available at: kaggle.com/bistaumanga/usps-dataset}& 9298&256&10& numerical \\ \hline
     6&Vehicle\tablefootnote{Available at: mldata.io/dataset-details/vehicle\_silhouette/}& 846&18&4& numerical \\ \hline
     7&Wine\tablefootnote{Available at: scikit-learn.org/stable/modules/generated/sklearn.datasets.load\_wine.html}& 178&13&3& numerical \\ \hline
     8&OBS\tablefootnote{Available at: archive.ics.uci.edu/ml/datasets/Burst+Header+Packet+\%28BHP\%29+\\flooding+attack+on+Optical+Burst+Switching+\%28OBS\%29+Network}& 1075&21&4& categorical and numerical \\ \hline
    
    \end{tabular}

    \caption{Public datasets.}
    \label{table::public_datasets}
\end{table}
\endgroup

All categorical features were one-hot encoded, ie. transformed by function $\zeta: \{k_1,...,k_m\} \rightarrow \mathbb{R}^m$, where $\{k_1,...,k_m\}$ is a set of categories and $m \in \mathbb{N}$ is a number of categories. It is defined $\forall j,l \in \{1,...,m\}$ as 
\begin{equation}
    [\zeta(k_j)]_l = \begin{cases}
    1, & j = l \\
    0, & \text{otherwise}
    \end{cases} \hspace{0.2cm}.
\end{equation}
Eventually, all dataset features (including newly created ones by one-hot encoding) were standardized (normalized to mean equals 0 and standard deviation equals 1 ) to ensure better numerical stability. It is done by formula $\hat{x}_j = \frac{x_j-\mu_j}{\sigma_j}$, where $x_j$ is a single realization of $j$-th feature, $\hat{x}_j$ is a new value of this realization, $\mu$ is the mean value of realizations of $j$-th feature and $\sigma_j$ is the standard deviation of realizations of $j$-th feature. This normalization had also positive effect on performance since all evaluation metrics were more favourable in the case of normalization than without normalization.

Final results of macro-average of F1-score resp. accuracy are presented in Table \ref{tab:results_macro_public} resp. Table \ref{tab:results_accuracy_public} (results in both tables correspond to the original values multiplied by 100).

\begingroup
%\verb|\medmuskip=0mu:|
%\setlength{\medmuskip}{0mu}
\begin{table}[h]
    \centering
    \small\addtolength{\tabcolsep}{-5pt}
    \makebox[\textwidth][c]{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
     DATASET & NN&LR&RF20&RF30&RF50&NRF\_DW&NRF&NRF\_EL&NRF\_EL\_DW\\ \hline
     Bank&$69.3\pm1.8$&$68.6\pm2.5$&$55.5\pm2.0$&$55.7\pm2.2$&$54.6\pm2.4$&$\boldsymbol{71.9}\pm3.2$&$71.6\pm1.5$&$71.7\pm2.6$&$\boldsymbol{71.9}\pm2.7$ \\ \hline
     Cars&$99.8\pm0.4$&$66.6\pm5.4$&$61.5\pm6.4$&$59.5\pm4.9$&$60.1\pm6.3$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$ \\ \hline
     Diabetes&$68.9\pm3.0$&$\boldsymbol{73.0}\pm4.8$&$72.7\pm4.7$&$72.3\pm2.8$&$72.7\pm2.9$&$71.6\pm3.9$&$72.7\pm2.4$&$72.3\pm3.1$&$71.6\pm5.2$ \\ \hline
     Messidor&$71.3\pm3.0$&$72.0\pm1.5$&$67.4\pm3.0$&$70.0\pm3.7$&$65.7\pm2.1$&$\boldsymbol{73.8}\pm2.6$&$73.5\pm1.0$&$73.7\pm2.3$&$73.6\pm3.7$ \\ \hline
     USPS&$94.0\pm1.9$&$92.6\pm1.2$&$90.2\pm1.8$&$90.9\pm1.7$&$90.9\pm1.3$&$\boldsymbol{97.3}\pm1.4$&$97.1\pm1.5$&$97.0\pm1.4$&$97.0\pm1.3$ \\ \hline
     Vehicle&$81.3\pm3.3$&$77.3\pm2.5$&$71.8\pm3.5$&$72.1\pm1.9$&$71.4\pm3.0$&$\boldsymbol{84.2}\pm3.5$&$\boldsymbol{84.2}\pm2.3$&$83.2\pm3.1$&$83.9\pm3.0$ \\ \hline
     Wine&$97.7\pm1.9$&$98.2\pm2.1$&$97.9\pm2.0$&$97.5\pm3.0$&$97.9\pm2.0$&$97.9\pm2.3$&$97.9\pm2.4$&$97.5\pm2.0$&$\boldsymbol{98.5}\pm2.1$ \\ \hline
     OBS&$99.4\pm0.4$&$99.2\pm1.0$&$90.2\pm2.0$&$91.9\pm1.9$&$92.4\pm1.8$&$99.8\pm0.4$&$\boldsymbol{99.9}\pm0.3$&$99.6\pm1.0$&$\boldsymbol{99.9}\pm0.2$ \\ \hline
    \end{tabular}
}
    \caption{Macro average of F1-score.}
    \label{tab:results_macro_public}
\end{table}

\begin{table}[h]
    \centering
    \small\addtolength{\tabcolsep}{-5pt}
    \makebox[\textwidth][c]{
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
     DATASET & NN&LR&RF20&RF30&RF50&NRF\_DW&NRF&NRF\_EL&NRF\_EL\_DW\\ \hline
     Bank&$88.1\pm1.0$&$\boldsymbol{90.0}\pm1.4$&$89.1\pm0.8$&$89.1\pm0.8$&$89.1\pm1.0$&$89.6\pm0.6$&$89.4\pm0.7$&$89.8\pm0.8$&$89.5\pm1.0$ \\ \hline
     Cars&$99.9\pm0.2$&$88.5\pm1.8$&$89.6\pm1.3$&$89.6\pm1.5$&$90.2\pm2.1$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$ \\ \hline
     Diabetes&$71.9\pm3.1$&$\boldsymbol{76.9}\pm4.4$&$76.4\pm4.1$&$76.2\pm2.0$&$76.7\pm2.2$&$74.6\pm3.7$&$75.8\pm2.6$&$75.0\pm2.8$&$74.3\pm5.1$ \\ \hline
     Messidor&$71.3\pm2.9$&$72.1\pm1.5$&$67.5\pm2.9$&$67.1\pm3.7$&$65.9\pm1.9$&$\boldsymbol{73.8}\pm2.7$&$73.5\pm1.0$&$\boldsymbol{73.8}\pm2.3$&$73.7\pm3.6$ \\ \hline
     USPS&$94.6\pm1.9$&$93.4\pm1.2$&$91.3\pm1.8$&$91.9\pm1.7$&$91.9\pm1.4$&$\boldsymbol{97.6}\pm1.4$&$97.4\pm1.4$&$97.3\pm1.3$&$97.3\pm1.2$ \\ \hline
     Vehicle&$81.4\pm3.1$&$77.9\pm2.6$&$73.2\pm4.1$&$73.5\pm1.7$&$72.8\pm3.1$&$84.1\pm4.1$&$\boldsymbol{84.3}\pm2.3$&$83.3\pm2.4$&$83.9\pm3.0$ \\ \hline
     Wine&$97.7\pm1.8$&$98.3\pm2.0$&$97.8\pm3.2$&$97.8\pm2.9$&$98.0\pm1.9$&$97.8\pm2.2$&$98.0\pm2.3$&$97.5\pm2.1$&$\boldsymbol{98.6}\pm2.0$ \\ \hline
     OBS&$99.3\pm0.5$&$99.1\pm0.9$&$87.7\pm2.5$&$89.5\pm2.6$&$90.2\pm2.5$&$99.7\pm0.6$&$\boldsymbol{99.9}\pm0.4$&$99.6\pm0.9$&$\boldsymbol{99.9}\pm0.3$ \\ \hline
    \end{tabular}
    }
    \caption{Accuracy.}
    \label{tab:results_accuracy_public}
\end{table}
\endgroup

From the first observation we may notice that all models performed very similarly on the majority of datasets with respect to both researched evaluation metrics. Essentially, overall performance of NF models in comparison with other competitive models is in fact equal or slightly better than in case of concurrent models. There is no significant deterioration of any of NF models compared to other models, especially compared to the RF models. On the contrary, we can see enhancement of values of NF models in most cases.

The most significant differences between NF models and RF models could be seen in the cases of Cars, Messidor, USPS, Vehicle and OBS datasets, where NF models have superior performance in comparison with RF models. On Bank dataset is performance of NF models basically equal or only slightly better. Only exception is visible in case of Diabetes dataset, where NF models do not produce any improvement and perform similarly or sligthly worse than RF models. It could be caused by many influencing factors, especially inappropriate choice of hyperparameters of NF models. The effect of hyperparameters on final performance will be researched later.

NF models among themselves produce very similar results. NRF\_DW and NRF\_EL\_DW have almost same performance on all datasets with very small differences, which is a good sign, since they both aim to simulate random forest already before training individual estimators with backpropagation and therefore have comparable initial conditions. Moreover, NRF\_DW has one hidden layer less, so stacking more layers do not necessarily mean better performance. NRF and NRF\_EL have relaxing layers with random weights, so there is more space left for individual neural networks to forget about decision tree initial knowledge and adapt in its own way. This could be beneficial especially in cases where RF models produce superior performance in comparison with other models and therefore there is not much space for further significant improvement if RF model is imitated. This is nicely demonstrated in the case of Diabetes dataset, where RF models produce fine results in the beginning and then NRF\_DW and NRF\_EL\_DW are not successful in further improving it. It even reached slightly "worse" results (note the quotation marks since there is a relatively high standard deviation, which makes it hard to distinguish better model from the worse one). On the other hand, NRF and NRF\_EL managed to come fairly close to RF model performance or even equalize it and have slightly better and more stable (lower standard deviations) performance than NRF\_DW and NRF\_EL\_DW in this case. Of course, better choice of hyperparameters of NRF\_DW and NRF\_EL\_DW (especially number of training epochs and transition parameters $\beta_1$ and $\beta_2$ which control the quality of imitation of a single decision tree) could lead eventually to surpassing even the well-performing RF. This issue will be studied later in the thesis.

\subsection{The effect of hyperparameters}

In this section are studied various combinations of hyperparameters of NF and RF models and different visualizations are provided to compare the results. Knowledge of RF model features and applied NF model beforehand could define suitable range of hyperparameter values, which could result in correct choice and superior performance of the model.

We use Vehicle and Diabetes datasets to analyze performance based on hyperparameter choice. RF model performed faintly on Vehicle dataset and finely on Diabetes dataset in comparison with other models, thus NF models initialized with RF models trained on those two datasets cover situations that could arise in practical use: boosting well-performing RF or poor-performing RF by NF. If we choose bad hyperparameters in the case of poor RF, the resulting NF could be poor as well and if we choose bad hyperparameters in the case of fine RF, the resulting NF could perform worse than the corresponding RF, which is undesirable.

All graphical visualizations are presented in the \autoref{figures} and referenced in this section.  Unless stated otherwise, all experiments were performed as 2 subsequent independent 5-fold cross validations (10 iterations in total) and outcomes are shown in the form "$\text{mean} \pm \text{std}$" in the corresponding figures. Also if the results for RF model are presented together with results of NF models in one figure, then this RF model was used to initialize all NF models in the same figure. In all following experiments, the hyperparameters of NF models which are not examined at the time remain same as defined in the previous section.

\subsubsection{Learning rate}

Learning rate stands for one of the most crucial hyperparameters that need to be set conscientiously in order to achieve quality performance. This choice usually depends on model type and dataset itself. It is done almost exclusively by exhaustive grid search on some specific validation dataset before picking a suitable value for the training phase. It is not possible to state versatile learning rate optimal for all applications.

We study different settings for learning rates in our model and research the sensitivity of NF models to the learning rate choice. We compare this sensitivity with same feedforward NN as applied on public dataset experiments (defined in \autoref{public:experiments}).

NF and corresponding RF models were set in the same manner as was described in the beginning of the \autoref{public:experiments} with 20 estimators. Graphs for varying learning rates are presented in the Picture \ref{learning_rates} for the case of Vehicle dataset. Graphs for Diabetes dataset are in the Figure \ref{figures:learning_rates_diabetes}. We present only accuracy results, since both visualizations of accuracy and F1-score have very similar trend and shape. Moreover, both datasets are balanced or have very low degree of imbalance between classes, so referring to accuracy is safe in this case.


Based on the Figure \ref{learning_rates}, we can see that in all cases the optimal value for learning rates could be found close to zero, approximately in the range $(0.001,0.02)$. With increasing value of learning rate, the performance of models is declining. The steepest downfall is registered in the case of NN model, where for the learning rate equals 0.1 drops accuracy to approximately 50\% of maximum reached accuracy. On the other hand, NF models did not suffer from such eminent decrease in the given range. This experiment suggests, that NF models could be slightly less sensitive to learning rate as single NN, but it really depends on the specific application. Resembling trend could be seen in the Figure \ref{figures:learning_rates_diabetes} on Diabetes dataset, altough not that vigorous. In this case the accuracy of NF models fluctuates a bit around almost constant value (mild decrease at the end of the range) in the given range of learning rates, whereas in the case of NN we can notice early decrease of accuracy in the range $(0.001,0.12)$ and then it remains basically constant. Consequently, we could choose suitable learning rates from wider range in the case of NF models than in the case of single NN.

\subsubsection{Number of epochs and number of estimators}

Another important hyperparameter of NF is a number of training epochs. Especially in the case of NF models which simulate RF behaviour since the beginning, they could improve the performance of initial RF already after few epochs, which could be time-efficient. We will investigate this issue alongside the effect of number of estimators in NF (RF), because high number of estimators could compensate low number of training epochs and vice versa. The fixed hyperparameters of NF and RF remain same as in the previous experiments.

First we will focus on the experiments on Vehicle dataset showed in the Figure \ref{figures:estimators_epochs_vehicle}. This is the case when RF model does not perform well in the comparison with other models, as was already seen in tables \ref{tab:results_macro_public} and \ref{tab:results_accuracy_public}. One may notice apparent pattern that increase of a number of training epochs causes enhancement of accuracy. This happens among all NF models and also in the case of single NN. NRF\_DW and NRF\_EL\_DW models could outperform initial RF already after 5 epochs of training and reach slightly higher values of accuracy than NF models with one layer of random weights until 30 epochs of training. For higher number of epochs are results very similar among all NF models except of NRF\_EL, which has slightly lower accuracy. Also NRF\_DW and NRF\_EL\_DW converge faster to the maximum values of accuracy, acquiring it at approximately 30 epochs of training, whereas other NF models still register increase of accuracy with higher number of training epochs. It could be due to the nature of individual models, where NRF\_DW and NRF\_EL\_DW imitate RF more precisely than other NF models and therefore could be closer to some optimal value of loss function than NRF and NRF\_EL since the beginning. Therefore they are more likely to reach their optimal performance earlier.

We can see only small differences in accuracy values with varying number of estimators. With increasing number of estimators the accuracy values usually also increase, but a number of epochs is more distinguishing factor. 

Another situation occurs in the case of Diabetes dataset, as illustrated in the Figure \ref{figures:estimators_epochs_diabetes}. This is the case of RF model performing well in the comparison with other models.

In this case arises quite an opposite situation than in the previous case on Vehicle dataset. With increasing number of training epochs is accuracy of NF models decreasing. All NF models except of NRF\_EL outperform RF with any number of estimators in the given range already after 5 epochs of training. They also reach their maximum accuracy in the given range of epochs. With higher number of epochs the accuracy decreases, eventually ending up with a worse model than initial RF. It could be caused by the problem of overfitting, which will be examined later. Especially in the case of NRF\_DW and NRF\_EL\_DW, the accuracy is decreasing quickly. In the case of NRF\_EL, the accuracy acquires its maximum value at approximately 20 epochs and then is slowly decreasing. It is beneficial to use early stopping\footnote{Early stopping is a method that check accuracy or error rate on validation dataset after each epoch and stops the training if the performance of the model stops improving.} technique during training NF models to avoid this problem. Moreover, single NN model follows the same pattern of decreasing accuracy with increasing number of epochs as NF models, which suggests that neural networks in this case converge quickly to optimal solution and with increasing number of epochs overfit to the problem. One noticeable fact is that performance of NF models degradates slower than single NN model, which may be caused by the benefit of ensemble voting (or averaging) of NF.

\subsubsection{Maximum depth of trees in RF}

Another examined hyperparameter of NF is a number of neurons in the first and second hidden layer of the individual neural networks. This number is fully controlled by the depth of the corresponding tree. We will examine the influence of maximum depth of trees in RF (which is set before the training of RF) on performance of NF. Especially for overgrown trees there is a significant risk of overfitting either in the case of the decision tree or subsequently in the case of transformed neural network, which will be tested. We also add number of training epochs of NF as another variable.

We only present graphs created on Diabetes dataset for NRF\_DW and NRF models, because for NF models with extra added layer the trend has turned out to be same. We added also values of Cross Entropy loss function on testing datasets in order to demonstrate overfitting. Accuracy and values of loss function depending on the maximum depth and number of epochs is depicted in the Figure \ref{figures:depth_epochs}.

It can be seen in the graphs that NRF\_DW model is much more prone to overfitting than NRF model, which is logical, since NRF\_DW model simulate RF behaviour better already before training with backpropagation. This applies especially to larger maximum depths (starting from maximum depth equals 6), where we can register fast decrease in accuracy and increase in loss function with increasing number of training epochs. In the case of NRF this decrease is not that vigorous and straightforward, but for larger depths is still visible.

In the Figure \ref{figures:depth_epochs_reg} are illustrated same models with regularization constant increased to $\lambda = 10$, which intensifies the influence of regularization term in the loss function. We can see that this modification prevent the models from the significant decrease of accuracy especially for the larger numbers of epochs, but in majority of depth values the maximum reached accuracy among those two variants remains favourable in the first case from the Figure \ref{figures:depth_epochs}. Still the differences are very small.

\subsubsection{Transition parameters $\beta_1$ and $\beta_2$}

Transition parameters $\beta_1$ and $\beta_2$ control the degree of transition of RF to NF. The higher both values are, the better approximation of RF is obtained by NF. Under certain circumstances it could be advantegous to increase those parameters and approximate RF accurately, especially when RF produces competitive results. Accuracy depending on both transition parameters is depicted for Vehicle dataset resp. Diabetes dataset in the Figure \ref{figures:beta_epochs} resp. Figure \ref{figures:beta_epochs_dia}.

In the case of Vehicle dataset we can observe that it is not beneficial to increase $\beta_1$ and $\beta_2$ since the highest reached accuracy belongs to the point $\beta_1 = 1$ and $\beta_2 = 1$ among all NF models. Since RF model performs badly in comparison with other models, this suggests that copying this RF model makes the subsequent NF model performs worse than it could when $\beta_1$ and $\beta_2$ are kept low and NF model is more relaxed. Increasing both values simultaneously causes deterioration of the results almost monotonically.

The situation is different in the case of Diabetes dataset in the Figure \ref{figures:beta_epochs_dia}. In the contrast with the Vehicle dataset, we may observe some pairs of $\beta_1$ and $\beta_2$ other than (1,1) that are more beneficial in terms of higher accuracy. Even points (12,12) perform more favourably in the most cases of NF models than point (1,1). This suggests that taking advantage of simulating well-performing RF model more accurately is the step towards better performance of NF.

Be aware of a problem of choosing transition parameters too high since it could negatively affect optimization process of neural networks. For high transition parameters the tanh functions approximate threshold functions too precisely and therefore the neurons in the first and second hidden layer could suffer from early saturation\footnote{The state of saturation in the terms of neural networks refers to the neuron which mainly outputs values close to the asymptotical endings (for tanh function it would be $\pm1$) \cite{saturation}. Since the gradient in this range is close to 0, it is unlikely that the current value of neuron is going to change significantly in the following iterations.}. Consequently, this could make the learning process of neural networks more difficult.

\subsubsection{Max. considered features in the split}

We also examined maximum number of considered features in the split of RF and its influence on the performance of NF. Standardly we leave this hyperparameter on the value $\lfloor\sqrt{d}\rfloor$, where $d \in \mathbb{N}$ represents number of features in the dataset. We tested the NF performance for values of max. considered features set to 0.2, 0.4, 0.6, 0.8 and 1, where each number symbolizes the fraction of considered features with respect to $d$. Number of estimators has been set to 20.

So far we can claim from the performed experiments on Vehicle dataset and Diabetes dataset, the effect of this hyperparameter is not very significant and differences between various settings of this hyperparameter are negligible.

\subsubsection{Summary of the previous observations}

In this part we shortly sum up the provisional experiments with NF and RF hyperparameters performed in the previous paragraphs of this section.

We saw that the sensitivity of learning rate has been slightly lower in the case of NF models than in the case of single NN. This could be due to the ensemble nature of NF with variety of single specific NN estimators, each differently sensitive to the current learning rate value. Overall sensitivity of NF model to the learning rate is then "averaged" over single NN estimators and therefore could be reduced in comparison with only single NN. Note that this comparison has been done for other parameters fixed and the sensitivity to the learning rate could correlate with other hyperparameters too, mainly with the transition parameters $\beta_1$ and $\beta_2$. So the most suitable learning rate should be picked after some grid search on the validation dataset with other hyperparameters set beforehand.

Following observations based on varying number of training epochs and other hyperparameters, such as maximum depth or number of estimators, offered another interesting perspective. Especially the NF models initialized with RF consisting of trees with large depth are more prone to overfitting then in the case of lower depths, so it should be beneficial to prevent overfitting by employing techniques such as early stopping or adding a regularization term to the loss function (L2-regularization for instance). The differences between results of NF models with varying number of estimators were small, slightly favourable in the case of larger number of estimators, as corresponds to the logical intuition.

As another hyperparameters were investigated the transition parameters $\beta_1$ and $\beta_2$. It turned out that in the case of poor-performing RF it is beneficial to leave both values low (for instance $\beta_1 = 1$ and $\beta_2 = 1$) and start with relaxed NF that do not simulate that RF accurately. On the other hand, when operating with well-performing RF with competitive results, it could reach more favourable performance when simulating RF more accurately by increasing both transition parameters. This issue will be also studied in the next chapter.

Last studied hyperparameter was the number of maximum considered features in the split of RF. From the performed experiments we did not detect a significant dependence influencing the performance of subsequent NF.

\section{Toy dataset experiments}

This section comprises experiments on artificially created data in 2D, where we draw and compare decision boundaries learned by individual methods and search for properties and inclinations of NF models based on varying hyperparameters and different properties of datasets.

We conducted the experiments by generating a various datasets, each one of different shape and trend. Moreover, we investigate datasets of different random noise and test the ability of classifiers to find the correct decision boundary and not to overfit on noise data, as tend to do especially overgrown RF models.

Each dataset was split to the training and testing set with ratio of 6:4 and the decision boundary was drawn on the input space. It is difficult and not straightforward to aggregate decision boundaries of several NFs and visualize it while preserving correct interpretability, especially when classifying into more than two labels. Suppose that we trained 4 independent NF classifiers on 3-class dataset (classes are denoted simply as 0,1,2) and decisions of NFs are subsequently [0,0,1,2] in that point. Also suppose that each class occupies different color (defined as a vector (R,G,B,$\alpha$), where R,G,B denote typical red,green,blue triplet and $\alpha$ denotes transparency level). If we pick the most frequent decision (or average the decision probabilities) and draw the point with the color of the most favourable class, then the result corresponds to the decision of an ensemble method consisting of several NFs and that is not what we desire to illustrate. Also we cannot depict all decisions in the picture, since the combination of 3 colors could result in an uninformative colors that does not capture the trend. For 2-class case, it is possible to illustrate the frequency of decisions by adopting the color gradient between two colors, but since we work also with 3-class datasets, we will not present the results separately in a different way for 2-class datasets.

Instead, the decision boundaries and their tendencies are for individual NFs demonstrated on multiple datasets, each of varying size, and for varying hyperparameters of NFs. In each studied case, the single NF is trained on one training set, which remains same for all presented models. As the hyperparameters change, we can see the evolution of decision boundaries of different NFs and their inclinations. We also present a single value of accuracy on testing set for all models only for quick comparison, but since this corresponds to only one realization, it should not be decisive in the final comparison of models. The comparison is more stable and reliable when comparing visually the decision boundaries (it is possible in 2D) in general and comparing their generalization ability.

\subsection{Number of epochs}

In this part we investigate the effect of varying number of training epochs on evolution of individual decision boundaries. Decision boundaries are drawn on 3 types of artificial datasets, each type is also depicted for different sizes, specifically for 50 and 200 instances. RF model used for NF initialization has 10 estimators, maximum depth of individual trees is set to 6 and number of features considered in the split is set to 1 (since $d = 2$ in this case). Transition parameters $\beta_1$ and $\beta_2$ of NF models are set to 1 and we use Adam as optimization algorithm (parameters of Adam and other hyperparameters of NF models are same as in the \autoref{experiments}). NF models are compared with single NN model of same hyperparameters as defined in the \autoref{experiments} and also with RF model used for initialization of all NF models.

All illustrations are depicted in the Figure \ref{figures:epochs_toy1}, Figure \ref{figures:epochs_toy2} and Figure \ref{figures:epochs_toy3}. We can see that in the cases of 50 input instances the NRF\_DW and NRF\_EL\_DW models manages to tune finely already after 5 epochs of training, whereas NRF and NRF\_EL do not adapt very reliably and their decision boundary is very far from ideal separation (for 5 to 30 training epochs). For larger number of training epochs NRF\_EL could establish decision boundaries competitively with other models, but NRF model could not. NRF model even struggle to learn competitive separation in the case of 200 input instances and can catch up slower than other models, approximately around 30 training epochs. Eventually for higher number of input instances the NRF model could perform competitively with other models.

Also we can highlight the striking resemblance of decision boundaries of NRF\_DW and \\NRF\_EL\_DW in all cases and even for every single number of training epochs. This proves the concept since both models are deterministic and they target to simulate RF, therefore they should share some similarities. 

This experiments suggests that models simulating the RF (NRF\_DW and NRF\_EL\_DW) share some similarities and could learn faster than other NF models. Moreover, they can adapt better to the problems with less training inputs.

\subsection{Transition parameters $\beta_1$ and $\beta_2$}

This part comprises experiments based on varying transition parameters $\beta_1$ and $\beta_2$, which control the accuracy of transition of RF to the corresponding NF. We will keep these parameters simultaneously on the same values and increase them at the same time, since increasing of both values results in more accurate simulation of RF. We use $\beta$ to denote both values $\beta_1$ and $\beta_2$ simultaneously, thus $\beta = \beta_1 = \beta_2$.

We use same RF model for initialization of all NF models. NF models are trained for 25 training epochs and other hyperparameters of NF models and RF remain same as defined in the previous part. We show decision boundaries on 3 different types of datasets, each consisting of 50 and 200 instances.

The illustrations of decision boundaries depending on $\beta$ are depicted in the Figure \ref{figures:betas_toy1}, Figure \ref{figures:betas_toy2} and Figure \ref{figures:betas_toy3}. We can see that in case of 50 data inputs the NRF model could not tune its decision boundary sufficiently even after 25 training epochs (for all $\beta$s), which further confirms that this model is not suitable when working with low number of training inputs. For 200 instances is NRF model able to catch up and produce competitive decision boundaries with other models.

For $\beta = 1$ all NF models produce various nonlinear decision boundaries, which are more adaptable then RF decision boundaries parallel with axes. They are able to create more complex (e.g. round or ellipsoidal related) shapes. This means that for low $\beta$s the NF models could benefit more from their neural network properties and therefore could tune better to the problems that cannot be solved properly by finite number of splits by RF, e.g. 3 classes ordered in concentric nested spheres made of data generated by standard normal distribution, as could be seen in the Figure \ref{figures:betas_toy3}.

When $\beta$ is increasing, we can see that the decision boundaries of NF models gradually converge to the decision boundary of corresponding RF model. Especially in the case of NRF\_DW and NRF\_EL\_DW the resemblance is obvious and the decision boundaries for $\beta = 50$ are almost identical. Also with increasing $\beta$ the decision boundaries of NF models evolve more in the RF manner, preferring more perpendicular and parallel shapes, as does RF itself. It is nicely demonstrated in the Figure \ref{figures:betas_toy3} in the case of 200 data inputs, where for $\beta = 1$ are the decision boundaries of NF models in a great contrast with the RF boundary. They form concentric spheres which separate the classes, whereas the boundary of RF is created as concentric rectangles. But already for $\beta = 5$ the decision boundary of individual NF models become rectangular as well.

We see that all NF models with higher values of $\beta_1$ and $\beta_2$ simulate initial RF quite precisely. For lower values of $\beta$ is resemblance less visible and the decision boundaries of NF models look more similar to the NN decision boundary, as could be seen for instance in the Figure \ref{figures:epochs_toy3}, where both transition parameters are set to 1.

\subsection{Noise in data}

In this part we investigate the influence of noise in data on NF decisions. We conduct these experiments in two variants, the first for $\beta = \beta_1 = \beta_2 = 1$ and the second for $\beta = \beta_1 = \beta_2 = 50$. We fix the size of dataset to 100 and all NF models are trained for 25 training epochs. All other hyperparameters of NF, RF and NN remain same as in the previous experiments. The noise is randomly generated from the normal distribution and takes on values ranging from 0.1 to 0.5. Each value denotes standard deviation of normal distribution that generates noise.

If noise occurs among data, the RF model in general tends to overfitting on this noise, especially when the individual trees are overgrown (large depth) and unpruned. We depict the dependence of decision boundaries on the noise in the Figure \ref{figures:noise_toy}.

We can see that in the case of $\beta = 1$ all NF models establish a fine separation of data despite the increasing amount of noise, whereas the RF model produces signs of overfitting (demonstrated by small areas cut out in the territory of different class). In this case the overfitting of RF model was not simulated by NF models and all NF models and NN model produced sufficiently generalized boundaries.

A different situation arises when we increase both transition parameters to $\beta = \beta_1 = \beta_2 = 50$. Then we could see a significant signs of overfitting among all NF models, especially in the case of NRF\_DW and NRF\_EL\_DW, which simulate the RF accurately and thus share similar properties as RF. This problem should be resolved by decreasing the transition parameters or adopting some regularization techniques, as will be briefly investigated in the next part.

\subsection{Overfitting and regularization}

When initializing NF models with RF consisting of a number of overgrown trees, there exists a significant risk that RF overfits the data and therefore even a subsequent NF model could be prone to overfitting as well, especially if the transition parameters are larger than 1. One way to avoid this problem is to decrease the transition parameters or add a regularization term to the loss function, where the overfitting should be controlled by modifying the regularization parameter $\lambda$.

We demonstrate the benefit of using L2-regularization term in the case of NRF\_DW and NRF\_EL\_DW models initialized with overtrained RF suffering from overfitting. It is done on artificially created dataset consisting of 100 instances and divided into 2 classes as depicted in the Figure \ref{figures:toy_reg}. Transition parameters are set to $\beta_1 = \beta_2 = 10$. The results were obtained after 2 subsequent 5-fold cross validations (10 iterations in total) and are presented in the Figure \ref{figures:toy_reg}. Apparently, adding the regularization term is helpful in this case, resulting in an increase of accuracy among both NRF\_DW and NRF\_EL\_DW models. Unfortunately, the most suitable value of regularization constant cannot be stated generally and depends on specific data and model. Therefore it should be picked after a grid search and cross validation.

\subsection{Commentary on the space and time complexity}

One paragraph, mention parallelization - time complexity is same as a single NN if trained on a sufficient number of CPUs. It could be further accelerated by GPU. Viděl jsem, že se v článcích příliš neřeší time complexita neuronek, ale můžeme uvést nějakou O() notaci. Nepřijde mi to ale příliš zajímavé, možná můžeme uvést jen odkaz na článek, kde je odvozena complexita jedné neuronky, každý si pak může dosadit tam. Co se týče space complexity, tam bychom museli odvodit, co všechno si musí neuronka pamatovat, aby se mohla natrénovat , tzn. všechny váhy a biasy + všechny potřebné mezivýpočty backpropu + hyperparametry - opět podle mě ne příliš zajímavá informace, řešeno v nekonečně mnoha článcích nejspíše. A je to možná na delší odvozování

\section{LUCAS dataset experiments}

In this part we focus on applying proposed NF models on the real-world dataset, which was obtained from the data collected by LUCAS project \cite{lucas}. This project focuses on patients diagnosed with bronchogenic carcinoma (lung cancer) and their treatment and examination. Unfortunately, the diagnosis of lung cancer belongs to the most severe and complicated cancer diseases ever. Only in the Czech Republic dies approximately 5400 patients per year on such diagnosis, which ensures sad primacy among all other tumor diseases.

LUCAS project was developed to thoroughly describe and analyze the complex path of such patients through the treatment process already from the diagnosis and therefore to provide subsequent analytical studies evaluating the scope and structure of the overall care, pharmacoeconomic background, overall survival and the sequence and efficiency of the treatments. All of these outcomes could help to enhance the diagnostic and treatment system and therefore could ensure superior care about patients diagnosed with lung cancer.

This project was launched on 1.6.2018 and already includes data of 2285 patients (in the period from 1.6.2018 to 14.5.2020). The data contain various information about the individual patients, including diagnostic records, personal characteristics, treatment records and information about patient's progress. The data are collected from seven medical centers in the Czech Republic - FN Olomouc, FN Hradec Králové, FN Brno, FN Motol, FN Plzeň, Nemocnice Na Bulovce Praha and Thomayerova nemocnice Praha.

The treatment options have widely expanded in the last couple of years and right now they include chemotherapy, radiotherapy, immunotherapy, targeted therapy and operational interventions. Since there exist only few tumors that could be removed by operation intervention, the other treatments are gaining great importance, especially targeted therapy and immunotherapy, which may prolong the life of patients by several years. But still the chemotherapy is more common choice and is often accepted as the preferred choice in the first line treatment (first treatment after diagnosis).

\subsection{Classification task and data}

For the overall survival of patients is in addition to the early diagnosis important also the very first treatment after the diagnosis. From the available data of LUCAS project this first treatment (among pharmacotherapy) is usually chemotherapy in most cases, but from the previously mentioned, it could be sometimes beneficial to replace chemotherapy (if the patient and illness have required properties) with other kinds of treatment, especially with targeted treatment and immunotherapy. Also it could be used in combination with the chemotherapy as well.

We test the ability of our NF models to predict the first line treatment based on patient's diagnostic and personal records. We define binary classification task with negative class (class 0) symbolizing chemotherapy and positive class (class 1) symbolizing all other treatments belonging to the pharmacotherapy (especially targeted therapy and immunotherapy). The solution of this classification task could help the doctors to decide on the choice of the first line treatment and potentially highlight the possibility of employing other treatments besides chemotherapy.

Overall we have 1133 records with any form of pharmacotherapy in the first line treatment, of which there are 937 cases of chemotherapy (class 0) and 196 cases of other treatment (class 1). We could notice that this dataset is significantly imbalanced with the negative instances to positive instances ratio equal approximately to 7:1. This makes this classification task challenging for many classifiers.

Unfortunately, according to the non-disclosure agreement we cannot describe the dataset in full detail, so at least we provide a shallow description. The dataset consists of 21 features, which were picked from the diagnostic and personal data of individual patients. There are 6 numerical features including for instance height and weight of patients, number of years of smoking, etc. Other 15 features are categorical which comprise symptoms, details of diagnosis, tumor type, tumor stage, etc. All categorical features were one-hot encoded, resulting together with numerical features in 91 final features. Finally, all features were standardized (normalized to mean value equal 0 and standard deviation equal 1).

\subsection{Models and evaluation}

Since our classification task corresponds to the binary classification on imbalanced dataset, we evaluate the performance mainly with respect to the precision and recall of the positive class. Moreover, we present the comparison of individual models with respect to the Precision - Recall (PR) curves and also according to the average precision (AP) score as defined in the Chapter [Evaluation]. Eventually, we present macro-average of F1-score and accuracy as well. We conducted 2 subsequent 5-fold cross validations (10 iterations in total) and we present final results in the form "mean$\pm$std", similarly as in the public experiments. To all individual iterations of single training - testing pairs correspond one PR curve and we visualize all iterations in one graph for each classification model. Additionally, we draw one PR curve corresponding to the concatenation of correct labels and predictions from all iterations. Since there is no natural and straightforward way (such as averaging) to combine PR curves from individual iterations into one, this PR curve serves as "aggregation" of individual PR curves and it is highlighted in the corresponding graphs. From now, we will refer to this PR curve as aggregation PR curve. All individual PR curves have also their AP (average precision score) and at aggregation PR curve we present mean value of individual APs and standard deviation of these APs (but not AP of aggregation PR curve, since this is hardly intepretable and we want it to represent the overall performance only visually).

We employ two NF models - NRF\_DW and NRF\_EL\_DW and compare their performance with Logistic Regression classifier, single NN and the RF used for initialization of NF models. We use same logistic regression model with L2 regularization as in the public experiments. Single NN has two hidden layers, each consisting of 150 neurons with ReLU activation functions and the softmax in the output layer. We use Cross Entropy as the loss function and Adam as optimization algorithm. All other hyperparameters remain same as defined for single NN in the public experiments.

RF consists of 30 decision trees, maximum depth is set to 6 and the number of features considered in the split is set to $\lfloor \sqrt{d} \rfloor$, where $d \in \mathbb{N}$ denotes number of features. We employ entropy as impurity function in information gain criterion.

NF models were initialized with RF model described above, with transition parameters set to $\beta_1 = 10$ and $\beta_2 = 10$. Learning rate for NRF\_DW is set to 0.002 and for NRF\_EL\_DW to 0.0025. We use Cross Entropy as loss function and Adam as optimization algorithm with same hyperparameters as single NN model. This time we omit adding the regularization term to the loss function and instead we employ early stopping technique (we use it also in the case of single NN model). This technique monitors the loss function on validation dataset (different from the testing dataset) after each epoch. It tracks the loss function for 10 subsequent epochs from each new epoch and if there is no occurrence of decrease, the training process is terminated. 

The testing fold in each iteration of cross validation was randomly divided into two halves, one used as validation dataset for early stopping and the rest for testing.

\subsection{Results}

The results of individual models are presented in the Table \ref{table::real_dataset} (original values multiplied by 100).

\begingroup
\begin{table}[h]
    \centering
    %\small\addtolength{\tabcolsep}{-5pt}
    \small\addtolength{\tabcolsep}{-5pt}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
     \multirow{2}{*}{MODEL}&\multicolumn{3}{c|}{Class 0} &\multicolumn{3}{c|}{Class 1}& \multirow{2}{*}{Macro-avg F1}& \multirow{2}{*}{Accuracy}\\ \cline{2-7}
     & Precision & Recall & F1-score &Precision & Recall & F1-score & & \\ \hline
     LR & $89.6 \pm 1.9$&$96.3 \pm 1.8$ & $92.8 \pm 1.3$&$72.1 \pm 13.7$&$46.6 \pm 12.1$ & $56.0 \pm 12.0$ & $74.4 \pm 6.6$&$87.6 \pm 6.6$\\ \hline
     NN & $88.6 \pm 2.1$& $93.4 \pm 2.7$ & $90.9 \pm 2.0$&$58.3 \pm 15.3$&$43.0 \pm 12.4$ & $49.0 \pm 12.6$&$69.9 \pm 7.2$& $84.6 \pm 3.4$\\ \hline
     RF & $88.9 \pm 1.7$& $98.7 \pm 1.0$& $93.5 \pm 0.9$&$87.0 \pm 10.5$&$41.0 \pm 11.4$ & $54.8 \pm 12.1$&$74.2 \pm 6.4$& $88.7 \pm 1.7$\\ \hline
     NRF\_DW & $89.6 \pm 1.7$& $98.3 \pm 1.0$& $93.7 \pm 1.0$&$84.7 \pm 7.8$ &$45.7 \pm 10.4$ & $58.7 \pm 10.5$&$76.2 \pm 5.7$&$89.1 \pm 1.8$\\ \hline
     NRF\_EL\_DW &$89.8 \pm 1.8$&$98.4 \pm 0.9$& $93.9 \pm 1.1$ &$85.5 \pm 7.4$& $46.6 \pm 11.4$ & $59.6 \pm 11.3$&$76.8 \pm 6.1$&$89.4 \pm 2.0$\\ \hline
     
    
    \end{tabular}

    \caption{LUCAS dataset results.}
    \label{table::real_dataset}
\end{table}
\endgroup

We can see that except of NN model all other classification models performed quite similarly on this dataset with respect to all researched evaluation metrics. Considering overall performance, NRF\_DW and NRF\_EL\_DW reached slightly better results than other models (mainly higher F1-score on the positive class), especially surpassing and enhancing the initialization RF model. This RF model was quite well simulated since the beginning by NF models due to high transition parameters. It turned out that this was a suitable choice, since the results with lower transition parameters ($\beta_1 = 1$ and $\beta_2 = 1$) were not that favourable. Macro-average of F1-score for NRF\_DW was $74.1 \pm 6.2$ and for NRF\_EL\_DW was $74.0 \pm 5.7$. This mild decrease suggests that this classification task suits better for the RF model than NN, which is further supported by unsatisfying results of single NN model in comparison with other models. Therefore it was beneficial to simulate the behaviour of RF model by NF models and eventually surpassing its performance.

We may notice that in the case of positive class, the standard deviations of all results are high. This variance could be caused mainly by the low number of testing instances of positive class (approximately only 16 positive testing instances per one training - testing run). Therefore it may be more beneficial to compare the models also on the level of individual training - testing runs and not only by mean values, since this could lead to biased interpretation. Building on this motivation, we present also PR curves with the corresponding average precision scores (AP) for all training - testing runs (10 iterations in total) and also one aggregation PR curve as defined above. All PR curves alongside the aggregation PR curve are visualized in one graph for each classification model. PR curves are presented in the Figure \ref{figures:pr_curves}.

If we compare individual runs, we can see that in most cases NRF\_DW and NRF\_EL\_DW models reached higher AP values in comparison with other models. Also we can notice the resemblance of the PR curves corresponding to the NF models and the PR curves corresponding to the initialization RF model, which is also the result of high transition parameters. In the summary graph in the Figure \ref{figures:pr_curves} is visible that aggregation PR curves of NRF\_DW, NRF\_EL\_DW and RF are similar in shape as well.

In the conducted experiments of binary classification on LUCAS dataset were only slight differences between individual models, but NF models showed the most favourable performance. On the contrary, the NN model could not achieve a competitive performance with other models, which could stand as a warning and the reason to increase the transition parameters in our NF models, rather preferring the RF model nature.






\chapter{Figures}
\label{figures}

\pagestyle{empty}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_DW_learning_rate_acc_ROUGH.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_identity_learning_rate_acc_ROUGH.png}}
    }
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_learning_rate_acc_ROUGH.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_learning_rate_acc_ROUGH.png}}
    }
        \hspace{0mm}
        \\[-0.8ex]
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NN_learning_rate_acc_rough.png}}

    \caption{The effect of varying learning rate on Vehicle dataset. Blue line illustrates mean value and orange lines symbolizes standard deviation. Dimmed gray lines are single realizations.}
\label{learning_rates}
\end{figure}
\vfill

\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_DW_learning_rate_acc_ROUGH_dia.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_identity_learning_rate_acc_ROUGH_dia.png}}
    \hspace{0mm}
    }
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_learning_rate_acc_ROUGH_dia.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_learning_rate_acc_ROUGH_dia.png}}
    }
        \hspace{0mm}
        \\[-0.8ex]
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NN_learning_rate_acc_rough_dia.png}}
    \caption{The effect of varying learning rate on Diabetes dataset. Blue line illustrates mean value and orange lines symbolizes standard deviation. Dimmed gray lines are single realizations.}
\label{figures:learning_rates_diabetes}
\end{figure}
\vfill

\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_DW_epochs_estimators_acc.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_identity_epochs_estimators_acc.png}}
    }
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_epochs_estimators_acc.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_epochs_estimators_acc.png}}
    }
        \hspace{0mm}
        \\[-0.8ex]
        \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{RF_epochs_estimators_acc.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NN_epochs_estimators_acc.png}}
    }
    \caption{Dependence of accuracy on number of estimators and number of training epochs on Vehicle dataset.}
\label{figures:estimators_epochs_vehicle}
\end{figure}
\vfill

\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_DW_epochs_estimators_acc_dia.png}
    }
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_identity_epochs_estimators_acc_dia.png}}
    }
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_epochs_estimators_acc_dia.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_epochs_estimators_acc_dia.png}}
    }
        \hspace{0mm}
        \\[-0.8ex]
        \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{RF_epochs_estimators_acc_dia.png}}
    \subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NN_epochs_estimators_acc_dia.png}}
    }
    \caption{Dependence of accuracy on number of estimators and number of training epochs on Diabetes dataset.}
\label{figures:estimators_epochs_diabetes}
\end{figure}
\vfill

\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{NRF_DW_epochs_depth_acc_dia.png}}
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{NRF_DW_epochs_depth_loss_dia.png}}
    }
    \hspace{0mm}
    \\[-5.0ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{NRF_epochs_depth_acc_dia.png}}
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{NRF_epochs_depth_loss_dia.png}}
    }
        \hspace{0mm}
        \\[-5.0ex]
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{RF_epochs_depth_acc_dia.png}}
    \caption{Dependence of accuracy and loss function on maximum depth of the decision trees in RF and number of training epochs. This experiment is performed on Diabetes dataset.}
\label{figures:depth_epochs}
\end{figure}
\vfill

\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{NRF_DW_epochs_depth_acc_reg_dia_reg.png}}
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{NRF_DW_epochs_depth_loss_reg_dia_reg.png}}
    }
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{NRF_epochs_depth_acc_reg_dia_reg.png}}
\subfloat{
    \centering
    \includegraphics[width=0.65\textwidth]{NRF_epochs_depth_loss_reg_dia_reg.png}}
    }

    \caption{Same models as in the Figure \ref{figures:depth_epochs} with regularization constant set to $\lambda = 10$.}
\label{figures:depth_epochs_reg}
\end{figure}
\vfill

\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.60\textwidth]{NRF_DW_beta1_beta2_acc.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_identity_beta1_beta2_acc.png}}
    }
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_beta1_beta2_acc.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_beta1_beta2_acc.png}}
    }
        \hspace{0mm}
        \\[-0.8ex]
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{RF_beta1_beta2_acc.png}}
    \caption{Dependence of accuracy on varying transition parameters $\beta_1$ and $\beta_2$ on Vehicle dataset.}
\label{figures:beta_epochs}
\end{figure}
\vfill

\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.60\textwidth]{NRF_DW_beta1_beta2_acc_dia.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_identity_beta1_beta2_acc_dia.png}}
    }
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_beta1_beta2_acc_dia.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{NRF_EL_DW_beta1_beta2_acc_dia.png}}
    }
        \hspace{0mm}
        \\[-0.8ex]
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{RF_beta1_beta2_acc_dia.png}}
    \caption{Dependence of accuracy on varying transition parameters $\beta_1$ and $\beta_2$ on Diabetes dataset.}
\label{figures:beta_epochs_dia}
\end{figure}
\vfill

%\newgeometry{bottom=0.1cm,top=0.1cm}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{epochs_1_samples50.png}}}
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{epochs_1_samples200.png}}}
    \caption{Type 1. Decision boundaries based on varying number of training epochs. The sizes of datasets are 50 and 200 in the top-bottom manner. The accuracy on the testing subset is in the right-bottom corner.}
\label{figures:epochs_toy1}
\end{figure}
%\restoregeometry
\vfill

%\newgeometry{bottom=0.1cm,top=0.1cm}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{epochs_2_samples50.png}}}
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{epochs_2_samples200.png}}}
    \caption{Type 2. Decision boundaries based on varying number of training epochs. The sizes of datasets are 50 and 200 in the top-bottom manner. The accuracy on the testing subset is in the right-bottom corner.}
\label{figures:epochs_toy2}
\end{figure}
%\restoregeometry
\vfill

%\newgeometry{bottom=0.1cm,top=0.1cm}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{epochs_3_samples50.png}}}
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{epochs_3_samples200.png}}}
    \caption{Type 3. Decision boundaries based on varying number of training epochs. The sizes of datasets are 50 and 200 in the top-bottom manner. The accuracy on the testing subset is in the left-bottom corner.}
\label{figures:epochs_toy3}
\end{figure}
%\restoregeometry
\vfill


%\newgeometry{bottom=0.1cm,top=0.1cm}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{betas1_samples50.png}}}
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{betas1_samples200.png}}}
    \caption{Type 1. Decision boundaries based on varying transition parameters, where  $\beta = \beta_1 = \beta_2$. The sizes of datasets are 50 and 200 in the top-bottom manner. The accuracy on the testing subset is in the right-bottom corner.}
\label{figures:betas_toy1}
\end{figure}
%\restoregeometry
\vfill

%\newgeometry{bottom=0.1cm,top=0.1cm}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{betas2_samples50.png}}}
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{betas2_samples200.png}}}
    \caption{Type 2. Decision boundaries based on varying transition parameters, where  $\beta = \beta_1 = \beta_2$. The sizes of datasets are 50 and 200 in the top-bottom manner. The accuracy on the testing subset is in the right-bottom corner.}
\label{figures:betas_toy2}
\end{figure}
%\restoregeometry

%\newgeometry{bottom=0.1cm,top=0.1cm}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{betas3_samples50.png}}}
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{betas3_samples200.png}}}
    \caption{Type 3. Decision boundaries based on varying transition parameters, where  $\beta = \beta_1 = \beta_2$. The sizes of datasets are 50 and 200 in the top-bottom manner. The accuracy on the testing subset is in the left-bottom corner.}
\label{figures:betas_toy3}
\end{figure}
%\restoregeometry


%\newgeometry{bottom=0.1cm,top=0.1cm}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{noise31_samples100.png}}}
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=\textwidth]{noise31_samples100_beta50.png}}}
    \caption{Decision boundaries based on occurrence of noise in data. The effect of noise is illustrated on dataset with 100 instances. In the upper figure is $\beta_1 = \beta_2 = 1$ among all NRF models. In the bottom figure is $\beta_1 = \beta_2 = 50$. The accuracy on the testing subset is in the right-bottom corner.}
\label{figures:noise_toy}
\end{figure}
%\restoregeometry

%\newgeometry{bottom=0.1cm,top=0.1cm}
\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.7\textwidth]{input_data_reg.png}}
    \subfloat{
    \centering
    \includegraphics[width=0.5\textwidth]{toy_reg_100samples.png}}}
    \caption{Accuracy depending on the regularization constant $\lambda$. In the left figure are depicted input data (100 instances) as two circles distorted by noise. The results are presented for RF, NRF\_DW and NRF\_EL\_DW in the form "mean $\pm$ std" after 2 subsequent 5-fold cross validations. The transition parameters $\beta_1$ and $\beta_2$ are set to 10.}
\label{figures:toy_reg}
\end{figure}
%\restoregeometry

\vfill
\begin{figure}[h!]
\centering
\makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{nrf_dw_pr.png}
    }
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{nrf_el_dw_pr.png}}
    }
    \hspace{0mm}
    \\[-1.5ex]
    \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{lr_pr.png}}
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{nn_pr.png}}
    }
        \hspace{0mm}
        \\[-0.8ex]
        \makebox[\textwidth][c]{
\subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{rf_pr.png}}
    \subfloat{
    \centering
    \includegraphics[width=0.6\textwidth]{summary_pr.png}}
    }
    \caption{PR curves on LUCAS dataset. Each graph contains PR curves of individual training - testing runs with AP values and one highlighted aggregation PR curve with mean AP value and standard deviation of all iterations. In the summary are visualized aggregation PR curves together.}
\label{figures:pr_curves}
\end{figure}
\vfill

\begin{thebibliography}{}
\bibitem{NRF} Neural Random Forest. Gerard Biau, Erwan Scornet, Johannes Welbl

\bibitem{evaluation1}G. Canbek, S. Sagiroglu, T. T. Temizel and N. Baykal, "Binary classification performance measures/metrics: A comprehensive visualized roadmap to gain new insights," 2017 International Conference on Computer Science and Engineering (UBMK), Antalya, 2017, pp. 821-826

\bibitem{evaluation2}Koyejo, O. \& Natarajan, Nagarajan \& Ravikumar, P. \& Dhillon, I.S.. (2014). Consistent binary classification with generalized performance metrics. Advances in Neural Information Processing Systems. 3. 2744-2752. 

\bibitem{multieval1}Hossin, Mohammad \& M.N, Sulaiman. (2015). A Review on Evaluation Metrics for Data Classification Evaluations. International Journal of Data Mining \& Knowledge Management Process. 5. 01-11. 10.5121/ijdkp.2015.5201. 

\bibitem{multieval2} Branco, Paula \& Torgo, Luís & Ribeiro, Rita. (2017). Relevance-Based Evaluation Metrics for Multi-class Imbalanced Domains. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). 698-710. 10.1007/978-3-319-57454-7\_54. 

\bibitem{imbalance}Chawla, Nitesh. (2005). Data Mining for Imbalanced Datasets: An Overview. 10.1007/0-387-25465-X\_40. 

\bibitem{bakalarka}bakalarka

\bibitem{imbalance_data}HE, H. a E. A. GARCIA. Learning from Imbalanced Data. IEEE Transactions on Knowledge and Data Engineering. 2009, (9), 1264-1284.

\bibitem{imbalance3} Akosa, J.S. (2017). Predictive Accuracy : A Misleading Performance Measure for Highly Imbalanced Data.

\bibitem{random_forest_intrusion}Random Forest Modeling for Network Intrusion Detection System
Citation DataProcedia Computer Science, ISSN: 1877-0509, Vol: 89, Page: 213-217 , 2016

\bibitem{computer_vision} Random Forest and computer vision
http://pages.iai.uni-bonn.de/frintrop\_simone//BVW13/BVW-gall.pdf

\bibitem{bioinf}R. D´ıaz-Uriarte and S. Alvarez de Andr´es. Gene selection and classification of
microarray data using random forest. BMC Bioinformatics, 7:1–13, 2006.

\bibitem{text_rec}Bernard, Simon \& Heutte, Laurent \& Adam, Sébastien. (2007). Using Random Forests for Handwritten Digit Recognition. Proceedings of the International Conference on Document Analysis and Recognition, ICDAR. 2. 1043-1047. 10.1109/ICDAR.2007.4377074. 

\bibitem{rf_overfit}Ali, Jehad \& Khan, Rehanullah \& Ahmad, Nasir \& Maqsood, Imran. (2012). Random Forests and Decision Trees. International Journal of Computer Science Issues(IJCSI). 9. 

\bibitem{rf_imp}PARR, Terence, Kerem TURGUTLU, Christopher CSISZAR a Jeremy HOWARD. Beware Default Random Forest Importances. Explained.ai [online]. 2018 [cit. 2018-05-15]. Dostupné z: http://explained.ai/rf-importance/index.html

\bibitem{rf_main}A. Criminisi, J. Shotton, and E. Konukoglu, Decision Forests: A Unified Framework for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning. Foundations and Trends in Computer Graphics and Computer Vision, Now Publishers Inc., 2012, 81-227.
\bibitem{c45}C4.5 algorithm and Multivariate Decision Trees
Thales Sehn Korting

\bibitem{cart}
Breiman, L., Friedman, J.H., Olshen, R., and Stone, C.J., 1984. Classification and Regression
Tree Wadsworth \& Brooks/Cole Advanced Books \& Software, Pacific Californi

\bibitem{id3}Xiaohu, Wang & Lele, Wang \& Nianfeng, Li. (2012). An Application of Decision Tree Based on ID3. Physics Procedia. 25. 1017-1021. 10.1016/j.phpro.2012.03.193. 

\bibitem{pruning}Zhou, Xinlei \& Yan, Dasen. (2019). Model tree pruning. International Journal of Machine Learning and Cybernetics. 10.1007/s13042-019-00930-9. 

\bibitem{pruning2}Esposito, Floriana \& Malerba, Donato \& Semeraro, Giovanni \& Kay, John. (1997). A Comparative Analysis of Methods for Pruning Decision Trees. Pattern Analysis and Machine Intelligence, IEEE Transactions on. 19. 476 - 491. 10.1109/34.589207. 

\bibitem{ensemble}Ensemble Methods in Machine Learning
Thomas G Dietterich
Oregon State University Corvallis Oregon USA
tgdcsorstedu WWW home page httpwwwcsorstedutgd

\bibitem{variance_rf} http://www.math.mcgill.ca/yyang/resources/doc/randomforest.pdf

\bibitem{adaboost} Coadou, Yann. (2013). Boosted Decision Trees and Applications. EPJ Web of Conferences. 55. 02004-. 10.1051/epjconf/20135502004. 

\bibitem{adaboost2}

\bibitem{culloch}W.S. McCulloch and W. Pitts. A logical calculus of the ideas immanent
in nervous activity. Bulletin of Mathematical Biology, 5(4):115–133, 1943.

\bibitem{perceptron}M. Minsky and S. Papert. Perceptrons. MIT Press, Cambridge, Mass,
1969.

\bibitem{werbos} P. J. Werbos. Beyond Regression: New Tools for Prediction and Analysis
in the Behavioral Sciences. PhD thesis, Harvard University, 1974.

\bibitem{backprop-cit}D. Rumelhart, G. Hinton, and R. Williams. Learning representations by
back-propagating errors. Nature, 323:533–536, October 1986

\bibitem{perceptron-nonsep}http://www.dkriesel.com/\_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf

\bibitem{nn-application}Hordri, Nur \& Yuhaniz, Siti \& Shamsuddin, Siti Mariyam. (2016). Deep Learning and Its Applications: A Review. 

\bibitem{feedforward} http://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf

\bibitem{cybenko}Cybenko, G. (1989) "Approximations by superpositions of sigmoidal functions", Mathematics of Control, Signals, and Systems, 2(4), 303–314. doi:10.1007/BF02551274

\bibitem{resnet}https://arxiv.org/pdf/1909.04653.pdf

\bibitem{actfunc}https://arxiv.org/pdf/1811.03378.pdf

\bibitem{tanh_prefer}B. Karlik and A. Vehbi, “Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks,”
International Journal of Artificial Intelligence and Expert Systems (IJAE), vol. 1, no. 4, pp. 111–122, 2011. [Online]. Available:
http://www.cscjournals.org/library/manuscriptinfo.php

\bibitem{relu}https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf

\bibitem{leaky}https://arxiv.org/pdf/1804.02763.pdf

\bibitem{grad}https://web.stanford.edu/~boyd/cvxbook/bv\_cvxbook.pdf

\bibitem{backprop}http://www.cs.cornell.edu/courses/cs5740/2016sp/resources/backprop.pdf citováno 30.3.2020

\bibitem{numpy}https://numpy.org/

\bibitem{adam_weightdecay}https://openreview.net/pdf?id=rk6qdGgCZ

\bibitem{scikit} Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.

\bibitem{keras} Keras. Chollet, Francois and others. 2015. \url{https://keras.io}

\bibitem{saturation}Bosman, Anna \& Engelbrecht, Andries. (2015). Measuring Saturation in Neural Networks. 10.1109/SSCI.2015.202. 

\bibitem{lucas} www.lucascz.cz (cited 14.5.2020)

\bibitem{roc} https://doi.org/10.1016/j.patrec.2005.10.010

\bibitem{roc_analysis} ROC analysis of classifiers in machine learning: A survey
May 2013Intelligent Data Analysis 17(3):531-558
DOI: 10.3233/IDA-130592

\bibitem{pr_roc_diff} https://www.biostat.wisc.edu/~page/rocpr.pdf

\bibitem{adam}https://arxiv.org/pdf/1609.04747.pdf

\bibitem{adam2}Diederik P. Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, pages 1–13, 2015.

\bibitem{adam3}https://arxiv.org/pdf/1412.6980.pdf

\bibitem{learn_slow}Golik, Pavel & Doetsch, Patrick & Ney, Hermann. (2013). Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH. 1756-1760. 

\bibitem{note_backprop}https://www.ics.uci.edu/~pjsadows/notes.pdf

\bibitem{loss_func}https://arxiv.org/pdf/1702.05659.pdf

\bibitem{regularization}https://pdfs.semanticscholar.org/0e9f/88e8f47b7a1b73d9090f0da3f0e5c2bec9aa.pdf

\bibitem{adam_criticism}https://openreview.net/pdf?id=rk6qdGgCZ

\bibitem{dropout} https://papers.nips.cc/paper/4878-understanding-dropout.pdf

\end{thebibliography}







\end{document}
