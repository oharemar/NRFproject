\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{makecell}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\title{MASTER'S THESIS}
\author{Bc. Martin Oharek }
\date{December 2019}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}

\chapter{Classification}
\section{State-of-the-art classification models}
\subsection{Logistic regression}
\subsection{Support vector machines (SVM)}

\section{Evaluation}

This section is focused on the summary and description of the evaluation metrics used in the experimental part, alongside the pros and cons of each metric and suitable application. This section includes only metrics that are applied in the testing phase. The correct choice of evaluation is the main and crucial task in the case of searching for optimal classification model. In practice, there exist numerous evaluation methods applied frequently to many classification tasks, each fits better in different cases. Before we choose set of evaluation metrics, we must be aware of the processed data and classification model to set our evaluation correctly. Also the purpose and future application scope of our model is significant. If this foremost analysis of the data and model is neglected or done wrong, we could end up choosing bad-shaped evaluation techniques and therefore misinterpret the performance of our model.

In the literature could be found significant amount of studies addressing the choice of evaluation metrics in the case of either binary classification \cite{evaluation1,evaluation2} or multi-class classification \cite{multieval1,multieval2}.

The classification models in this thesis are tested on multiple datasets, both binary and multi-class classification oriented. Consequently, there is not any particular property that could discriminate correct (the most correct) setting of evaluation methods in general, unless we study each dataset separately, which is not the purpose of this thesis at all. We only desire to compare classification performance quality of neural random forest mainly to the performance of regular random forests and provide the evidence of the superiority of neural random forests. Therefore we select subset of worldwide-accepted, well-functional evaluation techniques, summarize them and describe them separately. Combination of these metrics provides sufficient evaluation to build a conclusion.

Firstly, we define important notation that is referenced in the upcoming descriptions. This applies generally to the multi-class classification with arbitrary number of classes (labels). Set of all classes is denoted as $\mathbb{C}$.

\begin{definition}
\textit{Positive} class $c \in \mathbb{C}$ is such label, which is in the current scope of interest. Other classes $d \neq c$, $d \in \mathbb{C}$, are called \textit{negative} classes.
\label{definition_pos_neg}
\end{definition}

To clarify \ref{definition_pos_neg}, if we test the  classification performance of our model with respect to class $c \in \mathbb{C}$, then $c$ is called positive class and other labels $d \neq c$, $d \in \mathbb{C}$, are called negative classes. It is important to realize, that there could be many positive classes. It depends chiefly on the current scope of interest. For instance, computing accuracy of class $c_1 \in \mathbb{C}$ means that $c_1$ is (currently) positive class and other classes are negative. In the following, current positive class will be put into parentheses (such as $\text{TP}(c), \text{FN}(c)$ etc.) Also the instances belonging to the positive, resp. negative class could be referenced as positive, resp. negative instances.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    
       $\text{TP}(c)$ &\makecell{count of positive instances of class $c \in \mathbb{C}$ \\ correctly predicted } \\ \hline
         $\text{FP}(c)$& \makecell{count of negative instances \\ predicted incorrectly as positive class $c$ }  \\ \hline
         $\text{TN}(c)$ &\makecell{count of negative instances (with respect to the positive class $c$) \\ correctly predicted  }\\ \hline
         $\text{FN}(c)$ & \makecell{count of positive instances of class c incorrectly predicted as negative \\ class (with respect to the positive class $c$) }\\ \hline
    \end{tabular}
    \caption{Definitions of TP,FP,TN and FN.}
    \label{tab:definitions}
\end{table}
 


\subsubsection{Accuracy} 


Probably the most applied evaluation metric in the machine learning society, which provides quick and simple evaluation of the classification performance. Outcome of this metric is the ratio between number of correctly classified instances and number of all instances. Accuracy metric will be denoted as $Acc$.

\begin{equation}
    Acc = \frac{\text{number of correctly classified instances}}{\text{number of all instances}}
\end{equation}

Altough gaining this simple value for one classification model is very easy and quick, this metric possesses some significant drawbacks \cite{multieval1} and therefore relying only on this metric during final evaluation is dangerous. It does not take into account membership of instances to either positive or negative classes. Serious problem arises especially when processing imbalanced data \cite{imbalance}, which is the case of many real-world applications (fraud and malware detection \cite{bakalarka} etc.).

For instance, imagine data distributed into two classes, class 0 with 98\% occurrence and class 1 with only 2\% occurrence. This dataset is clearly significantly imbalanced. Moreover, we put higher preference on correctly classifying minority class 1 (e.g. consider class 1 as malware type communication and class 0 as arbitrary safe network communication, it could be basically anything rare vs. normal). Also define naive classifier to constantly classifying any instance as class 0. In this case, accuracy of this model is $Acc = 98\%$, which is very high. Based only on this value, the classification performance of our naive model is perfect. But claiming that this model is good is wrong, since it completely fails in classifying minority class, which has the highest priority. So even that accuracy is very high, the model itself is absolutely useless.

\subsubsection{Precision}

Another evaluation metric is called precision and is class-sensitive. In the contrast to the accuracy, it is computed for any particular class $c \in \mathbb{C}$ as
\begin{equation}
    Precision(c) = \frac{\text{TP}(c)}{\text{TP}(c)+\text{FP}(c)}
\end{equation}

It expresses the ratio between correctly predicted positive instances and all instances predicted as positive. In other words, it shows how precise is our classification model in predicting positive class.

This metric is also dependent on the rate of imbalance among current data. Let's suppose adding negative instances to the dataset. Then 

$$\text{FP}^{\text{less}}(c) \leq \text{FP}^{\text{more}}(c) \implies Precision^{\text{less}}(c) \geq Precision^{\text{more}}(c)$$

So even if our classifier predicts all positive instances correctly, the precision tends to decrease with increasing number of negative instances.

\subsubsection{Recall}

This metric is computed for class $c \in \mathbb{C}$ as

\begin{equation}
    Recall(c) = \frac{\text{TP}(c)}{\text{TP}(c) + \text{FN}(c)}
\end{equation}

Recall expresses ratio between number of correctly classified positive instances and number of all positive instances. Clearly, this metric does not depend on the rate of imbalance in our data and therefore is suitable choice for evaluation on imbalance datasets.

In many applications of classification on imbalance datasets is put a strong requirement on model to predict minority classes as precisely as possible (capture all positive instances, but also not to cause a lot of mistakes on negative instances), because misdetections could inflict great damage (e.g. malware detection \cite{bakalarka}). In some studies it turns out that combination of precision and recall is appropriate choice to satisfy these requirements and provides reliable evaluation \cite{imbalance_data}.

\subsubsection{F-measure}

F-measure metric combines precision and recall into single value as

\begin{equation}
    F_\beta(c) = \frac{(1+\beta^2)\cdot Precision(c) \cdot Recall(c)}{\beta^2 \cdot Precision(c) + Recall(c)}\hspace{0.5cm} ,
\end{equation}
where $c \in \mathbb{C}$ and $\beta$ is a coefficient which serves to adjust relative importance between precision and recall (often $\beta = 1$). For $\beta = 1$ it expresses harmonic mean of precision and recall. As $\beta \rightarrow 0$ the formula considers only precision and as $\beta \rightarrow \infty$ the formula considers only recall. Generally, $\beta < 1$ favors precision and $\beta > 1$ favors recall. It depends on a specific application to choose $\beta$ appropriately.

\subsubsection{G-measure}

G-measure is computed as

\begin{equation}
    G(c) = \sqrt{\frac{\text{TP}(c)}{\text{TP}(c) + \text{FN}(c)} \cdot \frac{\text{TN}(c)}{\text{TN}(c) + \text{FP}(c)}} \hspace{0.5cm},
    \label{g-meas}
\end{equation}
where $c \in \mathbb{C}$. The left factor in multiplication under the square root in \eqref{g-meas} is called \textit{Sensitivity} or \textit{true positive rate} (same as recall) and the right factor is called \textit{Specificity} or \textit{true negative rate}.

Optimization of this metric secures good balance between classification performance on both minority and majority classes. In the case of imbalance, even if the classification performance on negative instances is perfect, G-measure would end significantly low if classification of positive instances is poor \cite{imbalance3}. This is valuable property which makes this metric well-applicable to evaluation of the classification model on imbalanced data.

\textbf{Note:} Since we compare different classification models and track their overall performance on various datasets in the experimental part, the main purpose is not to compare metric values for each individual class, but rather to compare summaries of these evaluation outcomes. For this reason we adopt \textbf{macro/weighted average} to represent the individual metrics for different classes as single value.

For arbitrary evaluation metric $f(\cdot)$ is macro/weighted average defined as

\begin{equation}
    MacroAvg(f) = \frac{\sum_{l=1}^{M}f(c_l)}{M}
\end{equation}
\begin{equation}
    WeightedAvg(f) = \frac{\sum_{l=1}^{M}n_l \cdot f(c_l)}{\sum_{l=1}^{M}n_l} \hspace{0.5cm},
\end{equation}
where $c_l, l \in \{1,2,...,M\}$ is $l$-th class, $n_l$ is count of (testing) instances belonging to $c_l$ and $M$ is total number of classes.

It is also worth to mention \textbf{micro average} of precision and recall, which is defined as

\begin{equation}
    MicroAvg(Precision) = \frac{\sum_{l=1}^{M}\text{TP}(c_l)}{\sum_{l=1}^{M}(\text{TP}(c_l) + \text{FP}(c_l))}
\end{equation}

\begin{equation}
    MicroAvg(Recall) = \frac{\sum_{l=1}^{M}\text{TP}(c_l)}{\sum_{l=1}^{M}(\text{TP}(c_l) + \text{FN}(c_l))}
\end{equation}

$MicroAvg(Precision)$ computes basically fraction of the sum of all correctly predicted instances and sum of all correctly and incorrectly predicted instances, which is the sum of all instances. Therefore it expresses same value as accuracy of the model. Furthermore, since $\sum_{l=1}^{M}\text{FN}(c_l) = \sum_{l=1}^{M}\text{FP}(c_l)$, both values are equal. They will be used later in the experimental part of the thesis.


\subsubsection{Receiver operator charachteristics (ROC) curves}
ROC curves are frequently drawn especially in the cases of binary classifiers. Also they could be generalized to the multi-class classification by applying OneVsAll or OneVsOne approach and different averaging methods based on preference (e.g. $MacroAvg$,$MicroAvg$).

ZDE DOPSAT ZBYTEK + Precision - Recall Curves


\chapter{Random forest}

Random forest algorithm is a popular machine learning method used primarily on the classification and regression tasks. Its functionality is based on collecting predictions from several independent classifiers (ensemble of decision trees) and merging their outcomes into final prediction. It belongs to the group of supervised learning algorithms. In this thesis is usage of random forests restricted only to the classification tasks, so the theoretical part of this section will be limited only to the random forests used for classification. Main goal is to find a decision function $f: \mathbb{X} \rightarrow \mathbb{R}$, where $X \subset \mathbb{R}^d$ is the input space in the $d$-dimensional real-valued space, which outputs the response (class, label).

Random Forest nowadays is a widespread method exploited frequently in various domains, such as computer vision \cite{computer_vision}, text classification \cite{text_rec}, bioinformatics \cite{bioinf}, network threat detection \cite{random_forest_intrusion} etc. The individual classifiers - decision trees - could be also used as a clustering algorithm [ref Egypt článek].
Popularity of random forests is gained due to excellent generalization and application to wide-ranged spectrum of classification tasks with registering competitive classification quality in comparison with other state-of-the-art methods, such as logistic regression, support vector machines, neural networks etc. Also the method itself is in its basics quite simple and therefore is fast, easy implementable and could be analyzed and interpreted with instant gain of variable importances \cite{rf_imp}.

Unlike the neural networks, it has far less parameters to tune and is better-applicable to the small-sample sized (high-dimensional) datasets. With the usage of randomization, bagging, bootstrapping and other techniques based on randomness, random forests overcome easily problems of overfitting \cite{rf_overfit}.

\section{Decision tree}

As was mentioned previously, random forest consists of independent classifiers called decision trees (Figure \ref{fig:dec_tree}).

\begin{figure}[ht]
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{dec_tree.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{dec_tree_graph.png}
\end{minipage}
\caption{In the left part of the figure is illustrated decision tree with depth = 2. The first circle in red color is called root node. Two beige circles in the intermediate layer are inner nodes and green rectangles are leaf nodes, where probabilities of individual classes are stored . Each input vector $\boldsymbol{x} = (x_1,x_2) \in \mathbb{R}^2$ starts from the root node and traverses down the tree by applying subsequent split criterions in nodes until it reaches the leaf node. In the right figure is visualized segregation of input space by the hyperplanes (defined in root node and inner nodes of the decision tree) parallel to the axes (black lines).}
\label{fig:dec_tree}
\end{figure}

In this work we focus only on binary trees, which means, that every node has always 2 child nodes (following nodes in the hierarchical structure of the tree). The process of applying decision tree to obtain final prediction is basically described in Figure \ref{fig:dec_tree}. Beginning from the root node, the input vector $\boldsymbol{x} \in \mathbb{R}^d$ from $d$-dimensional input space is iteratively tested by the split functions associated with every single node in hierarchical order and based on the result is further sent either to the left or right child (the split functions usually define hyperplanes in the input space). This process is repeated until the vector reaches final node (leaf node). In the leaf node are stored classification predictions, which are eventually extracted.

The work with decision tree could be divided into two major parts:

\begin{enumerate}
    \item \textbf{Training stage}: In this stage the entire decision tree is built from the scratch on the basis of predefined criterions and properties. To perform this phase, the \textbf{training dataset} has to be adopted and used. It is usually selected as a subset of the complete data that are available. Magnitude of the training dataset depends on the complexity and application of our model (e.g. if the best model is selected and compared with other models, it should be sufficient to use less data or if the model is prepared for the production, it is desirable to use as much data as possible) and on the complexity of the problem. There does not exist any general consensus that dictates how large should training dataset be. It often depends on the personal preferences and experience and of course on the following evaluation results.
    
    \item \textbf{Testing/application stage}: After training is the decision tree directly applicable and therefore could be either tested with various evaluation methods or applied in practice. The usage of decision tree itself is very straightforward and was already described. We will focus on description of merging the individual results from the ensemble of decision trees and obtaining final predictions.
\end{enumerate}
Now will be provided mathematical details of the both stages.

\subsection{Training stage}

Consider training dataset $\mathbb{X} = \{\boldsymbol{x}_1,...\boldsymbol{x}_N\}$, where $\boldsymbol{x}_k \in \mathbb{R}^d$ and $k \in \{1,...,N\}$, where $N$ is total number of vectors in the training dataset and $d$ is the feature space dimension. The goal of the training stage is to establish proper decision tree structure and find suitable split functions in every node that provides the best class segregation. Splitting continues until some of the predefined stopping criterions are met.

Training begins always from the root node. Starting with the training set $\mathbb{M}$ chosen for the current decision tree (could be entire training dataset or its subset), it is interatively splits in each node to the two disjunct subsets $\mathbb{M}_1$ and $\mathbb{M}_2$, where $\mathbb{M}_1,\mathbb{M}_2 \subset \mathbb{M}, \mathbb{M}_1 \cup \mathbb{M}_2 = \mathbb{M}$. $\mathbb{M}_1$ corresponds to the subset of $\mathbb{M}$ sent to the left child and $\mathbb{M}_2$ to the right child. Splitting in each node is done via split functions. The shape of the split functions associated to the nodes is defined in advance. Generally it could be expressed as the binary function

\begin{equation}
    s(\boldsymbol{x},\boldsymbol{\theta}) = \begin{cases}
    0,  & \boldsymbol{x} \text{ is sent to the left child}\\
      1, & \boldsymbol{x} \text{ is sent to the right child}
    \end{cases}
\end{equation}

The parameters of the split functions are the input vector $\boldsymbol{x}$ and parameter vector $\boldsymbol{\theta}$, which defines geometric separation of the data (e.g. hyperplanes). This comprises thresholds, group of data features which will be considered in the split function and other necessary values needed for complete determination of the split function.

In the practice, widely used choice of the split function is \cite{rf_main}

\begin{equation}
    \psi(\boldsymbol{x}, i, \tau) = \begin{cases}
    0,  & x_i < \tau\\
      1, & \text{otherwise}
    \end{cases}
    \label{split_func}
\end{equation}

Parameter vector in this case is $\boldsymbol{\theta} = (i,\tau)$, where $i \in \{1,...,d\}$ represents one feature from the $d$-dimensional feature space and $\tau \in \mathbb{R}$ is a threshold. Together, equation $x_i = \tau$ embodies hyperplane parallel with one axis and split function divides vectors into two groups based on their relative position with respect to this hyperplane. With regard to the fact, that our proposed transformation of the decision tree to the corresponding neural network is build on usage of this special split function \eqref{split_func}, we will not focus on the other types of split functions in this thesis.

In each iteration, algorithm seeks for the best split, ie. for the parameters of the split function that provides this split. It is achieved by optimizing pre-specified split criterion, which expresses measure of (non)impurity in the corresponding node. By impurity is meant that if node contains instances belonging only to one class, the impurity is 0, whereas it is increased if instances of multiple classes are present in the node.

Among widely applied impurity functions belong \textbf{Entropy}, \textbf{Gini index}, which are adopted in the heuristic algorithms as C4.5 \cite{c45}, CART \cite{cart} and ID3 \cite{id3}. Another possibility is a \textbf{Classification error}. They are defined as (in the same order as were mentioned)

\begin{align}
    H_E(\mathbb{M}) &= -\sum_{c \in \mathbb{C}}^{}p(c)\log p(c) \\
    H_G(\mathbb{M}) &= \sum_{c \in \mathbb{C}}^{}p(c)(1-p(c))\\
    H_{CE}(\mathbb{M}) &= 1 - \max_{c \in \mathbb{C}}[p(c)] \hspace{0.5cm},
\end{align}
where the sum iterates over all classes $c \in \mathbb{C}$, $\mathbb{C}$ is a set of all classes, and
\begin{equation}
    p(c) = \frac{\sum_{\boldsymbol{x} \in \mathbb{M}}^{} 1|_{\boldsymbol{x} \in c}}{|\mathbb{M}|} \hspace{0.5cm},
\end{equation}
which is probability of instance in $\mathbb{M}$ belonging to the class $c$. $|\mathbb{M}|$ represents number of instances in set $\mathbb{M}$ and

\begin{equation}
    1|_{\boldsymbol{x} \in c} = \begin{cases}
    1, & \boldsymbol{x} \in c \\
    0, & \text{otherwise}
    \end{cases}
\end{equation}

These impurity functions are in general exploited in the split criterions, which are then optimized in order to obtain the best possible split. By optimizing these criterions should decrease the impurity function value after each split.
There are plenty of possibilities to choose split criterion that suits our problem, but as the most popular choice could be considered \textbf{Information gain} criterion, which is defined as \cite{rf_main}

\begin{equation}
    I(i,\tau,\mathbb{M}) = H(\mathbb{M}) - \sum_{i \in \{1,2\}}^{} \frac{|\mathbb{M}_j(i,\tau)|}{|\mathbb{M}|}H(\mathbb{M}_j(i,\tau))
    \label{inf_gain}
\end{equation}

$H(\mathbb{M})$ is arbitrary impurity function (e.g. entropy, gini index etc.) of set $\mathbb{M}$. $\mathbb{M}_j(i,\tau), j \in \{1,2\}$ represent the subsets of $\mathbb{M}$ proceeding to the left or right child. In parentheses is emphasized, that these subsets strictly depend on parameters $i,\tau$ of the split function. By maximizing information gain in each node are obtained final parameters $i^*, \tau^*$, that are stored and used in the testing stage. Thus parameters of the split function of the $j$-th node are acquired as

\begin{equation}
    \boldsymbol{\theta}_j^* = (i_j^*,\tau_j^*) = \text{arg}\max_{(i,\tau)}I(i,\tau,\mathbb{M}_j)\hspace{0.5cm},
\end{equation}
where $\mathbb{M}_j \subset \mathbb{M}$ is the subset of $\mathbb{M}$ in the $j$-th node.

Basic simulation of finding the best split is visualized in the Figure \ref{fig:split_example}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{hor_ver_split.png}
\caption{In the Figure are illustrated two cases of different splits.}
\label{fig:split_example}
\end{figure}
Values of information gain (IG) for different impurity functions are (Figure \ref{fig:split_example}):
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
       &\textbf{Horizontal split} & \textbf{Vertical split} \\ \hline
         IG (Entropy)& 0.34 & \textbf{0.69}\\ \hline
         IG (Gini)& 0.135& \textbf{0.25}\\ \hline
         IG (Classification Error)&0.20 & \textbf{0.25}\\ \hline
    \end{tabular}
    \caption{Values of IG for splits in Figure \ref{fig:split_example}.}
    \label{tab:ig_vals}
\end{table}
With change of the horizontal split to the vertical split occured increase in values of all studied criterions (see Table \ref{tab:ig_vals}). This suggests that vertical split divides data better than the horizontal split and should be preferred.

Apart of the split function and the split criterion, another necessary feature must be added to the decision tree model in advance. We need to define stopping criterion, which interrupt the growth of the tree in the current node when the specified condition is met.

One natural choice of the stopping criterion is to stop splitting when the corresponding node contains instances belonging to the same class. Therefore there does not make any sense to continue in splitting and growth could be ended. Also it is not possible to continue if there ends only one instance in the node. In practice are frequently used many different criterions, which helps to avoid overgrown trees prone to overfitting. If one of them applies, the growth in the current node is interrupted. As example could be mentioned \cite{pruning}:
\begin{itemize}
    \item \textit{Low information gain}: Lower threshold for the information gain is set. When a subsequent split produces insufficient information gain below specified threshold, the growth is terminated and node becomes a leaf node.
    \item \textit{Maximum depth}: When the number of splits in the current tree branch reaches the maximum depth, the current node becomes a leaf node.
    \item \textit{Maximum instances in the leaf}: This is a threshold for the maximum number of instances in the leaf nodes. When the number of instances in the current node is equal or below this threshold, the node becomes a leaf node.
\end{itemize}

All of these stopping criterions defined in advance belongs to the \textit{prepruning} methods \cite{pruning}. The fact, that these methods are defined in advance embodies certain drawback. For instance, we do not know in advance, if the predefined maximum depth would produce sufficient requirements. Maybe it is too small and the decision tree will tend to underfitting or is too large and the decision tree will tend to overfitting.

For this reason, there could be adopted the \textit{postpruning methods} for the large-depth trees which encounter this issue and solves efficiently the problem of overfitting. There were not applied in our experiments, so we will provide only short summary of these methods \cite{pruning2}.
\begin{itemize}
    \item \textit{Reduced Error Pruning (REP)}: The model is pruned by usage of the independent validation (pruning) dataset different from the training dataset. The subtree starting from the chosen inner node is replaced by the leaf node and the classification error on the validation dataset is measured. If the error is lower than the error in case of the previous non-pruned tree, the tree is pruned. This procedure runs in the bottom-up manner and is ended when no improvement in the classification error is observed.
    \item \textit{Pessimistic Error Pruning (PEP)}:  Unlike the REP method, PEP uses entire training dataset for the pruning procedure. It is based on the continuity correction of the error rate, since the error rate on the training dataset is significantly biased. Details could be found in \cite{pruning2} alongside the descriptions of other applied methods.
\end{itemize}

When the growth of the tree is successfully finished, the class distributions of the instances that ended in each leaf node form a probability distribution of classes, which is stored in the leaf node. For probability $p^j(c)$ of class $c \in \mathbb{C}$ in the leaf node $j \in \{1,...,L\}$, where $L$ is a total number of leaf nodes applies
\begin{equation}
    p^j(c) = \frac{n^j_c}{N^j} \hspace{0.5cm},
\end{equation}
where $n^j_c$ is a total number of instances of class $c$ in the leaf node $j$ and $N^j$ is a total number of instances in the entire leaf node $j$.

\subsection{Testing/application stage}

After the training stage, the decision tree is obtained and could be applied in practice. Starting from the root node, the testing instance $x \in \mathbb{X} \subset \mathbb{R}^d$ is traversed down the tree by iterative application of the split functions associated with individual nodes. When the instance reaches the leaf node, the probability distribution of classes in the leaf node is extracted and the values are observed. The class which has the maximum value is outputted as the final prediction. Mathematically, the output of the single decision tree is

\begin{equation}
    c^* = \text{arg}\max_{c \in \mathbb{C}}p^j(c) \hspace{0.5cm},
\end{equation}
where $j$ is the leaf node where $x$ ended.

\section{Ensemble of decision trees}

Ensemble of decision trees forms a strong classifier with higher predictive ability than individual decision trees. Idea behind this is to combine several weak classifiers to create one strong classifier. Ensembles generally boost the performance in the comparison with the single classifier (decision tree). For instance, there is a chance of single classifiers to get stuck in the local optima when optimizing different criterions (e.g. minimizing error rate). With more and more data, it could be for many classifiers computationally unreachable to find the best optimal value. In such cases, the employment of ensemble methods is valuable, because training of independent classifiers with different subsets of input data and merging their output could approximate real prediction function far better \cite{ensemble}.

Ensemble methods also significantly benefit from the averaging of results from individual classifiers, especially in the case of insufficient number of training data points. In such case, the learning algorithm could find many applicable parameters (e.g. hyperplanes of the decision tree) that will result in the same accuracy on the training dataset. Combining those models could suppress the possibility of picking a wrong classifier \cite{ensemble}.

Among popular techniques to create an ensemble of decision trees belong:
\begin{itemize}
    \item \textbf{Bagging}: This technique divide the training datasets into several subsets with replacement (bootstrapping) and each subset is used to the train the decision tree. Then the result from individual decision trees is combined. This technique helps to reduce variance of a single decision tree \cite{variance_rf}  (variance means, that if the data are randomly split more times and resulting models are compared, they could vary a lot (unlike the linear regression for example, which has generally low variance)).
    \item \textbf{Boosting} This method is based on idea of iterative boosting of weak classifiers into a strong one. In each iteration, the algorithm aims to boost previous-staged classifier by checking of the misclassified instances and attempting to fix these errors. As popular boosting algorithm could be mentioned AdaBoost algorithm \cite{adaboost2}. These methods often tend to overfitting problem \cite{adaboost}.
\end{itemize}

In the experimental part of this thesis is used exclusively bagging technique to make an ensemble of decision trees. To obtain random forest from the ensemble obtained by bagging algorithm, the \textit{randomization} of single trees needs to be added into the process. This technique helps to avoid correlation between individual decision trees and secures better indepedence of classifiers. An idea is to suppress the influence of strongly distinguishing features (dimensions), that are natural choice to split by. Occurrence of these features could result in the correlation of the decision trees, because algorithm would prefer those features especially in the early stages of growing phase and therefore indicates detectable similarity of the trees. Randomization attempts to obviate this problem and selects some random subset of the features in each splitting iteration and only those selected features are used for the split.

\subsection{Combining predictions of the decision trees}

As was mentioned previously, from every decision tree could be extracted either probability distribution of classes from the leaf node or directly predicted class label. There are naturally numerous possibilities of merging these results. Unless there is not special requirement on the model, practically are used two ways. If the outputs are discrete class labels, than the voting scheme is applied and the class with maximum votes is taken as final prediction. If the outputs are probability distributions, those values are averaged and the class with maximum probability is taken. Mathematically noted, the final prediction in this case is

\begin{equation}
    c^* = \text{arg}\max_{c in \mathbb{C}}\frac{1}{|T|}\sum_{t \in \mathbb{T}}p_t(c) \hspace{0.5cm},
    \label{final_pred}
\end{equation}
where $T$ is a set of all trees in the random forest and $p_t(c)$ is a resulting probability distribution from the tree $t \in \mathbb{T}$. In \eqref{final_pred} could be also a weighted average instead, which is applied e.g. if predicting some class is more preferred over predicting others.


\chapter{Neural Networks}
\section{Introduction to Neural Networks (NN)}

For thousands of years, the people have been pursuing the dream to perfectly simulate human or animal brains and adopt their ability to solve problems on the daily basis. For this purpose, the (artificial) neural network was developed and nowadays is frequently used for solving difficult data modelling tasks, statistical analysis and many others. Its functionality is based on the simplified version of real brain information processing and reasoning, where single processing units of brain - neurons - transfer information to other neurons in predefined structure and get activated based on activity status of other adjacent neurons. In the actual brain, the signal from the neuron is transferred to the connected neuron by the synapse. When the neuron accumulates all coming signals from synapses, then it evaluates this coupled signal and if it exceeds the threshold, the neuron sends signal through synapse (connection) to other neuron. This principle is basically what fuels artificial neural networks today.

The beginnings of the artificial neural network dates to early 1950s, almost simultaneously with development and application of programmable electronic units. In 1943, Warren McCulloch and Walter Pitts introduced models inspired by real brain neurons and adopted threshold switches \cite{culloch}. They provided the evidence that even simple model (network) motivated by this approach could compute almost any logic or arithmetic function. In 1957-1958, Frank Rosenblatt and Charles Wightman et al introduced the first successful neurocomputer, Mark I Perceptron, which could identify 20x20 pixel images of simple patterns. In 1959, Frank Rosenblatt covered different version of the perceptron and formulated and proved the convergence theorem of the perceptron (perceptron will be described later). Then Marvin Minsky and Seymour Papert published mathematic paper that studied perceptron in 1969. The conclusion of this paper pointed to the insufficiency of the single perceptron, which was not able to represent classical boolean XOR or sets that are not linearly separable \cite{perceptron}. This led to the approximately 15-year black-out in the field of the neural network research. In 1974 was introduced learning algorithm called backpropagation of error by Paul Werbos \cite{werbos}. Suprisingly, this algorithm was fully acknowledged approximately ten years later. The backpropagation of error as learning procedure was further developed and expanded in 1986 and widely published by the Parallel Distributed Processing Group \cite{backprop-cit}. In that time, the non-separability could be effectively solved by multilayer perceptrons and previous negative conclusions about perceptrons were disproved right away \cite{perceptron-nonsep}. Since then, the development in the field of neural networks and machine learning in general proceeded intensively and actively continues. It is not even possible to mention all milestones in this thesis. Many different neural networks architectures and training algorithms were tested, resulting in complex deep learning structures able to handle massive amount of unlabeled data. They are sucessfully applied in many challenging tasks as bioinformatics, speech recognition, natural language processing etc \cite{nn-application}.  Special NN called Convolutional Neural Networks (CNN) are frequently used especially in the field of image processing.

\section{Building blocks of ANN}

In this section will be described main components of artificial neural network. As we mentioned in the introductory section, architecture of neural network consists of the processing units - neurons, and the connections between neurons. General neuron is associated with the so called \textit{propagation function}, \textit{activation function} and \textit{output function}. These functions process internally the input from other neurons and builds the output of neuron. Propagation function firstly considers all inputs to the neuron and output from the propagation function serves as input for the activation function. The activation function transforms this information into activation value of current neuron and this activation values is then transformed by output function and output is sent to by the connection to the following neuron. Entire process is illustrated in the Figure \ref{fig:single_neuron}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{single_neuron.png}
\caption{Processing of input and creating output of neuron could be formulated as composition of three functions - propagation function, activation function and output function.}
\label{fig:single_neuron}
\end{figure}

As propagation function is almost solely used simple weighted sum of all inputs generally with addition of a constant (called bias). Consider input as vector (if single neuron is connected by the connections from multiple neurons) $\boldsymbol{x} = (x_1,x_2,...,x_k)$, where $k \in \mathbb{N}$ is number of inputs to neuron and $b \in \mathbb{R}$ is a bias and $\boldsymbol{w} = (w_1,...,w_k) \in \mathbb{R}^k$ are connection weights used for weighted sum. Then output $z \in \mathbb{R}$ from propagation function $f: \mathbb{R}^k \rightarrow \mathbb{R}$ is

\begin{equation}
    z = f(\boldsymbol{x}) = \sum_{i=1}^{k}(w_i \cdot x_i) + b   
\end{equation}


The term \textit{weights} is crucial for neural network terminology. Weights are usually associated with the connections and bias term is associated with the neuron. It depends mainly on personal preference how to imagine and comprehend the neural network settings.

As the output function of neuron serves in the most cases simple identity function. This thesis will not focus on details of other possible choices of propagation and output functions, since in the practice are weighted sum for propagation function and identity function for output function common choice and other options are very rarely used.

With these presumptions on propagation and output function, the output of neuron is fully controlled by activation function, which takes as input the weighted sum of input values with added bias. For arbitrary activation function $\phi$ is the procedure visualized in the Figure \ref{fig:perceptron}.


\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{perceptron.png}
\caption{Sum of input values $x_1,...,x_k$, where $k \in \mathbb{N}$ weighted by the weights $w_1,...,w_k$ of connections with added bias term $b$ is taken as argument of arbitrary activation function $\phi$. Resulting value of activation function embodies amount of activity produced by neuron. Activity value of neuron is taken as single output of neuron and could be transferred to other connected neurons.}
\label{fig:perceptron}
\end{figure}


The bias term might be thought as the negative threshold of neuron ($b = - \text{threshold}$). When the weighted sum exceeds the absolute value of the threshold, it gets activated. This discrete behaviour is perfectly simulated by special neuron - perceptron. Its activation function is so called threshold function $\tau : \mathbb{R} \rightarrow \mathbb{R}$ with binary output
\begin{equation}
    \tau(z) = \begin{cases}
    1 & z > 0 \\
    0 & \text{otherwise}
    \end{cases} \hspace{0.5cm}
\end{equation}

If the argument of $\tau$ is positive (threshold value for this neuron is 0), the output is 1, otherwise it is 0. Even this simple model is applicable to many different tasks, but reliably solves only problems with linearly-separable data. This is due to the determinative equation $\sum_{i=1}^{k}(w_i \cdot x_i) + b = 0$, which defines a hyperplane in $\mathbb{R}^k$ and divides the space on two half-spaces. This drawback was surpassed by application of multilayer perceptrons, which is term for interconnected layers of multiple perceptrons (neural network with perceptrons).

\section{Architecture of neural network}

Since we defined a general shape of one processing unit of neural network - neuron, we could define more complex architectures of neurons that helps to solve more complex, non-linear problems. In this thesis we mainly focus on the so called \textbf{feedforward} neural networks \cite{feedforward}, which almost exclusively refers to the topology of multiple layers of neurons and weighted connections only between neurons of two consecutive layers (no skipping allowed). Furthermore, the first layer is referred as \textit{input layer}, then follow arbitrary number of \textit{hidden layers} and the last layer is called the \textit{output layer}. General architecture of the feedforward neural network is depicted in the Figure \ref{fig:feedforward}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{feedforward.png}
\caption{In the Figure is illustrated general architecture of feedforward neural network. Connections are only between neurons of two consecutive layers. Input (each neuron usually receives value of one feature - neuron $j \in \mathbb{N}$ of the input layer accepts value of $j$-th feature $x_j$ of input vector $\boldsymbol{x}$) of the neural network is inserted to the input layer and the output is extracted from the output layer.}
\label{fig:feedforward}
\end{figure}

The universal approximation theorem proved by G. Cybenko \cite{cybenko} states that under some presumption on the activation functions of neuron (e.g. with usage of sigmoid function, this will be described later), this neural network structure could approximate any continuous function on compact subsets of $\mathbb{R}^n$. Also later studies approves, that that the multilayered (even with only one hidden layer) feedforward neural networks could be considered as universal approximators.

When speaking about feedforward neural networks, we usually mean full-connected system, which means, that single neuron in layer $l$ is connected to all neurons in layer $l+1$. If it is necessary to turn down the neurons output to 0 (delete the connection), we force the connection weight to 0 and therefore supress the output of the neuron.

There exists another possible architectures of neural networks, which will be not examined in this thesis, but certainly worth mentioning them.

\begin{itemize}
    \item \textbf{Shortcut connections architecture}:
    This feedforward setting allowed connections skipping of one or more levels. These skipped connections have to be directed towards the output layer (no backwards skipping). Application of this architecture was successfully exploited for example in residual networks (ResNet) \cite{resnet}.
    
    \item \textbf{Direct recurrence networks}: Some neuron in networks could influence themselves. The simple realization is to connect single neuron with itself, which may therefore adaptively strenghten or weaken its activation value.
    
    \item \textbf{Indirect recurrence networks} If connections are permitted backwards (to the foregoing layers), we talk about indirect recurrence \cite{perceptron-nonsep}. 
\end{itemize}

\subsubsection{Insufficiency of perceptron}

When working with neural networks, usual procedure is to adapt all parameters of neurons (weights and biases) to force neural network to behave in the way we require. This procedure is done by employing predefined learning strategy (it will be described later in the thesis), which defines the manner in which neural networks learn on the data and then act independently.

Consider for a moment a feedforward neural network consisting of perceptrons with $\tau$ as activation function and consider arbitrary classification problem (for example classification of digits 0-9). If the neural network encounter training instance, it adapts the weights and biases in the way that the instance is then correctly classified. But even in the case of small change in the weights and biases, the output from $\tau$ function could flip vigorously to ($1 - \text{previous value}$) , so entirely to the other type of activity of neuron. This jump may cause the previous instances to be totally misclassified. So the discontinuity property of threshold function $\tau$ makes it very difficult for neural network to learn correct parameters, especially when solving more complicated, multiclass problems.

To enable better learning ability of neural networks, it is desirable to adopt the property that small change in the weights or biases $\Delta w$ and $\Delta b$ causes only a small change in the output of neuron \cite{feedforward}. This is the property that is not met by $\tau$ function. It is the key to employ other types of activation functions that meet this property in order to establish effective learning procedures.

\section{Activation functions}

In this section is provided summary and descriptions of other applied activation functions.

\subsubsection{Sigmoid function}

One way of solving a discontinuity problem of perceptron is to replace $\tau$ with its smooth approximation - sigmoid function. It is also called logistic function. The shape of this function could be seen in the Figure \ref{fig:sigmoid}. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{sigmoid.png}
\caption{Sigmoid function approximates threshold function $\tau$.}
\label{fig:sigmoid}
\end{figure}
For real input $z \in \mathbb{R}$ is sigmoid function $\sigma$ defined as

\begin{equation}
    \sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}
This function is bounded, nonlinear, differentiable and has positive derivatives. It is successfully applied especially in the output layers of deep neural networks (output lies within (0,1) range, so in the case of classification it could be thought as the probability of the instance acquiring particular label) or in the shallow neural networks (with 1 or max. 2 hidden layers) \cite{actfunc}. Despite some of its favourable properties, it suffers from significant drawbacks related to popular training methods. These drawbacks will be mentioned later once the gradient descent and backpropagation algorithm for training will be described.

\subsubsection{Hyperbolic tangent}

Hyperbolic tangent (tanh) is another alternative as activation function. The shape of the tanh is depicted in the Figure \ref{fig:hyperbolic}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{hyperbolic_tangent.png}
\caption{Hyperbolic tangent function. It has conformable shape as sigmoid function, but its range lies within (-1,1).}
\label{fig:hyperbolic}
\end{figure}

For real input $z \in \mathbb{R}$ is defined as

\begin{equation}
    tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\end{equation}

Hyperbolic tangent has very similar properties as sigmoid function, but inherents some more beneficial properties, e.g. its range lies within (-1,1) and therefore is zero-centered, which produces advantegous behaviour in backpropagation training. Also tanh became preferred choice over sigmoid function in the case of multilayer neural networks due to showing better training performance \cite{tanh_prefer}.

On the other hand, it shares few common drawbacks with the sigmoid function, e.g. vanishing gradient problem, which will be mentioned and described later in the thesis.

\subsubsection{Softmax}

Last of the exponential-based activation functions mentioned in this section is called softmax activation function $\boldsymbol{\theta} : \mathbb{R}^k \rightarrow \mathbb{R}^k$. It is defined for input vector $\boldsymbol{a} \in \mathbb{R}^k$ and $j \in \{1,...,k\}$, where $k \in \mathbb{N}$ as

\begin{equation}
    [\boldsymbol{\theta}(\boldsymbol{a})]_j = \frac{e^{a_j}}{\sum_{i=1}^{k}e^{a_i}}
\end{equation}

Softmax activation function is frequently used especially in the output layers of neural networks, because its definition secures that output vector of softmax is interpretable as probability distribution (output components can be added to 1 and range of softmax is (0,1)).

\subsubsection{Rectified Linear Unit (ReLU)}

ReLU activation function was indtroduced in 2010 by Nair and Hinto \cite{relu} and since then belongs to the most popular activation functions worldwide with state-of-the-art results. It is the most widely used activation function in the field of deep learning. ReLU function is visualized in the Figure \ref{fig:relu}

\begin{figure}[ht]
\begin{minipage}[b]{0.48\linewidth}
\centering
\includegraphics[width=\textwidth]{relu.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.48\linewidth}
\centering
\includegraphics[width=\textwidth]{leakyrelu.png}
\end{minipage}
\caption{In the left part of the Figure is graph of ReLU function. In the right part is graph of Leaky ReLU for $\alpha = 0.05$.}
\label{fig:relu}
\end{figure}


It is defined for real input $z \in \mathbb{R}$ as
\begin{equation}
    \text{ReLU}(z) = \max\{0,z\}
\end{equation}
For positive values of x it behaves as simple linear function and for negative values the output value remains 0. Its similarity with simple linear functions makes it easy to optimize learning algorithms and therefore is training of ReLU neurons more effective and quicker (it need not count exponentials and divisions). The generalization ability, speed of convergence and often better performance with comparison to the sigmoid and tanh functions cause that ReLU functions are much more often used mainly in the hidden layers than those competitive exponentially-based activation functions \cite{actfunc}. Also ReLU function lacks vanishing gradient drawback, which will be discussed later.

ReLU function also suffers from drawbacks, such as easy inclination to overfitting in comparison with the sigmoid function and production of the so called "dead" neurons. It is inflicted when argument of ReLU is negative and ReLU outputs 0. With regard to the fact, that slope of ReLU in the negative part is also 0, it is probable that output of this neuron will stuck on 0 a do not return back. This neuron has no further effect in determining the output and is useless. This problematic will be covered after description of gradient-based learning methods.

Problem of dead neurons could be solved by adoption of the Leaky ReLU activation function \cite{leaky}, which softens the strict condition of forcing output to 0 when input is negative and replaces constant 0 value in the negative part of the horizontal axis with the linear function of small slope parameter $\alpha \in \mathbb{R}$. Therefore the gradient will never be 0. It is defined for real input $z \in \mathbb{R}$ as
\begin{equation}
    \text{LeakyReLU}(z) = \begin{cases}
    z & z > 0 \\
    \alpha z & \text{otherwise}
    \end{cases}
\end{equation}
Values of parameter $\alpha$ should be small, popular choice is $\alpha = 0,01$. Shape of Leaky ReLU could be seen in the right part of the Figure \ref{fig:relu}.

\section{Training methods}

We already sufficiently define neural networks architecture and the way it works by transferring activation values between neurons. What remains is to describe the algorithms which enable our neural network to learn and adapt to new problems.

Let us define some notation in order to describe this issue more clearly. The notation is inspired by \cite{feedforward}. We denote correct (desired) output of neural network as $\boldsymbol{y}(\boldsymbol{x})$ for input vector $\boldsymbol{x}$ and real output (estimation) of the neural network as $\hat{\boldsymbol{y}}(\boldsymbol{x})$. The quality of performance is evaluated by the means of minimizing the so called \textit{loss function} (in the literature could be found also as objective function or cost function). We denote general loss function as $\Lambda$. One typical example of loss function is \textit{Mean Squared Error} (MSE), also known as quadratic cost function,which is defined as
\begin{equation}
    \Lambda(\mathbb{W},\mathbb{B},\mathbb{X}) = \frac{1}{2n} \sum_{i=1}^{n}||\boldsymbol{y}({\boldsymbol{x}}_i)- \hat{\boldsymbol{y}}({\boldsymbol{x}}_i)||^2 \hspace{0.5cm},
    \label{mse}
\end{equation}
where $\mathbb{W}$ is a set of all weights, $\mathbb{B}$ is a set of all biases and $\mathbb{X} = \{\boldsymbol{x}_1,...,\boldsymbol{x}_n\}$ is a set of all training vectors, where $n \in \mathbb{N}$ is a number of all training vectors. Norm $||\cdot|| = ||\cdot||_2$ is Euclidean norm. It is evident that estimation $\hat{\boldsymbol{y}}({\boldsymbol{x}})$ depend on $\boldsymbol{x}$ as well as on all weights and biases in the neural network. Later we will mention other frequently used loss functions with better application especially to the task of classification.

Now will be described the most popular algorithm used for finding optimal weights and biases, which optimize the general loss function $\Lambda$ - \textit{gradient descent}.

\subsection{Gradient Descent}

Gradient descent algorithm is a popular nonlinear programming technique for a wide range of minimization tasks. It is considered as a benchmark for the minimization of loss functions in the field of neural networks. Let us consider general loss function $\Lambda(\boldsymbol{v})$ depending on vector $\boldsymbol{v}$. Gradient descent constructs the sequence of $\boldsymbol{v}^{(k)}$, that for each $k \in \mathbb{N}$ satisfies the condition $\Lambda(\boldsymbol{v}^{(k+1)}) < \Lambda(\boldsymbol{v}^{(k)})$ (except when $\boldsymbol{v}^{(k)}$ is optimal). The sequence is constructed as
\begin{equation}
    \boldsymbol{v}^{(k+1)} = \boldsymbol{v}^{(k)} + \eta\Delta \boldsymbol{d} ^{(k)} \hspace{0.5cm},
\end{equation}
where $\Delta \boldsymbol{d}^{(k)}$ is a real vector called the step or search direction, with same dimension as $\boldsymbol{v}^{(k)}$ and $\eta > 0$ is called a learning rate \cite{grad}. Question is how to choose $\Delta \boldsymbol{d}^{(k)}$ in order to fulfill the condition $\Lambda(\boldsymbol{v}^{(k+1)}) < \Lambda(\boldsymbol{v}^{(k)})$.

We must choose $\Delta \boldsymbol{d}^{(k)}$ in the way the $\Lambda$ function decreases in that direction. We could apply the calculus and search the direction in the way that the following condition is met:
\begin{equation}
    \nabla_{\Delta \boldsymbol{d}^{(k)}} \Lambda(\boldsymbol{v}^{(k)}) = \Delta \boldsymbol{d}^{(k)} \cdot \nabla \Lambda(\boldsymbol{v}^{(k)}) < 0
    \label{condition_calculus} \hspace{0.5cm},
\end{equation}
where $\nabla \Lambda(\boldsymbol{v}^{(k)})$ is gradient of function $\Lambda$ and is defined for vector $\boldsymbol{v} = (v_1,...,v_m)^T$, $m \in \mathbb{N}$ as

\begin{equation}
    \nabla \Lambda(\boldsymbol{v}) = (\frac{\partial \Lambda}{\partial v_1}(\boldsymbol{v}),...,\frac{\partial \Lambda}{\partial v_m}(\boldsymbol{v}))^T.
\end{equation}
Condition \eqref{condition_calculus} expresses derivative of $\Lambda(\boldsymbol{v}^{(k)})$ in the direction of $\Delta \boldsymbol{d}^{(k)}$. If this derivative is negative, we ensures that $\Lambda$ decreases in that direction.

Suppose choosing $\Delta \boldsymbol{d}^{(k)} = - \nabla \Lambda(\boldsymbol{v}^{(k)})$. Then
\begin{equation}
    \Delta \boldsymbol{d}^{(k)} \cdot \nabla \Lambda(\boldsymbol{v}^{(k)}) = - ||\nabla \Lambda(\boldsymbol{v}^{(k)})||^2 \leq 0
    \label{condition_full}
\end{equation}

Moreover, equality in \eqref{condition_full} is held only if $\nabla \Lambda(\boldsymbol{v}^{(k)}) = \boldsymbol{0}$ and therefore no further update is performed, so the algorithm could be terminated. In the practice, euclidean norm of gradient is compared to the small positive threshold $\epsilon$ and algorithm is terminated when $||\nabla \Lambda(\boldsymbol{v}^{(k)})|| \leq \epsilon$ \cite{grad}.

Final equation for constructing the sequence is therefore

\begin{equation}
    \boldsymbol{v}^{(k+1)} = \boldsymbol{v}^{(k)} - \eta\nabla \Lambda(\boldsymbol{v}^{(k)})
\end{equation}

To provide more intuitive perspective, gradient descent algorithm iteratively searches for directions in which the value of loss function is decreasing and stops when it is sufficiently close to optimum (could be local or global minimum). The initial (starting) point of algorithm need to be selected in advance. It is usually picked randomly. The procedure is illustrated in the Figure \ref{fig:grad}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.75\textwidth]{grad2_desc.png}
\caption{Gradient descent algorithm. In each iteration it finds the direction in which loss function decreases. In this case, the green point is "rolling down" towards the global minimum $(0,0)^T$.}
\label{fig:grad}
\end{figure}
Of course, gradient descent is not universal method which always find global minimum of the arbitrary function. There are some requirements on loss function to ensure functionality. Especially when the loss function is not "nice" as one in the Figure \ref{fig:grad}, it could stuck in the local minimum instead or even do not converge, which also depends on the initial point of algorithm. Theory behind the convergence to global optimum is properly described almost exclusively for (strictly) convex functions \cite{grad}, which is for example function in the Figure \ref{fig:grad}, $f(x,y) = x^2 + y^2$ or quadratic cost function defined in \eqref{mse}.

Also the learning rate $\eta$ significantly affects convergence. If it is too small, the time until convergence could be very long and if it is too big, it could bounce back and forth around the optimal value and never reach it. Deriving the most suitable learning rate for particular application usually requires a bit of experimenting around. There also exists methods to set learning rate or adjust it during execution. Some of them has been used in the experimental part of this thesis and will be mentioned later.

We should concretize the update rule in the case of modifying weights and biases of neural network by gradient descent. Update rules for arbitrary weight $w$, bias $b$ and $k$-th iteration of gradient descent is
\begin{align}
    w^{(k+1)} &= w^{(k)} - \eta \frac{\partial \Lambda}{\partial w}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X}) \\
    b^{(k+1)} &= b^{(k)} - \eta \frac{\partial \Lambda}{\partial b}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X})
\end{align}

\subsection{Stochastic Gradient Descent (SGD)}

In order to accelerate learning of neural network, \textit{stochastic gradient descent} algorithm was developed \cite{feedforward}. It is based on estimating the gradient of loss function instead of computing it precisely.
For instance, we show the algorithm in the case of quadratic cost function in \eqref{mse}. It could be rewritten as
\begin{align}
    \Lambda(\mathbb{W},\mathbb{B},\mathbb{X}) &= \frac{1}{n}\sum_{i=1}^{n}\Lambda_{\boldsymbol{x}_i} = \frac{1}{n}\sum_{i=1}^{n}\frac{||\boldsymbol{y}(\boldsymbol{x}_i) - \hat{\boldsymbol{y}}(\boldsymbol{x}_i)||^2}{2}\\ \Lambda_{\boldsymbol{x}_i} &= \frac{||\boldsymbol{y}(\boldsymbol{x}_i) - \hat{\boldsymbol{y}}(\boldsymbol{x}_i)||^2}{2}
\end{align}
Therefore, gradient of loss function could be written as (we omit the parameters of $\Lambda$ for simplification)

\begin{equation}
    \nabla \Lambda = \frac{1}{n}\sum_{i=1}^{n} \nabla \Lambda_{\boldsymbol{x}_i}
    \label{good_shape}
\end{equation}
Basically, this term is only average of gradients of $\Lambda_{\boldsymbol{x}_i}$ across all training instances. Idea of stochastic gradient descent is to replace averaging across all training instances with averaging only across randomly sample mini-batch \\ $x_{i_1},...,x_{i_m}$ of size $m < n$, $m \in \mathbb{N}$. We obtain estimate
\begin{equation}
    \nabla \Lambda = \frac{1}{n}\sum_{i=1}^{n} \nabla \Lambda_{\boldsymbol{x}_i} \approx \frac{1}{m}\sum_{l=1}^{m} \nabla \Lambda_{\boldsymbol{x}_{i_l}}
\end{equation}

If the size of mini-batch is large enough, we could expect that this approximation will be roughly equal to the real gradient. This algorithm works for loss functions that could be rewritten in the same manner as quadratic cost function from the example. Of course, there could be some statistical deviations and the estimation is not always precise, but what we need is to move in direction where the loss function is expected to decrease, it need not to be exactly the direction of gradient, so working with estimation is justified. We shall rewrite the update rule in the case of SGD as
\begin{align}
    w^{(k+1)} &= w^{(k)} - \frac{\eta}{m}\sum_{l=1}^{m} \frac{\partial \Lambda_{\boldsymbol{x}_{i_l}}}{\partial w}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X})
    \label{sgd_weight}\\
    b^{(k+1)} &= b^{(k)} - \frac{\eta}{m}\sum_{l=1}^{m} \frac{\partial \Lambda_{\boldsymbol{x}_{i_l}}}{\partial b}(\mathbb{W}^{(k)},\mathbb{B}^{(k)},\mathbb{X})
    \label{sgd_bias}
\end{align}

The algorithm in practice usually runs in several epochs. Each epoch is characterized by random sampling of mini batches of given size $m$ and then by training with those mini batches. When one epoch is finished, training instances are randomly mixed and mini batches are sampled again and training continues in new epoch with new mini batches.

In \eqref{sgd_weight} and \eqref{sgd_bias} could be omitted $\frac{1}{m}$ factor, which corresponds only to the scaling of learning rate. Factor $\frac{1}{n}$ is sometimes omitted also in definition of MSE in \eqref{mse} \cite{feedforward}.

\section{Backpropagation algorithm}

We already presented an idea of training neural network via (stochastic) gradient descent. But we did not provide the way of computing partial derivatives of loss function with respect to weights and biases, which are necessary to execute the training algorithm. This could be of course done by computing the derivatives analytically, which is definitely very uncomfortable and almost insane idea, especially when adopting large neural networks. For this reason, the \textit{backpropagation algorithm} was developed in order to retrieve partial derivatives with respect to all weights and biases involved in the neural network in comfortable and quick way.

The derivation and description of backpropagation algorithm is motivated by papers \cite{backprop} and \cite{feedforward}. We define some helpful notation in advance in order to simplify the derivation. Let us denote output from $l$-th layer of neural network and $j$-th neuron of that layer as $a_j^l$, also entire output vector of $l$-th layer will be denoted as $\boldsymbol{a}^l$. Term $w_{jk}^l$ denotes weight of connection going from neuron $k$ in previous layer $(l-1)$ to neuron $j$ in layer $l$ (it is in reversed order on purpose, it will simplify further notation with respect to the matrix multiplication) and term $\mathbb{W}^l = (w_{jk}^l)_{j,k}$ denotes matrix of weights directed from layer $(l-1)$ to $l$. Bias of node $j$ in layer $l$ is denoted as $b_j^l$ and biases folded into vector of biases of layer $l$ is $\boldsymbol{b}^l$. By applying this notation, we could write comfortably a feedforward dependency for output from layer $l$ as
\begin{equation}
    \boldsymbol{a}^l = \sigma(\mathbb{W}^l \boldsymbol{a}^{l-1} + \boldsymbol{b}^l) \hspace{0.5cm},
    \label{comfort_write}
\end{equation}
where $\sigma(\cdot)$ is arbitrary activation function used in neurons of layer $l$, which is applied in element-wise fashion. We also denote argument of $\sigma(\cdot)$ in \eqref{comfort_write} as
\begin{equation}
    \boldsymbol{z}^l = \mathbb{W}^l \boldsymbol{a}^{l-1} + \boldsymbol{b}^l
\end{equation}
The aim of backpropagation algorithm is to compute $\frac{\partial \Lambda}{\partial w}$ and $\frac{\partial \Lambda}{\partial b}$ (we will omit arguments of the functions for simplification, all of them are evaluated in the values of current weights, biases and training instance) with respect to arbitrary weight $w$ and bias $b$. These partial derivatives expresses, how much is loss function changed when $w$ or $b$ is changed. Of course, the main goal is to find optimal weights and biases, which corresponds to the local (global) minimum of loss function. Intuitively, we could imagine that in each layer of neural network arises some error (from imperfection of current weights and biases), which cause flaws in the output layer. Furthermore, we could say that even individual neurons produce these errors, which consequently leads to corrupted output. But what we can measure is only deviation of current output (extracted from output layer) of neural network from desired value, because we do not know in advance "correct" output (activation values) of other layers. So briefly, what backpropagation really does is that it \textit{backpropagates} error from the output layer to the previous layers and then from these \textit{error} terms computes corresponding partial derivatives $\frac{\partial \Lambda}{\partial w}$ and $\frac{\partial \Lambda}{\partial b}$. This procedure is illustrated in the Figure [BLA].

We will now define this \textit{error} term heuristically. Output from $j$-th neuron of layer $l$ could be written as $a_j^l = \sigma(z_j^l)$. Let's imagine for a moment, that layer $l$ has corrupted output (not corresponding to the optimal value) denoted as $\hat{a}_j^l$. This corrupted value could be written as $\hat{a}_j^l = \sigma(\hat{z}_j^l) =\sigma(z_j^l + \Delta z_j^l)$, where $\Delta z_j^l$ expresses deviation of argument $z_j^l$ from optimal value. In case that other weights and biases are already optimal, the difference between optimal cost and "corrupted" cost would be approximately $\frac{\partial \Lambda}{\partial z_j^l}\Delta z_j^l$ (from Taylor expansion, we suppose that $\Delta z_j^l$ is "small enough"). Deviation is getting larger, when value of $\frac{\partial \Lambda}{\partial z_j^l}$ is getting larger. Also when value of $\frac{\partial \Lambda}{\partial z_j^l}$ is close to 0, then neuron is very close to optimal state. Therefore, we could define error term as
\begin{equation}
    \delta_j^l = \frac{\partial \Lambda}{\partial z_j^l}
\end{equation}
This is heuristic derivation and of course, it is not absolutely precise and certainly requires deeper math behind (e.g. we omitted trade-off between $\frac{\partial \Lambda}{\partial z_j^l}$ and $\Delta z_j^l$ in estimating the deviation), but this derivation serves mainly for description of intuitive behaviour of $\delta_j^l$. Also more important is, that $\delta_j^l$ is only provisional term, which will lead us to derivation of values of real interest - $\frac{\partial \Lambda}{\partial w}$ and $\frac{\partial \Lambda}{\partial b}$.

The algorithm always backpropagates the error of single training instance $\boldsymbol{x}$ (we can use only one training instance in time), so what it really computes is $\frac{\partial\Lambda_\boldsymbol{x}}{\partial w}$ and $\frac{\partial \Lambda_\boldsymbol{x}}{\partial b}$. Therefore the loss function $\Lambda$ needs to be rewritten in the same manner as in \eqref{good_shape}, which we assume. From now, we will omit the subscript and denote $\Lambda_\boldsymbol{x}$ simply as $\Lambda$. Another assumption on loss function is that it can be expressed as function of output $\boldsymbol{a}^L$ from the output layer, where $L$ denotes the output layer. We need $\Lambda = \Lambda(\boldsymbol{a}^L)$, but it may depend also on other variables. This assumption will be explained later. For instance, both assumptions are fulfilled by quadratic cost function and also by other loss functions presented later in this thesis.


Since we defined $\delta_j^l$, we now need to find equations for values of real interest, ig. partial derivatives of loss function with respect to weights and biases. Entire backpropagation algorithm stands on 4 equations, which allow us to compute values of real interest from errors in individual layers of neural network. They can be simply derived by careful application of the chain rule from multivariate calculus.

\begin{enumerate}
    \item \textbf{Equation to calculate error $\boldsymbol{\delta}^L$ of the output layer L}:
    \begin{equation}
        \delta_j^L = \frac{\partial \Lambda}{\partial a_j^L}\sigma'(z_j^L)
        \label{BP1}
    \end{equation}
    As we mentioned before, the backpropagation algorithm runs backwards. First, we need to compute error in the output layer. It is calculated by multiplication of partial derivative of loss function with respect to the output $a_j^L$ of $j$-th neuron of layer $L$ and derivative of activation function $\sigma$ in $z_j^L$. This equation is perfectly intuitively understandable, because the error term is basically expressed as trade-off between "how much is loss function changing with respect to the output from neuron ($\frac{\partial \Lambda}{\partial a_j^L}$)" and "how much is output of the neuron changing with respect to the input value to the activation function ($\sigma'(z_j^L)$)". If any of these values are small, then error of that neuron is also small. Furthermore, both values could be easily retrieved and the shape of derivatives is usually computed analytically (obviously depending on the shape of loss function and activation function).
    
    \item \textbf{Equation to backpropagate the error from layer $(l+1)$ to previous layer $l$}
    \begin{equation}
        \delta_j^l = [(\mathbb{W}^{(l+1)})^T\boldsymbol{\delta}^{(l+1)}]_j \sigma'(z_j^l)
        \label{BP2}
    \end{equation}
    This equation gives us a way to backpropagate the error backwards. Together with \eqref{BP1}, we can compute $\boldsymbol{\delta}^l$ (vector of errors $\delta_j^l$ of neurons in layer $l$) in arbitrary layer $l$. It is obtained by multiplying transpose of weight matrix $\mathbb{W}^{(l+1)}$ with vector of errors of $(l+1)$ layer $\boldsymbol{\delta}^{(l+1)}$. This could be thought as a "projection" of error to the previous layer. Then the result is multiplied by $\sigma'(z_j^l)$, same as in the case \eqref{BP1}, because the error is influenced by the speed of change of the activation function in $z_j^L$.
    
    \item \textbf{Equation to calculate} $\frac{\partial \Lambda}{\partial b_j^l}$
    \begin{equation}
        \frac{\partial \Lambda}{\partial b_j^l} = \delta_j^l
        \label{BP3}
    \end{equation}
    This first value of real interest could be determined easily, because equation states that it is equal directly to the error term. And \eqref{BP1} and \eqref{BP2} already give us tools to compute the error in arbitrary layer.
    
    \item \textbf{Equation to calculate} $\frac{\partial \Lambda}{\partial w_{jk}^l}$
    \begin{equation}
        \frac{\partial \Lambda}{\partial w_{jk}^l} = a_k^{(l-1)}\delta_j^l
        \label{BP4}
    \end{equation}
    This equation computes the partial derivative of the loss function with respect to any weight. It depends on activation (output) from $k$-th neuron of layer $(l-1)$ and error of $j$-the neuron of the consecutive layer $l$.
\end{enumerate}
By applying these 4 equations, we could compute all values of interest easily. Also this equations could be rewritten completely into the vector shape and therefore solved more efficiently (e.g. by exploiting different algebraic libraries, such as Numpy \cite{numpy}).

Now we will derive \eqref{BP3} and \eqref{BP4} by gradually applying the chain rule. Derivation of \eqref{BP1} and \eqref{BP2} could be found for example in \cite{feedforward}.

DOPSAT ODVOZENÍ, DODĚLAT OBRÁZKY, DALŠÍ LOSS FUNCTIONS, REGULARIZATION A ADAM A NESTEROV (cca 6-7 dalších stran) - v případě potřeby se dá zkrátit na to nejnutnější (3 strany cca)









\chapter{Decision-Tree-Inspired NN architecture}
In this section is provided detailed derivation of the basic neural network architecture, which is further employed in all subsequently proposed models. The main idea is built on the article \cite{NRF} describing 1 to 1 transformation of arbitrary regression tree (random forest regressor) to the specifically designed neural network (ensemble of neural networks).

Unfortunately, an approach presented in \cite{NRF} is not uniformly convertible to the classification problem. Based on this issue, the  alternative architecture and initial settings are proposed in order to simulate the exact behaviour of corresponding decision tree classifier.

This reformulation provides a sensible opportunity to enhance the decision tree performance, because parameters of newly constructed neural network could be better adapted with usage of the backpropagation algorithm of neural networks and therefore achieve superior classification.

\section{Architecture and initial settings}

The initial weights and biases of neurons and architecture of input layer and first two hidden layers will remain same among all proposed models. The settings was motivated by \cite{NRF}. A sample of such architecture (only input layer and first two hidden layers) is illustrated in Figure \ref{fig:basic_arch} copying decisions of the decision tree depicted in \ref{fig:dec_ilust}, which splits the space by two hyperplanes as illustrated also in \ref{fig:dec_ilust}. Let's first consider neurons in the first and second hidden layer of network as perceptrons. This means, that activation function is
\begin{equation}
    \tau(\boldsymbol{x}) = 2\mathbf{1}_{\boldsymbol{x} \geq 0} - 1\hspace{0.5cm},
\end{equation}
where all math operations on vector $\boldsymbol{x}$ are element-wise. Also
\begin{equation}
    (\mathbf{1}_{\boldsymbol{x} \geq 0})_i = \begin{cases}
    1, & \text{if}\hspace{0.25cm} x_i \geq 0 \\
    0, & \text{otherwise}
    \end{cases}\hspace{0.5cm},
\end{equation}

where $(\mathbf{1}_{\boldsymbol{x} \geq 0})_i$ is $i$-th element of $\mathbf{1}_{\boldsymbol{x} \geq 0}$.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{ilustrace_in_space.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{decision_tree_ilustrace.png}
\end{minipage}
\caption{In the left Figure are depicted 3 color-coded classes divided by the decision tree in the right Figure, that are separated by two hyperplanes: \\$x_1 - 2 = 0$ and $x_2 - 1 = 0$. Inner nodes of the decision tree are green colored with corresponding split function next to them, whereas the leaves are red colored. All nodes are numbered.}
\label{fig:dec_ilust}
\end{figure}
\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{ilustrace_basic_structure.png}
\caption{Transformation of the decision tree in Figure \ref{fig:dec_ilust} to the neural network with two hidden layers. The first hidden layer detects the decisions of inner nodes same numbered as in \ref{fig:dec_ilust}. The second hidden layer retrieves leaf membership of input vector same as in the decision tree. Not null connections between neurons are bold higlighted with corresponding weights. Biases are written next to the neurons. All dashed connections indicates null connection (weight equals 0).}
\label{fig:basic_arch}
\end{figure}

So perceptrons from the first and second hidden layer outputs $+1$ or $-1$. Why is it chosen in such manner will be clear from further explanations.

\subsection{First hidden layer}

The first hidden layer should copy decisions of all inner nodes present in the corresponding decision tree. The first hidden layer has same number of neurons as number of inner nodes. As was explained in the chapter about decision trees and random forests, each inner node $k \in \{1,...,L-1\}$ of the decision tree, where $L-1$ is total number of inner nodes (in fact, if $L-1$ is total number of inner nodes, then $L$ is total number of leaves present in the decision tree), possesses split function with parameters $j_k \in \{1,...,n\}$, which is one dimension in a $n$-dimensional space that is used for split and also $\alpha_{j_k}$, which is a threshold. Let's also define function $s_k$ as 
\begin{equation}
    s_k(\boldsymbol{x}) = x_{j_k} - \alpha_{j_k}\hspace{0.5cm}.
\end{equation}
It is apparent that equation $s_k(\boldsymbol{x}) = 0$ defines a hyperplane in $\mathbb{R}^n$, which splits the space in inner node $k$.

In order to obtain all decisions of inner nodes in the corresponding neurons of our neural network, we initialize weights in neuron $k$ as $(0,..,0,1,0,..,0)^T$ with single 1 in $j_k$-th position and 0 otherwise. Bias is set to $-\alpha_{j_k}$. Hence, output of the first hidden layer is $(\tau(s_1(\boldsymbol{x})),\tau(s_2(\boldsymbol{x})),...,\tau(s_{L-1}(\boldsymbol{x}))$ and it precisely copies decisions of inner nodes, with +1 indicating that input vector belongs to the right side of the hyperplane and -1 otherwise (and +1 if it belongs to the hyperplane). This is also done for the inner nodes outside the path of the input vector. The illustration can be seen in Figure \ref{fig:first_hid_layer}.


\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{first_hidden_layer_space.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{first_hidden_layer.png}
\end{minipage}
\caption{Space in the left figure is divided by red (in inner node number 1) and black (in inner node number 2) hyperplanes. If we consider point $\boldsymbol{x} = (x_1,x_2)^T = (2.75,0.45)^T$ highlighted in left Figure, the output from the first hidden layer is +1 from neuron 1 (corresponding to the red hyperplane) and -1 from neuron 2 (corresponding to the black hyperplane).}
\label{fig:first_hid_layer}
\end{figure}

\subsection{Second hidden layer}

From all inner node decisions obtained by the first hidden layer it should be possible to reconstruct the exact leaf membership of the input vector $\boldsymbol{x}$. This is the main task to accomplish by the second hidden layer. If there are $L-1$ neurons in the first hidden layer, then the second hidden layer consists of $L$ neurons, each one corresponding to the one individual leaf in the decision tree.

We connect neuron $m$ in the first hidden layer to neuron $m'$ in the second hidden layer with not null weight if and only if inner node corresponding to the neuron $m$ belongs to the path from root node to the leaf corresponding to the neuron $m'$. The weight is initialized to +1 if the split by inner node $m$ is to the right child and -1 otherwise. If neuron $m$ is not part of the path from root to the leaf, the weight is initialized always to 0.

Based on this setting, it could be simply deduced that number of not null connections from the first hidden layer (weights) to the (arbitrary) neuron $m'$ in the second hidden layer is same as length of the path from root to the leaf $m'$. This is illustrated in Figure \ref{fig:sec_hidd_len}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{sec_hidd.png}
\caption{In the left figure is highlighted blue path from the root node 1 to the leaf 3. This has length 1 and also only one initialized not null connection from the first hidden layer exists, because only root node is part of the path. In the remaining figures are depicted paths for other leaves (blue colored) from the root node (except of leaf 6, which is in same depth as leaf 7). It is easy to see, that equality between length of the path from the root node to the particular leaf and number of not null connections from the first hidden layer to the corresponding neuron holds always.}
\label{fig:sec_hidd_len}
\end{figure}

If output from the first hidden layer is $\boldsymbol{v} = (\pm 1, \pm 1,....,\pm 1)^T$, which encodes all decisions of inner nodes of the decision tree, then output from the neuron $m'$ in the second hidden layer is $\tau(\sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m'))$, where\\ $\boldsymbol{w}^{m'} = (w_1^{m'},w_2^{m'},...,w_{L-1}^{m'})^T$ is a vector of  weights for connections to neuron $m'$. These weights are not null if the corresponding inner node is involved in the path from root to the leaf $m'$ and are +1 if it is sent to the right child and -1 otherwise. For all inner nodes that are not involved in the root-leaf path are weights initialized to 0.

Desired behaviour of the neuron $m'$ in the second hidden layer is to output +1 if the input vector ends in leaf $m'$ and -1 otherwise. For this purpose, $bias(m')$ must be correctly set. To do so, it is sufficient to notice that the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i)$ as the first term in the argument of $\tau(\cdot)$ function equals to the length of the path from root node to leaf $m'$, if and only if the input ends in leaf $m'$. For convenience, let us denote this length as $l(m')$. It is a simple consequence of the fact, that the number of not null weights $w_{i_k}^{m'}$ is same as $l(m')$ and also they have same magnitude ($|w_{i_k}^{m'}| = 1$) and same sign as $v_{i_k}$, where $i_k \in \{1,...,L-1\}, w_{i_k}^{m'} \neq 0$. Therefore, each member of the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i) = \sum_{i_k=1,w_{i_k}^{m'} \neq 0}^{L-1}(w_{i_k}^{m'}v_{i_k})$ equals to +1 and all members sum up to $l(m')$.

Moreover, if the input does not end in leaf $m'$, then in the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i)$ exists a not null member in which interfere 2 integers (ones) with different signs, resulting in -1. Hence is clear, that if input does not end in leaf $m'$, the sum holds inequality $\sum_{i=1}^{L-1}(w_i^{m'}v_i) \leq l(m')-1 < l(m')$. For more precise intuition, the illustration is provided in Figure \ref{fig:sec_hidd_ilust}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{sec_hidd_ilust.png}
\caption{Demonstration of inequality $\sum_{i=1}^{L-1}(w_i^{m'}v_i) \leq l(m')-1 < l(m')$ in case of leaf number 7 (corresponding to one neuron in the second hidden layer of our network), if input does not end in leaf $m'$. The red path in the picture indicates real path of input in the decision tree. Our sample input ends in leaf number 6, as could be seen from the illustration. The output from the first hidden layer would therefore be $\boldsymbol{v} = (v_1,v_2,v_3)^T = (+1,-1,-1)^T$. But weights corresponding to neuron 7 (leaf 7) are $\boldsymbol{w}^7 = (w_1^7,w_2^7,w_3^7)^T = (+1,-1,+1)^T$. After multiplying the values in red circles and summing the results up, we obtain $\sum_{i=1}^{L-1}(w_i^{7}v_i) = 2 < 3$, where 3 means the length of the root-leaf path. A decrease is caused due to the interference of different signs in a grey circle.}
\label{fig:sec_hidd_ilust}
\end{figure}

After these considerations, the reasonable choice of $bias(m')$ is
\begin{equation}
    bias(m') = -l(m') + 0.5
\label{eq:bias}    
\end{equation}
and then $\sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m')$ has following property:
\begin{equation}
    \sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m')\begin{cases}
    > 0, & \text{if input ends in leaf}\hspace{0.18cm} m' \\
    < 0, & \text{otherwise}
    \end{cases}
\label{eq:sec_hidd_property}
\end{equation}

With respect to the property of $\tau(\cdot)$ function argument in \eqref{eq:sec_hidd_property}, the second hidden layer outputs a vector of $(-1,...,-1,+1,-1,...,-1)^T$ with a single positive 1 indicating the correct leaf membership of an input.

To retain this $\tau(\cdot)$ argument property, it is sufficient to choose any other arbitrary constant in \eqref{eq:bias} in range $(0,1)$ instead of 0.5. But to stay consistent with \cite{NRF}, we also used the proposed value of 0.5 in conducted experiments.

At this stage, we already defined architecture and initial weights and biases settings of first two hidden layers in order to transform decision tree into neural network with the same properties. All that remains is to gain classification predictions from the second hidden layer.

\subsection{Output layer}

In this section is proposed architecture and initial setting for output layer, that will gain same predictions as the decision tree. Unfortunately, same method proposed for regression trees in \cite{NRF} is not directly applicable in the classification case. Therefore we propose an alternative for the classification case, that give same classification outcomes as the corresponding decision tree.

The output layer will be constructed as follows: The number of neurons in output layer is equal to number of classes we desire to classify. Each neuron corresponds to only one particular class (one label). Neuron with the highest activation represents the predicted class of neural network. In the experiments were used two types of neurons in output layer - with sigmoid and softmax activation functions, as were discussed in Chapter [Activation functions].  To get the same performance as the decision tree, we must retrieve probability distributions stored in leaves from the leaf membership encoded in the second hidden layer. If the output layer outcomes the same probability values for classes as the decision tree, hence the neural network performs alike.

Let's denote output from the second hidden layer as\\ $\boldsymbol{r} = (-1,...,-1,+1,-1,...,-1)^T$, where the position of +1 indicates the leaf where the input falls in. For each leaf $l \in \{1,...,L\}$ we denote a probability vector $\boldsymbol{p^l} = (p^l_1,p^l_2,...,p^l_C)^T$ with probabilities of individual classes, that are stored in leaf $l$, where $C$ is total number of classes. If $\boldsymbol{r}$ has +1 as the first element (corresponding to the first leaf), i.e. $r_1 = +1$, the output layer should outcome $\boldsymbol{p}^1$. If $r_2 = +1$, the outcome should be $\boldsymbol{p}^2$ etc.

If we initialize biases in the output layer to 0, then appropriate initialization weights could be obtained by solving the system of linear equations with a regular matrix $\mathbb{A}$
\begin{equation}
    \mathbb{A}=\begin{pmatrix}
    1 & -1 & -1 &\hdots & -1 \\
    -1 & 1 & -1 & \hdots & -1\\
    \vdots & \ddots & \ddots & \ddots&\vdots \\
    -1 & \ddots & \ddots & \ddots&-1 \\
    -1 & -1 & \hdots & -1 & 1
    \end{pmatrix}\hspace{0.5cm}.
\end{equation}

Matrix $\mathbb{A} \in \mathbb{R}^{L\times L}$, where $L$ is a total number of leaves in the decision tree. The determinant of matrix $\mathbb{A}$ is
\begin{equation}
    \text{det}\mathbb{A} = \begin{vmatrix}
    1 & -1 & -1 &\hdots & -1 \\
    -1 & 1 & -1 & \hdots & -1\\
    \vdots & \ddots & \ddots & \ddots&\vdots \\
    -1 & \ddots & \ddots & \ddots&-1 \\
    -1 & -1 & \hdots & -1 & 1
    \end{vmatrix} = (-1)^{2L-3}\cdot2^{L-1}\cdot(L-2)
\label{eq:determinant}
\end{equation}

For the proof of \eqref{eq:determinant} see Chapter [Proves]. Matrix $\mathbb{A}$ is therefore always regular with except of $L = 2$. In this case, $\text{det}\mathbb{A} = 0$ and matrix $\mathbb{A}$ is singular. But for $L=2$, the decision tree has only root node and 2 leaves, which is rarely a well-functional model in practical use. It could have a good performance almost only in case of data with significantly unambigous geometric deployment, where occurs only 2 classes and exists one hyperplane that sufficiently separates them. As the conclusion, we will almost never encounter such an elementary model.

In case of invertible activation function $\sigma(\cdot)$ in the output layer (in our experiments were used sigmoid and Leaky ReLU activation functions, which are both invertible (softmax is problematic, describe it)), we obtain appropriate weights for neuron $c$ (also represents class $c \in \{1,...,C\}$ that this neuron corresponds to) in output layer after solving the following system of linear equations:

\begin{equation}
    \mathbb{A}\begin{pmatrix}
    w^c_1\\
    w^c_2\\
    \vdots\\
    w^c_L
    \end{pmatrix} = \sigma^{-1}(\begin{pmatrix}
    p^1_c\\
    p^2_c\\
    \vdots\\
    p^L_c
    \end{pmatrix})\hspace{0.5cm},
    \label{eq:lin_eq}
\end{equation}

where $\boldsymbol{w}^c = (w^c_1,...,w^c_L)^T$ are weights of connections from the second hidden layer to neuron $c$ in the output layer and $\sigma^{-1}(\cdot)$ is inverse function of $\sigma(\cdot)$. In other words, the appropriate weights for neuron $c$ in the output layer are obtained as

\begin{equation}
    \begin{pmatrix}
    w^c_1\\
    w^c_2\\
    \vdots\\
    w^c_L
    \end{pmatrix} = \mathbb{A}^{-1}(\sigma^{-1}(\begin{pmatrix}
    p^1_c\\
    p^2_c\\
    \vdots\\
    p^L_c
    \end{pmatrix}))\hspace{0.5cm}.
    \label{eq:weights_output}
\end{equation}

After initialization of biases to 0 and solving the system of linear equations for all neurons in the output layer (or computing the inverse of matrix $\mathbb{A}$) and for appropriate activation function chosen in advance, we gain also all initialization weights. This initial setting causes the neural network to output same predictions as from the corresponding decision tree and hence to get equally performing classification model.

\section{NN models with decision-tree-initialization}

\subsection{Perceptron activation function replacement}

In order to apply reasonable training procedure with backpropagation algorithm on proposed method, it is suitable to replace perceptron activation function $\tau(\cdot)$ with its smooth approximation. For this purpose the hyperbolic tangent function was adopted (Figure BLA). This affects original transformation, because due to this approximation the neural random tree is no longer one-to-one transformation to former decision tree. But still under certain conditions (especially the transition slope from the negative part of the $\tanh(\cdot)$ function to the positive one - the more upright, the better approximation we get) it could come fairly close to the perceptron function and therefore entire model performance remains unchanged with respect to the decision tree. Usage of $\tanh(\cdot)$ in first and second hidden layer is necessary with respect to the transformation procedure, even that $\tanh(\cdot)$ has some drawbacks as activation function (e.g. vanishing gradients [ref]). Using other activation functions in this case is not appropriate, because it will lack any decision tree relationship.

The closeness of $\tanh(\cdot)$ to the perceptron activation function in our experiments is controlled with $\beta > 0$ parameter.

\begin{equation}
    \tanh(\beta z) = \frac{e^{2\beta z - 1}}{e^{2\beta z} +1} 
\end{equation}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{tanh.png}
\caption{Shape of hyperbolic tangent for different $\beta$ parameters.}
\label{fig:tanh}
\end{figure}


The higher $\beta$ is, the better approximation of perceptron activation function is observed. With $\beta \rightarrow \infty$, $\tanh(\cdot)$ converges to the $\tau(\cdot)$. In experiments were exploited two parameters, $\beta_1$ and $\beta_2$, each to control transitions of $\tanh(\cdot)$ in different hidden layers. They will be mentioned and examined in further chapters. The influence of these parameters on the shape is depicted in the Figure \ref{fig:tanh}.

\subsection{Competitive decision-tree-motivated models}
In this section is provided overview of all decision-tree-motivated models examined in the experimental part of the thesis. Alongside the reference model proposed in Chapter 3, we propose another competitive models motivated by original decision tree structure. These models exploits knowledge acquired by training the decision tree and use it for setting the proper weights and biases up, but not all of them. Especially they relax conditions on weights in output layer (the main model from Chapter 3 has deterministic weights gained from the decision tree in all layers) and leave them random. This reduces inclination of model to overfitting, but does not provide accurate decision tree transformation as our reference model.

\subsubsection{Reference model (NRT deterministic weights)}

Acquisition of this model is described in Chapter 3. Its main purpose is to simulate corresponding decision tree behaviour as good as possible from the very beginning (quality of simulation is controlled with $\beta$ parameter of activation functions in the first and second hidden layer). With both  $\beta$s approach infinity, the reference model gives same predictions as decision tree. Of course, with respect to the predicting procedure of the decision tree, it could converge to predicting same results with $\beta$s only "high enough".
% sem dát možná nějaké vysvětlení, jaká vliv má změna v beta na prediction, použít nějaký horní odhad výstupního vektoru, maticové normy
After transformation the neural network is already a sufficient model capable of making relevant predictions, because it keeps similar behaviour as corresponding decision tree. This feature makes neural network very sensitive to the learning rate hyperparameter, which needs to be set beforehand. Also there exists significant risk of overfitting, which should be reduced by adapting regularization terms. Further analysis of these issues will be provided in later chapters. The closeness of this model to the corresponding decision tree gives good opportunity to enhance performance of the previous model by further backpropagation training. Also convergence should be very fast, which will be examined in the experimental part.

\subsubsection{NRT basic}

First competitive model has first and second hidden layer similar to the layers in the reference model. The difference is in setting weights and biases in the output layer. In this case we leave weights and biases in output layer purely random, as is depicted in Figure \ref{fig:nrt_basic}. This modification should be more relaxing with respect to the backpropagation training and reduce amount of sensitivity to the learning rate and risk of overfitting. On the other hand, it should slow down convergence in comparison with the reference model.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_basic.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_basic}
\end{figure}

\subsubsection{NRT extra layer}

Next competitive model add one extra layer of neurons between second hidden layer and output layer. This layer should serve as an aggregate layer that first summarize information from the second hidden layer (information from leaves of the decision tree) and then transfer this information to the output layer. Clearly, number of neurons in the third hidden layer is optional, but in our experiments was this value set up same as number of neurons in the output layer (number of classes). Weights and biases in this (third) hidden layer and output layer are purely random, as illustrated in the Figure \ref{fig:nrt_extra_layer}. We also exploited sigmoid function and Leaky ReLu as activation functions. This neural network setting uses decision tree to set a decisive start to the neural network to output leaf membership of an instance in the second hidden layer, then summarize information from this layer in the third hidden layer and extract output in the output layer. Adding extra layer with random parameters should in this case slow down convergence and enlarge the number of hyperparameters, but could have positive impact on the final performance and outperform previous models.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_extra_layer.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_extra_layer}
\end{figure}

\subsubsection{NRT extra layer - deterministic weights}

Final competitive model is the compound of 'NRT extra layer' and reference model. There is added an extra layer and weight and biases of the third hidden layer are initialized in the same manner as in the output layer of the reference model (usage of inverse of matrix $\mathbb{A}$). 

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_extra_layer_deterministic.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_extra_layer_det}
\end{figure}

%Describe all five competitive models that we use in experiments with illustrations and algorithms. Discuss replacement of $\tau$ with tanh activation function and its influence on performance and backpropagation. Divide basic model to sparse setting and full connected setting. In sparse setting we allow to train with backpropagation only notnull connections, to preserve decision tree interpretation. That will help us also to demonstrate the effect of backpropagation. In full connected setting we train all parameters.

\chapter{Neural Random Forest}

Combination of multiple neural random trees into an ensemble, similarly as in the case of random forests.

\subsection{MaxVote}
\subsection{Averaging}


\chapter{Experiments}

\begin{itemize}
    \item scaling (standardization)
    \item choice of models
    \item choice of hyperparameters/parameters
    \item choice of loss functions, output functions, optimizers (Adam)
    \item describe datasets
    \item describe evaluation (5-fold cross validation)
    \item insert table with results
    \item research sensitivity of hyperparameters
    \item insert supporting graphs
\end{itemize}
\begingroup
%\verb|\medmuskip=0mu:|
\setlength{\medmuskip}{0mu}
\begin{table}[h]
    \centering
    \small\addtolength{\tabcolsep}{-5pt}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
     DATASET & NN&LR&RF10&RF30&RF50&NRF\_DW&NRF&NRF\_DW\_EL&NRF\_EL\\ \hline
     Bank&$69.4\pm3.2$&$68.8\pm4.1$&$55.9\pm1.9$&$55.8\pm2.0$&$56.1\pm3.5$&$72.6\pm3.4$&$72.6\pm2.8$&$72.0\pm2.5$&$\boldsymbol{73.5}\pm3.7$ \\ \hline
     Cars&$99.9\pm0.1$&$84.5\pm2.8$&$56.0\pm6.0$&$56.1\pm7.8$&$59.4\pm5.9$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$ \\ \hline
     Diabetes&$71.7\pm3.8$&$72.8\pm5.2$&$70.7\pm2.5$&$71.4\pm5.4$&$72.3\pm2.1$&$72.0\pm3.6$&$\boldsymbol{73.5}\pm3.2$&$72.0\pm5.5$&$72.7\pm1.2$ \\ \hline
     Messidor&$70.4\pm3.2$&$72.3\pm2.3$&$64.5\pm2.8$&$66.7\pm3.0$&$65.2\pm2.7$&$71.3\pm1.4$&$73.9\pm5.4$&$\boldsymbol{74.2}\pm2.6$&$72.1\pm5.3$ \\ \hline
     USPS&$94.4\pm2.3$&$92.5\pm1.4$&$92.5\pm1.4$&$92.5\pm1.4$&$91.1\pm1.9$&$96.2\pm1.7$&$\boldsymbol{96.7}\pm1.7$&$96.5\pm1.4$&$95.8\pm1.4$ \\ \hline
     Vehicle&$80.1\pm4.1$&$79.4\pm3.3$&$71.2\pm6.5$&$71.1\pm4.0$&$71.1\pm4.9$&$82.4\pm3.6$&$83.1\pm2.1$&$\boldsymbol{84.3}\pm1.3$&$79.0\pm3.0$ \\ \hline
     Wine&$97.1\pm1.9$&$97.6\pm2.6$&$97.8\pm1.2$&$98.5\pm2.2$&$98.8\pm1.6$&$97.9\pm2.4$&$\boldsymbol{99.4}\pm1.3$&$99.0\pm1.5$&$\boldsymbol{99.4}\pm1.4$ \\ \hline
     OBS&$99.0\pm0.6$&$99.7\pm0.4$&$99.7\pm0.4$&$99.8\pm3.2$&$92.7\pm1.3$&$99.7\pm0.5$&$\boldsymbol{100}\pm0.0$&$99.6\pm0.5$&$98.3\pm1.4$ \\ \hline
    \end{tabular}
    \caption{Macro average of F1-score.}
    \label{tab:results_macro_public}
\end{table}

\begin{table}[h]
    \centering
    \small\addtolength{\tabcolsep}{-5pt}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
     DATASET & NN&LR&RF10&RF30&RF50&NRF\_DW&NRF&NRF\_DW\_EL&NRF\_EL\\ \hline
     Bank&$69.4\pm3.2$&$68.8\pm4.1$&$55.9\pm1.9$&$55.8\pm2.0$&$56.1\pm3.5$&$72.6\pm3.4$&$72.6\pm2.8$&$72.0\pm2.5$&$\boldsymbol{73.5}\pm3.7$ \\ \hline
     Cars&$99.9\pm0.1$&$84.5\pm2.8$&$56.0\pm6.0$&$56.1\pm7.8$&$59.4\pm5.9$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$&$\boldsymbol{100}\pm0.0$ \\ \hline
     Diabetes&$71.7\pm3.8$&$72.8\pm5.2$&$70.7\pm2.5$&$71.4\pm5.4$&$72.3\pm2.1$&$72.0\pm3.6$&$\boldsymbol{73.5}\pm3.2$&$72.0\pm5.5$&$72.7\pm1.2$ \\ \hline
     Messidor&$70.4\pm3.2$&$72.3\pm2.3$&$64.5\pm2.8$&$66.7\pm3.0$&$65.2\pm2.7$&$71.3\pm1.4$&$73.9\pm5.4$&$\boldsymbol{74.2}\pm2.6$&$72.1\pm5.3$ \\ \hline
     USPS&$94.4\pm2.3$&$92.5\pm1.4$&$92.5\pm1.4$&$92.5\pm1.4$&$91.1\pm1.9$&$96.2\pm1.7$&$\boldsymbol{96.7}\pm1.7$&$96.5\pm1.4$&$95.8\pm1.4$ \\ \hline
     Vehicle&$80.1\pm4.1$&$79.4\pm3.3$&$71.2\pm6.5$&$71.1\pm4.0$&$71.1\pm4.9$&$82.4\pm3.6$&$83.1\pm2.1$&$\boldsymbol{84.3}\pm1.3$&$79.0\pm3.0$ \\ \hline
     Wine&$97.1\pm1.9$&$97.6\pm2.6$&$97.8\pm1.2$&$98.5\pm2.2$&$98.8\pm1.6$&$97.9\pm2.4$&$\boldsymbol{99.4}\pm1.3$&$99.0\pm1.5$&$\boldsymbol{99.4}\pm1.4$ \\ \hline
     OBS&$99.0\pm0.6$&$99.7\pm0.4$&$99.7\pm0.4$&$99.8\pm3.2$&$92.7\pm1.3$&$99.7\pm0.5$&$\boldsymbol{100}\pm0.0$&$99.6\pm0.5$&$98.3\pm1.4$ \\ \hline
    \end{tabular}
    \caption{Average accuracy.}
    \label{tab:results_macro_public}
\end{table}



\endgroup










\begin{thebibliography}{}
\bibitem{NRF} Neural Random Forest. Gerard Biau, Erwan Scornet, Johannes Welbl

\bibitem{evaluation1}G. Canbek, S. Sagiroglu, T. T. Temizel and N. Baykal, "Binary classification performance measures/metrics: A comprehensive visualized roadmap to gain new insights," 2017 International Conference on Computer Science and Engineering (UBMK), Antalya, 2017, pp. 821-826

\bibitem{evaluation2}Koyejo, O. & Natarajan, Nagarajan \& Ravikumar, P. & Dhillon, I.S.. (2014). Consistent binary classification with generalized performance metrics. Advances in Neural Information Processing Systems. 3. 2744-2752. 

\bibitem{multieval1}Hossin, Mohammad & M.N, Sulaiman. (2015). A Review on Evaluation Metrics for Data Classification Evaluations. International Journal of Data Mining \& Knowledge Management Process. 5. 01-11. 10.5121/ijdkp.2015.5201. 

\bibitem{multieval2} Branco, Paula \& Torgo, Luís & Ribeiro, Rita. (2017). Relevance-Based Evaluation Metrics for Multi-class Imbalanced Domains. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). 698-710. 10.1007/978-3-319-57454-7\_54. 

\bibitem{imbalance}Chawla, Nitesh. (2005). Data Mining for Imbalanced Datasets: An Overview. 10.1007/0-387-25465-X\_40. 

\bibitem{bakalarka}bakalarka

\bibitem{imbalance_data}HE, H. a E. A. GARCIA. Learning from Imbalanced Data. IEEE Transactions on Knowledge and Data Engineering. 2009, (9), 1264-1284.

\bibitem{imbalance3} Akosa, J.S. (2017). Predictive Accuracy : A Misleading Performance Measure for Highly Imbalanced Data.

\bibitem{random_forest_intrusion}Random Forest Modeling for Network Intrusion Detection System
Citation DataProcedia Computer Science, ISSN: 1877-0509, Vol: 89, Page: 213-217 , 2016

\bibitem{computer_vision} Random Forest and computer vision
http://pages.iai.uni-bonn.de/frintrop\_simone//BVW13/BVW-gall.pdf

\bibitem{bioinf}R. D´ıaz-Uriarte and S. Alvarez de Andr´es. Gene selection and classification of
microarray data using random forest. BMC Bioinformatics, 7:1–13, 2006.

\bibitem{text_rec}Bernard, Simon \& Heutte, Laurent \& Adam, Sébastien. (2007). Using Random Forests for Handwritten Digit Recognition. Proceedings of the International Conference on Document Analysis and Recognition, ICDAR. 2. 1043-1047. 10.1109/ICDAR.2007.4377074. 

\bibitem{rf_overfit}Ali, Jehad \& Khan, Rehanullah \& Ahmad, Nasir \& Maqsood, Imran. (2012). Random Forests and Decision Trees. International Journal of Computer Science Issues(IJCSI). 9. 

\bibitem{rf_imp}PARR, Terence, Kerem TURGUTLU, Christopher CSISZAR a Jeremy HOWARD. Beware Default Random Forest Importances. Explained.ai [online]. 2018 [cit. 2018-05-15]. Dostupné z: http://explained.ai/rf-importance/index.html

\bibitem{rf_main}A. Criminisi, J. Shotton, and E. Konukoglu, Decision Forests: A Unified Framework for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning. Foundations and Trends in Computer Graphics and Computer Vision, Now Publishers Inc., 2012, 81-227.
\bibitem{c45}C4.5 algorithm and Multivariate Decision Trees
Thales Sehn Korting

\bibitem{cart}
Breiman, L., Friedman, J.H., Olshen, R., and Stone, C.J., 1984. Classification and Regression
Tree Wadsworth \& Brooks/Cole Advanced Books \& Software, Pacific Californi

\bibitem{id3}Xiaohu, Wang & Lele, Wang \& Nianfeng, Li. (2012). An Application of Decision Tree Based on ID3. Physics Procedia. 25. 1017-1021. 10.1016/j.phpro.2012.03.193. 

\bibitem{pruning}Zhou, Xinlei \& Yan, Dasen. (2019). Model tree pruning. International Journal of Machine Learning and Cybernetics. 10.1007/s13042-019-00930-9. 

\bibitem{pruning2}Esposito, Floriana \& Malerba, Donato \& Semeraro, Giovanni \& Kay, John. (1997). A Comparative Analysis of Methods for Pruning Decision Trees. Pattern Analysis and Machine Intelligence, IEEE Transactions on. 19. 476 - 491. 10.1109/34.589207. 

\bibitem{ensemble}Ensemble Methods in Machine Learning
Thomas G Dietterich
Oregon State University Corvallis Oregon USA
tgdcsorstedu WWW home page httpwwwcsorstedutgd

\bibitem{variance_rf} http://www.math.mcgill.ca/yyang/resources/doc/randomforest.pdf

\bibitem{adaboost} Coadou, Yann. (2013). Boosted Decision Trees and Applications. EPJ Web of Conferences. 55. 02004-. 10.1051/epjconf/20135502004. 

\bibitem{adaboost2}

\bibitem{culloch}W.S. McCulloch and W. Pitts. A logical calculus of the ideas immanent
in nervous activity. Bulletin of Mathematical Biology, 5(4):115–133, 1943.

\bibitem{perceptron}M. Minsky and S. Papert. Perceptrons. MIT Press, Cambridge, Mass,
1969.

\bibitem{werbos} P. J. Werbos. Beyond Regression: New Tools for Prediction and Analysis
in the Behavioral Sciences. PhD thesis, Harvard University, 1974.

\bibitem{backprop-cit}D. Rumelhart, G. Hinton, and R. Williams. Learning representations by
back-propagating errors. Nature, 323:533–536, October 1986

\bibitem{perceptron-nonsep}http://www.dkriesel.com/\_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf

\bibitem{nn-application}Hordri, Nur \& Yuhaniz, Siti \& Shamsuddin, Siti Mariyam. (2016). Deep Learning and Its Applications: A Review. 

\bibitem{feedforward} http://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf

\bibitem{cybenko}Cybenko, G. (1989) "Approximations by superpositions of sigmoidal functions", Mathematics of Control, Signals, and Systems, 2(4), 303–314. doi:10.1007/BF02551274

\bibitem{resnet}https://arxiv.org/pdf/1909.04653.pdf

\bibitem{actfunc}https://arxiv.org/pdf/1811.03378.pdf

\bibitem{tanh_prefer}B. Karlik and A. Vehbi, “Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks,”
International Journal of Artificial Intelligence and Expert Systems (IJAE), vol. 1, no. 4, pp. 111–122, 2011. [Online]. Available:
http://www.cscjournals.org/library/manuscriptinfo.php

\bibitem{relu}https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf

\bibitem{leaky}https://arxiv.org/pdf/1804.02763.pdf

\bibitem{grad}https://web.stanford.edu/~boyd/cvxbook/bv\_cvxbook.pdf

\bibitem{backprop}http://www.cs.cornell.edu/courses/cs5740/2016sp/resources/backprop.pdf citováno 30.3.2020

\bibitem{numpy}https://numpy.org/

\end{thebibliography}







\end{document}
