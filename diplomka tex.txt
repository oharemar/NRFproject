\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{makecell}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\title{MASTER'S THESIS}
\author{Bc. Martin Oharek }
\date{December 2019}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}

\chapter{Classification}
\section{State-of-the-art classification models}
\subsection{Logistic regression}
\subsection{Support vector machines (SVM)}
\subsection{Random forest}
\subsubsection{Decision tree}
\subsubsection{Ensemble of decision trees}
\subsection{Neural network (NN)}
\subsubsection{Perceptron and neuron}
\subsubsection{Activation functions}
\subsubsection{Backpropagation algorithm}
\section{Evaluation}

This section is focused on the summary and description of the evaluation metrics used in the experimental part, alongside the pros and cons of each metric and suitable application. This section includes only metrics that are applied in the testing phase. The correct choice of evaluation is the main and crucial task in the case of searching for optimal classification model. In practice, there exist numerous evaluation methods applied frequently to many classification tasks, each fits better in different cases. Before we choose set of evaluation metrics, we must be aware of the processed data and classification model to set our evaluation correctly. Also the purpose and future application scope of our model is significant. If this foremost analysis of the data and model is neglected or done wrong, we could end up choosing bad-shaped evaluation techniques and therefore misinterpret the performance of our model.

In the literature could be found significant amount of studies addressing the choice of evaluation metrics in the case of either binary classification \cite{evaluation1,evaluation2} or multi-class classification \cite{multieval1,multieval2}.

The classification models in this thesis are tested on multiple datasets, both binary and multi-class classification oriented. Consequently, there is not any particular property that could discriminate correct (the most correct) setting of evaluation methods in general, unless we study each dataset separately, which is not the purpose of this thesis at all. We only desire to compare classification performance quality of neural random forest mainly to the performance of regular random forests and provide the evidence of the superiority of neural random forests. Therefore we select subset of worldwide-accepted, well-functional evaluation techniques, summarize them and describe them separately. Combination of these metrics provides sufficient evaluation to build a conclusion.

Firstly, we define important notation that is referenced in the upcoming descriptions. This applies generally to the multi-class classification with arbitrary number of classes (labels). Set of all classes is denoted as $\mathbb{C}$.

\begin{definition}
\textit{Positive} class $c \in \mathbb{C}$ is such label, which is in the current scope of interest. Other classes $d \neq c$, $d \in \mathbb{C}$, are called \textit{negative} classes.
\label{definition_pos_neg}
\end{definition}

To clarify \ref{definition_pos_neg}, if we test the  classification performance of our model with respect to class $c \in \mathbb{C}$, then $c$ is called positive class and other labels $d \neq c$, $d \in \mathbb{C}$, are called negative classes. It is important to realize, that there could be many positive classes. It depends chiefly on the current scope of interest. For instance, computing accuracy of class $c_1 \in \mathbb{C}$ means that $c_1$ is (currently) positive class and other classes are negative. In the following, current positive class will be put into parentheses (such as $\text{TP}(c), \text{FN}(c)$ etc.) Also the instances belonging to the positive, resp. negative class could be referenced as positive, resp. negative instances.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    
       $\text{TP}(c)$ &\makecell{count of positive instances of class $c \in \mathbb{C}$ \\ correctly predicted } \\ \hline
         $\text{FP}(c)$& \makecell{count of negative instances \\ predicted incorrectly as positive class $c$ }  \\ \hline
         $\text{TN}(c)$ &\makecell{count of negative instances (with respect to the positive class $c$) \\ correctly predicted  }\\ \hline
         $\text{FN}(c)$ & \makecell{count of positive instances of class c incorrectly predicted as negative \\ class (with respect to the positive class $c$) }\\ \hline
    \end{tabular}
    \caption{Definitions of TP,FP,TN and FN.}
    \label{tab:definitions}
\end{table}
 


\subsubsection{Accuracy} 


Probably the most applied evaluation metric in the machine learning society, which provides quick and simple evaluation of the classification performance. Outcome of this metric is the ratio between number of correctly classified instances and number of all instances. Accuracy metric will be denoted as $Acc$.

\begin{equation}
    Acc = \frac{\text{number of correctly classified instances}}{\text{number of all instances}}
\end{equation}

Altough gaining this simple value for one classification model is very easy and quick, this metric possesses some significant drawbacks \cite{multieval1} and therefore relying only on this metric during final evaluation is dangerous. It does not take into account membership of instances to either positive or negative classes. Serious problem arises especially when processing imbalanced data \cite{imbalance}, which is the case of many real-world applications (fraud and malware detection \cite{bakalarka} etc.).

For instance, imagine data distributed into two classes, class 0 with 98\% occurrence and class 1 with only 2\% occurrence. This dataset is clearly significantly imbalanced. Moreover, we put higher preference on correctly classifying minority class 1 (e.g. consider class 1 as malware type communication and class 0 as arbitrary safe network communication, it could be basically anything rare vs. normal). Also define naive classifier to constantly classifying any instance as class 0. In this case, accuracy of this model is $Acc = 98\%$, which is very high. Based only on this value, the classification performance of our naive model is perfect. But claiming that this model is good is wrong, since it completely fails in classifying minority class, which has the highest priority. So even that accuracy is very high, the model itself is absolutely useless.

\subsubsection{Precision}

Another evaluation metric is called precision and is class-sensitive. In the contrast to the accuracy, it is computed for any particular class $c \in \mathbb{C}$ as
\begin{equation}
    Precision(c) = \frac{\text{TP}(c)}{\text{TP}(c)+\text{FP}(c)}
\end{equation}

It expresses the ratio between correctly predicted positive instances and all instances predicted as positive. In other words, it shows how precise is our classification model in predicting positive class.

This metric is also dependent on the rate of imbalance among current data. Let's suppose adding negative instances to the dataset. Then 

$$\text{FP}^{\text{less}}(c) \leq \text{FP}^{\text{more}}(c) \implies Precision^{\text{less}}(c) \geq Precision^{\text{more}}(c)$$

So even if our classifier predicts all positive instances correctly, the precision tends to decrease with increasing number of negative instances.

\subsubsection{Recall}

This metric is computed for class $c \in \mathbb{C}$ as

\begin{equation}
    Recall(c) = \frac{\text{TP}(c)}{\text{TP}(c) + \text{FN}(c)}
\end{equation}

Recall expresses ratio between number of correctly classified positive instances and number of all positive instances. Clearly, this metric does not depend on the rate of imbalance in our data and therefore is suitable choice for evaluation on imbalance datasets.

In many applications of classification on imbalance datasets is put a strong requirement on model to predict minority classes as precisely as possible (capture all positive instances, but also not to cause a lot of mistakes on negative instances), because misdetections could inflict great damage (e.g. malware detection \cite{bakalarka}). In some studies it turns out that combination of precision and recall is appropriate choice to satisfy these requirements and provides reliable evaluation \cite{imbalance_data}.

\subsubsection{F-measure}

F-measure metric combines precision and recall into single value as

\begin{equation}
    F_\beta(c) = \frac{(1+\beta^2)\cdot Precision(c) \cdot Recall(c)}{\beta^2 \cdot Precision(c) + Recall(c)}\hspace{0.5cm} ,
\end{equation}
where $c \in \mathbb{C}$ and $\beta$ is a coefficient which serves to adjust relative importance between precision and recall (often $\beta = 1$). For $\beta = 1$ it expresses harmonic mean of precision and recall. As $\beta \rightarrow 0$ the formula considers only precision and as $\beta \rightarrow \infty$ the formula considers only recall. Generally, $\beta < 1$ favors precision and $\beta > 1$ favors recall. It depends on a specific application to choose $\beta$ appropriately.

\subsubsection{G-measure}

G-measure is computed as

\begin{equation}
    G(c) = \sqrt{\frac{\text{TP}(c)}{\text{TP}(c) + \text{FN}(c)} \cdot \frac{\text{TN}(c)}{\text{TN}(c) + \text{FP}(c)}} \hspace{0.5cm},
    \label{g-meas}
\end{equation}
where $c \in \mathbb{C}$. The left factor in multiplication under the square root in \eqref{g-meas} is called \textit{Sensitivity} or \textit{true positive rate} (same as recall) and the right factor is called \textit{Specificity} or \textit{true negative rate}.

Optimization of this metric secures good balance between classification performance on both minority and majority classes. In the case of imbalance, even if the classification performance on negative instances is perfect, G-measure would end significantly low if classification of positive instances is poor \cite{imbalance3}. This is valuable property which makes this metric well-applicable to evaluation of the classification model on imbalanced data.

\textbf{Note:} Since we compare different classification models and track their overall performance on various datasets in the experimental part, the main purpose is not to compare metric values for each individual class, but rather to compare summaries of these evaluation outcomes. For this reason we adopt \textbf{macro/weighted average} to represent the individual metrics for different classes as single value.

For arbitrary evaluation metric $f(\cdot)$ is macro/weighted average defined as

\begin{equation}
    MacroAvg(f) = \frac{\sum_{l=1}^{M}f(c_l)}{M}
\end{equation}
\begin{equation}
    WeightedAvg(f) = \frac{\sum_{l=1}^{M}n_l \cdot f(c_l)}{\sum_{l=1}^{M}n_l} \hspace{0.5cm},
\end{equation}
where $c_l, l \in \{1,2,...,M\}$ is $l$-th class, $n_l$ is count of (testing) instances belonging to $c_l$ and $M$ is total number of classes.

It is also worth to mention \textbf{micro average} of precision and recall, which is defined as

\begin{equation}
    MicroAvg(Precision) = \frac{\sum_{l=1}^{M}\text{TP}(c_l)}{\sum_{l=1}^{M}(\text{TP}(c_l) + \text{FP}(c_l))}
\end{equation}

\begin{equation}
    MicroAvg(Recall) = \frac{\sum_{l=1}^{M}\text{TP}(c_l)}{\sum_{l=1}^{M}(\text{TP}(c_l) + \text{FN}(c_l))}
\end{equation}

$MicroAvg(Precision)$ computes basically fraction of the sum of all correctly predicted instances and sum of all correctly and incorrectly predicted instances, which is the sum of all instances. Therefore it expresses same value as accuracy of the model. Furthermore, since $\sum_{l=1}^{M}\text{FN}(c_l) = \sum_{l=1}^{M}\text{FP}(c_l)$, both values are equal. They will be used later in the experimental part of the thesis.


\subsubsection{Receiver operator charachteristics (ROC) curves}
ROC curves are frequently drawn especially in the cases of binary classifiers. Also they could be generalized to the multi-class classification by applying OneVsAll or OneVsOne approach and different averaging methods based on preference (e.g. $MacroAvg$,$MicroAvg$).

ZDE DOPSAT ZBYTEK + Precision - Recall Curves













\chapter{Decision-Tree-Inspired NN architecture}
In this section is provided detailed derivation of the basic neural network architecture, which is further employed in all subsequently proposed models. The main idea is built on the article \cite{NRF} describing 1 to 1 transformation of arbitrary regression tree (random forest regressor) to the specifically designed neural network (ensemble of neural networks).

Unfortunately, an approach presented in \cite{NRF} is not uniformly convertible to the classification problem. Based on this issue, the  alternative architecture and initial settings are proposed in order to simulate the exact behaviour of corresponding decision tree classifier.

This reformulation provides a sensible opportunity to enhance the decision tree performance, because parameters of newly constructed neural network could be better adapted with usage of the backpropagation algorithm of neural networks and therefore achieve superior classification.

\section{Architecture and initial settings}

The initial weights and biases of neurons and architecture of input layer and first two hidden layers will remain same among all proposed models. The settings was motivated by \cite{NRF}. A sample of such architecture (only input layer and first two hidden layers) is illustrated in Figure \ref{fig:basic_arch} copying decisions of the decision tree depicted in \ref{fig:dec_ilust}, which splits the space by two hyperplanes as illustrated also in \ref{fig:dec_ilust}. Let's first consider neurons in the first and second hidden layer of network as perceptrons. This means, that activation function is
\begin{equation}
    \tau(\boldsymbol{x}) = 2\mathbf{1}_{\boldsymbol{x} \geq 0} - 1\hspace{0.5cm},
\end{equation}
where all math operations on vector $\boldsymbol{x}$ are element-wise. Also
\begin{equation}
    (\mathbf{1}_{\boldsymbol{x} \geq 0})_i = \begin{cases}
    1, & \text{if}\hspace{0.25cm} x_i \geq 0 \\
    0, & \text{otherwise}
    \end{cases}\hspace{0.5cm},
\end{equation}

where $(\mathbf{1}_{\boldsymbol{x} \geq 0})_i$ is $i$-th element of $\mathbf{1}_{\boldsymbol{x} \geq 0}$.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{ilustrace_in_space.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{decision_tree_ilustrace.png}
\end{minipage}
\caption{In the left Figure are depicted 3 color-coded classes divided by the decision tree in the right Figure, that are separated by two hyperplanes: \\$x_1 - 2 = 0$ and $x_2 - 1 = 0$. Inner nodes of the decision tree are green colored with corresponding split function next to them, whereas the leaves are red colored. All nodes are numbered.}
\label{fig:dec_ilust}
\end{figure}
\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{ilustrace_basic_structure.png}
\caption{Transformation of the decision tree in Figure \ref{fig:dec_ilust} to the neural network with two hidden layers. The first hidden layer detects the decisions of inner nodes same numbered as in \ref{fig:dec_ilust}. The second hidden layer retrieves leaf membership of input vector same as in the decision tree. Not null connections between neurons are bold higlighted with corresponding weights. Biases are written next to the neurons. All dashed connections indicates null connection (weight equals 0).}
\label{fig:basic_arch}
\end{figure}

So perceptrons from the first and second hidden layer outputs $+1$ or $-1$. Why is it chosen in such manner will be clear from further explanations.

\subsection{First hidden layer}

The first hidden layer should copy decisions of all inner nodes present in the corresponding decision tree. The first hidden layer has same number of neurons as number of inner nodes. As was explained in the chapter about decision trees and random forests, each inner node $k \in \{1,...,L-1\}$ of the decision tree, where $L-1$ is total number of inner nodes (in fact, if $L-1$ is total number of inner nodes, then $L$ is total number of leaves present in the decision tree), possesses split function with parameters $j_k \in \{1,...,n\}$, which is one dimension in a $n$-dimensional space that is used for split and also $\alpha_{j_k}$, which is a threshold. Let's also define function $s_k$ as 
\begin{equation}
    s_k(\boldsymbol{x}) = x_{j_k} - \alpha_{j_k}\hspace{0.5cm}.
\end{equation}
It is apparent that equation $s_k(\boldsymbol{x}) = 0$ defines a hyperplane in $\mathbb{R}^n$, which splits the space in inner node $k$.

In order to obtain all decisions of inner nodes in the corresponding neurons of our neural network, we initialize weights in neuron $k$ as $(0,..,0,1,0,..,0)^T$ with single 1 in $j_k$-th position and 0 otherwise. Bias is set to $-\alpha_{j_k}$. Hence, output of the first hidden layer is $(\tau(s_1(\boldsymbol{x})),\tau(s_2(\boldsymbol{x})),...,\tau(s_{L-1}(\boldsymbol{x}))$ and it precisely copies decisions of inner nodes, with +1 indicating that input vector belongs to the right side of the hyperplane and -1 otherwise (and +1 if it belongs to the hyperplane). This is also done for the inner nodes outside the path of the input vector. The illustration can be seen in Figure \ref{fig:first_hid_layer}.


\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{first_hidden_layer_space.png}
\end{minipage}
%\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\textwidth]{first_hidden_layer.png}
\end{minipage}
\caption{Space in the left figure is divided by red (in inner node number 1) and black (in inner node number 2) hyperplanes. If we consider point $\boldsymbol{x} = (x_1,x_2)^T = (2.75,0.45)^T$ highlighted in left Figure, the output from the first hidden layer is +1 from neuron 1 (corresponding to the red hyperplane) and -1 from neuron 2 (corresponding to the black hyperplane).}
\label{fig:first_hid_layer}
\end{figure}

\subsection{Second hidden layer}

From all inner node decisions obtained by the first hidden layer it should be possible to reconstruct the exact leaf membership of the input vector $\boldsymbol{x}$. This is the main task to accomplish by the second hidden layer. If there are $L-1$ neurons in the first hidden layer, then the second hidden layer consists of $L$ neurons, each one corresponding to the one individual leaf in the decision tree.

We connect neuron $m$ in the first hidden layer to neuron $m'$ in the second hidden layer with not null weight if and only if inner node corresponding to the neuron $m$ belongs to the path from root node to the leaf corresponding to the neuron $m'$. The weight is initialized to +1 if the split by inner node $m$ is to the right child and -1 otherwise. If neuron $m$ is not part of the path from root to the leaf, the weight is initialized always to 0.

Based on this setting, it could be simply deduced that number of not null connections from the first hidden layer (weights) to the (arbitrary) neuron $m'$ in the second hidden layer is same as length of the path from root to the leaf $m'$. This is illustrated in Figure \ref{fig:sec_hidd_len}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{sec_hidd.png}
\caption{In the left figure is highlighted blue path from the root node 1 to the leaf 3. This has length 1 and also only one initialized not null connection from the first hidden layer exists, because only root node is part of the path. In the remaining figures are depicted paths for other leaves (blue colored) from the root node (except of leaf 6, which is in same depth as leaf 7). It is easy to see, that equality between length of the path from the root node to the particular leaf and number of not null connections from the first hidden layer to the corresponding neuron holds always.}
\label{fig:sec_hidd_len}
\end{figure}

If output from the first hidden layer is $\boldsymbol{v} = (\pm 1, \pm 1,....,\pm 1)^T$, which encodes all decisions of inner nodes of the decision tree, then output from the neuron $m'$ in the second hidden layer is $\tau(\sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m'))$, where\\ $\boldsymbol{w}^{m'} = (w_1^{m'},w_2^{m'},...,w_{L-1}^{m'})^T$ is a vector of  weights for connections to neuron $m'$. These weights are not null if the corresponding inner node is involved in the path from root to the leaf $m'$ and are +1 if it is sent to the right child and -1 otherwise. For all inner nodes that are not involved in the root-leaf path are weights initialized to 0.

Desired behaviour of the neuron $m'$ in the second hidden layer is to output +1 if the input vector ends in leaf $m'$ and -1 otherwise. For this purpose, $bias(m')$ must be correctly set. To do so, it is sufficient to notice that the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i)$ as the first term in the argument of $\tau(\cdot)$ function equals to the length of the path from root node to leaf $m'$, if and only if the input ends in leaf $m'$. For convenience, let us denote this length as $l(m')$. It is a simple consequence of the fact, that the number of not null weights $w_{i_k}^{m'}$ is same as $l(m')$ and also they have same magnitude ($|w_{i_k}^{m'}| = 1$) and same sign as $v_{i_k}$, where $i_k \in \{1,...,L-1\}, w_{i_k}^{m'} \neq 0$. Therefore, each member of the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i) = \sum_{i_k=1,w_{i_k}^{m'} \neq 0}^{L-1}(w_{i_k}^{m'}v_{i_k})$ equals to +1 and all members sum up to $l(m')$.

Moreover, if the input does not end in leaf $m'$, then in the sum $\sum_{i=1}^{L-1}(w_i^{m'}v_i)$ exists a not null member in which interfere 2 integers (ones) with different signs, resulting in -1. Hence is clear, that if input does not end in leaf $m'$, the sum holds inequality $\sum_{i=1}^{L-1}(w_i^{m'}v_i) \leq l(m')-1 < l(m')$. For more precise intuition, the illustration is provided in Figure \ref{fig:sec_hidd_ilust}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{sec_hidd_ilust.png}
\caption{Demonstration of inequality $\sum_{i=1}^{L-1}(w_i^{m'}v_i) \leq l(m')-1 < l(m')$ in case of leaf number 7 (corresponding to one neuron in the second hidden layer of our network), if input does not end in leaf $m'$. The red path in the picture indicates real path of input in the decision tree. Our sample input ends in leaf number 6, as could be seen from the illustration. The output from the first hidden layer would therefore be $\boldsymbol{v} = (v_1,v_2,v_3)^T = (+1,-1,-1)^T$. But weights corresponding to neuron 7 (leaf 7) are $\boldsymbol{w}^7 = (w_1^7,w_2^7,w_3^7)^T = (+1,-1,+1)^T$. After multiplying the values in red circles and summing the results up, we obtain $\sum_{i=1}^{L-1}(w_i^{7}v_i) = 2 < 3$, where 3 means the length of the root-leaf path. A decrease is caused due to the interference of different signs in a grey circle.}
\label{fig:sec_hidd_ilust}
\end{figure}

After these considerations, the reasonable choice of $bias(m')$ is
\begin{equation}
    bias(m') = -l(m') + 0.5
\label{eq:bias}    
\end{equation}
and then $\sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m')$ has following property:
\begin{equation}
    \sum_{i=1}^{L-1}(w_i^{m'}v_i) + bias(m')\begin{cases}
    > 0, & \text{if input ends in leaf}\hspace{0.18cm} m' \\
    < 0, & \text{otherwise}
    \end{cases}
\label{eq:sec_hidd_property}
\end{equation}

With respect to the property of $\tau(\cdot)$ function argument in \eqref{eq:sec_hidd_property}, the second hidden layer outputs a vector of $(-1,...,-1,+1,-1,...,-1)^T$ with a single positive 1 indicating the correct leaf membership of an input.

To retain this $\tau(\cdot)$ argument property, it is sufficient to choose any other arbitrary constant in \eqref{eq:bias} in range $(0,1)$ instead of 0.5. But to stay consistent with \cite{NRF}, we also used the proposed value of 0.5 in conducted experiments.

At this stage, we already defined architecture and initial weights and biases settings of first two hidden layers in order to transform decision tree into neural network with the same properties. All that remains is to gain classification predictions from the second hidden layer.

\subsection{Output layer}

In this section is proposed architecture and initial setting for output layer, that will gain same predictions as the decision tree. Unfortunately, same method proposed for regression trees in \cite{NRF} is not directly applicable in the classification case. Therefore we propose an alternative for the classification case, that give same classification outcomes as the corresponding decision tree.

The output layer will be constructed as follows: The number of neurons in output layer is equal to number of classes we desire to classify. Each neuron corresponds to only one particular class (one label). Neuron with the highest activation represents the predicted class of neural network. In the experiments were used two types of neurons in output layer - with sigmoid and softmax activation functions, as were discussed in Chapter [Activation functions].  To get the same performance as the decision tree, we must retrieve probability distributions stored in leaves from the leaf membership encoded in the second hidden layer. If the output layer outcomes the same probability values for classes as the decision tree, hence the neural network performs alike.

Let's denote output from the second hidden layer as\\ $\boldsymbol{r} = (-1,...,-1,+1,-1,...,-1)^T$, where the position of +1 indicates the leaf where the input falls in. For each leaf $l \in \{1,...,L\}$ we denote a probability vector $\boldsymbol{p^l} = (p^l_1,p^l_2,...,p^l_C)^T$ with probabilities of individual classes, that are stored in leaf $l$, where $C$ is total number of classes. If $\boldsymbol{r}$ has +1 as the first element (corresponding to the first leaf), i.e. $r_1 = +1$, the output layer should outcome $\boldsymbol{p}^1$. If $r_2 = +1$, the outcome should be $\boldsymbol{p}^2$ etc.

If we initialize biases in the output layer to 0, then appropriate initialization weights could be obtained by solving the system of linear equations with a regular matrix $\mathbb{A}$
\begin{equation}
    \mathbb{A}=\begin{pmatrix}
    1 & -1 & -1 &\hdots & -1 \\
    -1 & 1 & -1 & \hdots & -1\\
    \vdots & \ddots & \ddots & \ddots&\vdots \\
    -1 & \ddots & \ddots & \ddots&-1 \\
    -1 & -1 & \hdots & -1 & 1
    \end{pmatrix}\hspace{0.5cm}.
\end{equation}

Matrix $\mathbb{A} \in \mathbb{R}^{L\times L}$, where $L$ is a total number of leaves in the decision tree. The determinant of matrix $\mathbb{A}$ is
\begin{equation}
    \text{det}\mathbb{A} = \begin{vmatrix}
    1 & -1 & -1 &\hdots & -1 \\
    -1 & 1 & -1 & \hdots & -1\\
    \vdots & \ddots & \ddots & \ddots&\vdots \\
    -1 & \ddots & \ddots & \ddots&-1 \\
    -1 & -1 & \hdots & -1 & 1
    \end{vmatrix} = (-1)^{2L-3}\cdot2^{L-1}\cdot(L-2)
\label{eq:determinant}
\end{equation}

For the proof of \eqref{eq:determinant} see Chapter [Proves]. Matrix $\mathbb{A}$ is therefore always regular with except of $L = 2$. In this case, $\text{det}\mathbb{A} = 0$ and matrix $\mathbb{A}$ is singular. But for $L=2$, the decision tree has only root node and 2 leaves, which is rarely a well-functional model in practical use. It could have a good performance almost only in case of data with significantly unambigous geometric deployment, where occurs only 2 classes and exists one hyperplane that sufficiently separates them. As the conclusion, we will almost never encounter such an elementary model.

In case of invertible activation function $\sigma(\cdot)$ in the output layer (in our experiments were used sigmoid and softmax activation functions, which are both invertible), we obtain appropriate weights for neuron $c$ (also represents class $c \in \{1,...,C\}$ that this neuron corresponds to) in output layer after solving the following system of linear equations:

\begin{equation}
    \mathbb{A}\begin{pmatrix}
    w^c_1\\
    w^c_2\\
    \vdots\\
    w^c_L
    \end{pmatrix} = \sigma^{-1}(\begin{pmatrix}
    p^1_c\\
    p^2_c\\
    \vdots\\
    p^L_c
    \end{pmatrix})\hspace{0.5cm},
    \label{eq:lin_eq}
\end{equation}

where $\boldsymbol{w}^c = (w^c_1,...,w^c_L)^T$ are weights of connections from the second hidden layer to neuron $c$ in the output layer and $\sigma^{-1}(\cdot)$ is inverse function of $\sigma(\cdot)$. In other words, the appropriate weights for neuron $c$ in the output layer are obtained as

\begin{equation}
    \begin{pmatrix}
    w^c_1\\
    w^c_2\\
    \vdots\\
    w^c_L
    \end{pmatrix} = \mathbb{A}^{-1}(\sigma^{-1}(\begin{pmatrix}
    p^1_c\\
    p^2_c\\
    \vdots\\
    p^L_c
    \end{pmatrix}))\hspace{0.5cm}.
    \label{eq:weights_output}
\end{equation}

After initialization of biases to 0 and solving the system of linear equations for all neurons in the output layer (or computing the inverse of matrix $\mathbb{A}$) and for appropriate activation function chosen in advance, we gain also all initialization weights. This initial setting causes the neural network to output same predictions as from the corresponding decision tree and hence to get equally performing classification model.

\section{NN models with decision-tree-initialization}

\subsection{Perceptron activation function replacement}

In order to apply reasonable training procedure with backpropagation algorithm on proposed method, it is suitable to replace perceptron activation function $\tau(\cdot)$ with its smooth approximation. For this purpose the hyperbolic tangent function was adopted (Figure BLA). This affects original transformation, because due to this approximation the neural random tree is no longer one-to-one transformation to former decision tree. But still under certain conditions (especially the transition slope from the negative part of the $\tanh(\cdot)$ function to the positive one - the more upright, the better approximation we get) it could come fairly close to the perceptron function and therefore entire model performance remains unchanged with respect to the decision tree. Usage of $\tanh(\cdot)$ in first and second hidden layer is necessary with respect to the transformation procedure, even that $\tanh(\cdot)$ has some drawbacks as activation function (e.g. vanishing gradients [ref]). Using other activation functions in this case is not appropriate, because it will lack any decision tree relationship.

The closeness of $\tanh(\cdot)$ to the perceptron activation function in our experiments is controlled with $\beta > 0$ parameter.

\begin{equation}
    \tanh(\beta z) = \frac{e^{2\beta z - 1}}{e^{2\beta z} +1} 
\end{equation}

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{tanh.png}
\caption{Shape of hyperbolic tangent for different $\beta$ parameters.}
\label{fig:tanh}
\end{figure}


The higher $\beta$ is, the better approximation of perceptron activation function is observed. With $\beta \rightarrow \infty$, $\tanh(\cdot)$ converges to the $\tau(\cdot)$. In experiments were exploited two parameters, $\beta_1$ and $\beta_2$, each to control transitions of $\tanh(\cdot)$ in different hidden layers. They will be mentioned and examined in further chapters. The influence of these parameters on the shape is depicted in the Figure \ref{fig:tanh}.

\subsection{Competitive decision-tree-motivated models}
In this section is provided overview of all decision-tree-motivated models examined in the experimental part of the thesis. Alongside the reference model proposed in Chapter 3, we propose another competitive models motivated by original decision tree structure. These models exploits knowledge acquired by training the decision tree and use it for setting the proper weights and biases up, but not all of them. Especially they relax conditions on weights in output layer (the main model from Chapter 3 has deterministic weights gained from the decision tree in all layers) and leave them random. This reduces inclination of model to overfitting, but does not provide accurate decision tree transformation as our reference model.

\subsubsection{Reference model (NRT deterministic weights)}

Acquisition of this model is described in Chapter 3. Its main purpose is to simulate corresponding decision tree behaviour as good as possible from the very beginning (quality of simulation is controlled with $\beta$ parameter of activation functions in the first and second hidden layer). With both  $\beta$s approach infinity, the reference model gives same predictions as decision tree. Of course, with respect to the predicting procedure of the decision tree, it could converge to predicting same results with $\beta$s only "high enough".
% sem dát možná nějaké vysvětlení, jaká vliv má změna v beta na prediction, použít nějaký horní odhad výstupního vektoru, maticové normy
After transformation the neural network is already a sufficient model capable of making relevant predictions, because it keeps similar behaviour as corresponding decision tree. This feature makes neural network very sensitive to the learning rate hyperparameter, which needs to be set beforehand. Also there exists significant risk of overfitting, which should be reduced by adapting regularization terms. Further analysis of these issues will be provided in later chapters. The closeness of this model to the corresponding decision tree gives good opportunity to enhance performance of the previous model by further backpropagation training. Also convergence should be very fast, which will be examined in the experimental part.

\subsubsection{NRT basic}

First competitive model has first and second hidden layer similar to the layers in the reference model. The difference is in setting weights and biases in the output layer. In this case we leave weights and biases in output layer purely random, as is depicted in Figure \ref{fig:nrt_basic}. This modification should be more relaxing with respect to the backpropagation training and reduce amount of sensitivity to the learning rate and risk of overfitting. On the other hand, it should slow down convergence in comparison with the reference model.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_basic.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_basic}
\end{figure}

\subsubsection{NRT extra layer}

Next competitive model add one extra layer of neurons between second hidden layer and output layer. This layer should serve as an aggregate layer that first summarize information from the second hidden layer (information from leaves of the decision tree) and then transfer this information to the output layer. Clearly, number of neurons in the third hidden layer is optional, but in our experiments was this value set up same as number of neurons in the output layer (number of classes). Weights and biases in this (third) hidden layer and output layer are purely random, as illustrated in the Figure \ref{fig:nrt_extra_layer}. We also exploited sigmoid function and Leaky ReLu as activation functions. This neural network setting uses decision tree to set a decisive start to the neural network to output leaf membership of an instance in the second hidden layer, then summarize information from this layer in the third hidden layer and extract output in the output layer. Adding extra layer with random parameters should in this case slow down convergence and enlarge the number of hyperparameters, but could have positive impact on the final performance and outperform previous models.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_extra_layer.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_extra_layer}
\end{figure}

\subsubsection{NRT extra layer - deterministic weights}

Final competitive model is the compound of 'NRT extra layer' and reference model. There is added an extra layer and weight and biases of the third hidden layer are initialized in the same manner as in the output layer of the reference model (usage of inverse of matrix $\mathbb{A}$). 

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{nrt_extra_layer_deterministic.png}
\caption{NRT basic model. In comparison with the reference model, the weights and biases of the output layer are initialized randomly.}
\label{fig:nrt_extra_layer_det}
\end{figure}

%Describe all five competitive models that we use in experiments with illustrations and algorithms. Discuss replacement of $\tau$ with tanh activation function and its influence on performance and backpropagation. Divide basic model to sparse setting and full connected setting. In sparse setting we allow to train with backpropagation only notnull connections, to preserve decision tree interpretation. That will help us also to demonstrate the effect of backpropagation. In full connected setting we train all parameters.


\chapter{Experiments}

\begin{thebibliography}{}
\bibitem{NRF} Neural Random Forest. Gerard Biau, Erwan Scornet, Johannes Welbl

\bibitem{evaluation1}G. Canbek, S. Sagiroglu, T. T. Temizel and N. Baykal, "Binary classification performance measures/metrics: A comprehensive visualized roadmap to gain new insights," 2017 International Conference on Computer Science and Engineering (UBMK), Antalya, 2017, pp. 821-826

\bibitem{evaluation2}Koyejo, O. & Natarajan, Nagarajan \& Ravikumar, P. & Dhillon, I.S.. (2014). Consistent binary classification with generalized performance metrics. Advances in Neural Information Processing Systems. 3. 2744-2752. 

\bibitem{multieval1}Hossin, Mohammad & M.N, Sulaiman. (2015). A Review on Evaluation Metrics for Data Classification Evaluations. International Journal of Data Mining \& Knowledge Management Process. 5. 01-11. 10.5121/ijdkp.2015.5201. 

\bibitem{multieval2} Branco, Paula \& Torgo, Luís & Ribeiro, Rita. (2017). Relevance-Based Evaluation Metrics for Multi-class Imbalanced Domains. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). 698-710. 10.1007/978-3-319-57454-7\_54. 

\bibitem{imbalance}Chawla, Nitesh. (2005). Data Mining for Imbalanced Datasets: An Overview. 10.1007/0-387-25465-X\_40. 

\bibitem{bakalarka}bakalarka

\bibitem{imbalance_data}HE, H. a E. A. GARCIA. Learning from Imbalanced Data. IEEE Transactions on Knowledge and Data Engineering. 2009, (9), 1264-1284.

\bibitem{imbalance3} Akosa, J.S. (2017). Predictive Accuracy : A Misleading Performance Measure for Highly Imbalanced Data.

\end{thebibliography}







\end{document}
